{"2024-09-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.04667v2","updated":"2024-09-10T17:56:52Z","published":"2024-09-07T00:46:58Z","title":"QueryBuilder: Human-in-the-Loop Query Development for Information\n  Retrieval","summary":"  Frequently, users of an Information Retrieval (IR) system start with an\noverarching information need (a.k.a., an analytic task) and proceed to define\nfiner-grained queries covering various important aspects (i.e., sub-topics) of\nthat analytic task. We present a novel, interactive system called\n$\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create\nqueries with a small amount of effort, through efficient exploration of an\nEnglish development corpus in order to rapidly develop cross-lingual\ninformation retrieval queries corresponding to the user's information needs.\nQueryBuilder performs near real-time retrieval of documents based on\nuser-entered search terms; the user looks through the retrieved documents and\nmarks sentences as relevant to the information needed. The marked sentences are\nused by the system as additional information in query formation and refinement:\nquery terms (and, optionally, event features, which capture event $'triggers'$\n(indicator terms) and agent/patient roles) are appropriately weighted, and a\nneural-based system, which better captures textual meaning, retrieves other\nrelevant content. The process of retrieval and marking is repeated as many\ntimes as desired, giving rise to increasingly refined queries in each\niteration. The final product is a fine-grained query used in Cross-Lingual\nInformation Retrieval (CLIR). Our experiments using analytic tasks and requests\nfrom the IARPA BETTER IR datasets show that with a small amount of effort (at\nmost 10 minutes per sub-topic), novice users can form $\\textit{useful}$\nfine-grained queries including in languages they don't understand. QueryBuilder\nalso provides beneficial capabilities to the traditional corpus exploration and\nquery formation process. A demonstration video is released at\nhttps://vimeo.com/734795835\n","authors":["Hemanth Kandula","Damianos Karakos","Haoling Qiu","Benjamin Rozonoyer","Ian Soboroff","Lee Tarlin","Bonan Min"],"pdf_url":"https://arxiv.org/pdf/2409.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06691v1","updated":"2024-09-10T17:54:28Z","published":"2024-09-10T17:54:28Z","title":"Geometric-Averaged Preference Optimization for Soft Preference Labels","summary":"  Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority.\n","authors":["Hiroki Furuta","Kuang-Huei Lee","Shixiang Shane Gu","Yutaka Matsuo","Aleksandra Faust","Heiga Zen","Izzeddin Gur"],"pdf_url":"https://arxiv.org/pdf/2409.06691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06679v1","updated":"2024-09-10T17:44:35Z","published":"2024-09-10T17:44:35Z","title":"E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning","summary":"  In the realm of Large Language Models (LLMs), the ability to process long\ncontexts is increasingly crucial for tasks such as multi-round dialogues, code\ngeneration, and document summarization. This paper addresses the challenges of\nenhancing the long-context performance, reducing computational complexity, and\nleveraging pretrained models collectively termed the \"impossible triangle.\" We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. The method involves splitting long\ncontexts into chunks, compressing each into embedding vectors via a pretrained\ntext encoder, and utilizing an adapter to align these representations with a\ndecoder-only LLM. Two training objectives, focusing on reconstruction of the\nencoder output and long-context instruction fine-tuning, are employed to\nfacilitate the understanding of soft prompts by the LLM. Experimental results\ndemonstrate that E2LLM achieves superior performance in long-context scenarios\nwhile balancing efficiency, performance, and compatibility with pretrained\nmodels. Our framework thus represents a significant advancement in the field,\ncontributing to effective long-text modeling.\n","authors":["Zihan Liao","Jun Wang","Hang Yu","Lingxiao Wei","Jianguo Li","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06679v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.17692v2","updated":"2024-09-10T17:39:41Z","published":"2024-01-31T09:28:06Z","title":"Mitigating the Influence of Distractor Tasks in LMs with Prior-Aware\n  Decoding","summary":"  The broad capabilities of Language Models (LMs) can be limited by their\nsensitivity to distractor tasks: LMs can infer secondary tasks from the prompt\nin addition to the intended one, leading to unwanted outputs. For example,\nprompt injection attacks can cause models to deviate from explicit directives.\nIn some 'inverse scaling' cases, this unwanted behaviour actually worsens as\nmodels scale up to at least 540B parameters. We present a theoretical framework\nthat interprets LMs as a product of experts that combine multiple data\ngeneration processes. Based on this framework, we demonstrate prior-aware\ndecoding (PAD) - a simple contrastive inference method to reduce the influence\nof distractor tasks. We apply PAD to eleven models, across four datasets, and\nfind improvements in 41 out of 44 task-model combinations, with a median\nincrease in task completion proportion of 40%. The results suggest a promising\ndirection for further development towards more reliable language models.\n","authors":["Raymond Douglas","Andis Draguns","Tomáš Gavenčiak"],"pdf_url":"https://arxiv.org/pdf/2401.17692v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.06666v1","updated":"2024-09-10T17:34:34Z","published":"2024-09-10T17:34:34Z","title":"LLaMA-Omni: Seamless Speech Interaction with Large Language Models","summary":"  Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.\n","authors":["Qingkai Fang","Shoutao Guo","Yan Zhou","Zhengrui Ma","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2409.06666v1.pdf","comment":"Preprint. Project: https://github.com/ictnlp/LLaMA-Omni"},{"id":"http://arxiv.org/abs/2409.00055v2","updated":"2024-09-10T17:26:29Z","published":"2024-08-21T04:47:26Z","title":"SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models","summary":"  The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.\n","authors":["Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2409.00055v2.pdf","comment":"12 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.06656v1","updated":"2024-09-10T17:20:11Z","published":"2024-09-10T17:20:11Z","title":"Sortformer: Seamless Integration of Speaker Diarization and ASR by\n  Bridging Timestamps and Tokens","summary":"  We propose Sortformer, a novel neural model for speaker diarization, trained\nwith unconventional objectives compared to existing end-to-end diarization\nmodels. The permutation problem in speaker diarization has long been regarded\nas a critical challenge. Most prior end-to-end diarization systems employ\npermutation invariant loss (PIL), which optimizes for the permutation that\nyields the lowest error. In contrast, we introduce Sort Loss, which enables a\ndiarization model to autonomously resolve permutation, with or without PIL. We\ndemonstrate that combining Sort Loss and PIL achieves performance competitive\nwith state-of-the-art end-to-end diarization models trained exclusively with\nPIL. Crucially, we present a streamlined multispeaker ASR architecture that\nleverages Sortformer as a speaker supervision model, embedding speaker label\nestimation within the ASR encoder state using a sinusoidal kernel function.\nThis approach resolves the speaker permutation problem through sorted\nobjectives, effectively bridging speaker-label timestamps and speaker tokens.\nIn our experiments, we show that the proposed multispeaker ASR architecture,\nenhanced with speaker supervision, improves performance via adapter techniques.\nCode and trained models will be made publicly available via the NVIDIA NeMo\nframework\n","authors":["Taejin Park","Ivan Medennikov","Kunal Dhawan","Weiqing Wang","He Huang","Nithin Rao Koluguri","Krishna C. Puvvada","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2409.06656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06639v1","updated":"2024-09-10T16:54:32Z","published":"2024-09-10T16:54:32Z","title":"TeXBLEU: Automatic Metric for Evaluate LaTeX Format","summary":"  LaTeX is highly suited to creating documents with special formatting,\nparticularly in the fields of science, technology, mathematics, and computer\nscience. Despite the increasing use of mathematical expressions in LaTeX format\nwith language models, there are no evaluation metrics for evaluating them. In\nthis study, we propose TeXBLEU, an evaluation metric tailored for mathematical\nexpressions in LaTeX format, based on the n-gram-based BLEU metric that is\nwidely used for translation tasks. The proposed TeXBLEU includes a predefined\ntokenizer trained on the arXiv paper dataset and a finetuned embedding model.\nIt also considers the positional embedding of tokens. Simultaneously, TeXBLEU\ncompares tokens based on n-grams and computes the score using exponentiation of\na logarithmic sum, similar to the original BLEU. Experimental results show that\nTeXBLEU outperformed traditional evaluation metrics such as BLEU, Rouge, CER,\nand WER when compared to human evaluation data on the test dataset of the\nMathBridge dataset, which contains 1,000 data points. The average correlation\ncoefficient with human evaluation was 0.71, which is an improvement of 87%\ncompared with BLEU, which had the highest correlation with human evaluation\ndata among the existing metrics. The code is available at\nhttps://github.com/KyuDan1/TeXBLEU.\n","authors":["Kyudan Jung","Nam-Joon Kim","Hyongon Ryu","Sieun Hyeon","Seung-jun Lee","Hyeok-jae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06639v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.06635v1","updated":"2024-09-10T16:46:18Z","published":"2024-09-10T16:46:18Z","title":"MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders","summary":"  The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.\n","authors":["Wenyu Zhang","Shuo Sun","Bin Wang","Xunlong Zou","Zhuohan Liu","Yingxu He","Geyu Lin","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2409.06635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06624v1","updated":"2024-09-10T16:26:43Z","published":"2024-09-10T16:26:43Z","title":"A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio","summary":"  Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.\n","authors":["Ningyuan Xi","Yetao Wu","Kun Fan","Teng Chen","Qingqing Gu","Peng Yu","Jinxian Qu","Chenxi Liu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.06624v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.06622v1","updated":"2024-09-10T16:22:18Z","published":"2024-09-10T16:22:18Z","title":"Exploring Italian sentence embeddings properties through multi-tasking","summary":"  We investigate to what degree existing LLMs encode abstract linguistic\ninformation in Italian in a multi-task setting. We exploit curated synthetic\ndata on a large scale -- several Blackbird Language Matrices (BLMs) problems in\nItalian -- and use them to study how sentence representations built using\npre-trained language models encode specific syntactic and semantic information.\nWe use a two-level architecture to model separately a compression of the\nsentence embeddings into a representation that contains relevant information\nfor a task, and a BLM task. We then investigate whether we can obtain\ncompressed sentence representations that encode syntactic and semantic\ninformation relevant to several BLM tasks. While we expected that the sentence\nstructure -- in terms of sequence of phrases/chunks -- and chunk properties\ncould be shared across tasks, performance and error analysis show that the\nclues for the different tasks are encoded in different manners in the sentence\nembeddings, suggesting that abstract linguistic notions such as constituents or\nthematic roles does not seem to be present in the pretrained sentence\nembeddings.\n","authors":["Vivi Nastase","Giuseppe Samo","Chunyang Jiang","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2409.06622v1.pdf","comment":"9 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.06601v1","updated":"2024-09-10T15:51:15Z","published":"2024-09-10T15:51:15Z","title":"Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling","summary":"  Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.\n","authors":["Yetao Wu","Yihong Wang","Teng Chen","Chenxi Liu","Ningyuan Xi","Qingqing Gu","Hongyang Lei","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.06601v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.06595v1","updated":"2024-09-10T15:39:32Z","published":"2024-09-10T15:39:32Z","title":"GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question\n  Answering","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use\nLarge Language Models (LLMs) alongside private and up-to-date knowledge bases.\nIn this work, we address the challenges of using LLM-as-a-Judge when evaluating\ngrounded answers generated by RAG systems. To assess the calibration and\ndiscrimination capabilities of judge models, we identify 7 generator failure\nmodes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a\nmeta-evaluation benchmark of 144 unit tests. This benchmark reveals that\nexisting automated RAG evaluation frameworks often overlook important failure\nmodes, even when using GPT-4 as a judge.\n  To improve on the current design of automated RAG evaluation frameworks, we\npropose a novel pipeline and find that while closed models perform well on\nGroUSE, state-of-the-art open-source judges do not generalize to our proposed\ncriteria, despite strong correlation with GPT-4's judgement. Our findings\nsuggest that correlation with GPT-4 is an incomplete proxy for the practical\nperformance of judge models and should be supplemented with evaluations on unit\ntests for precise failure mode detection.\n  We further show that finetuning Llama-3 on GPT-4's reasoning traces\nsignificantly boosts its evaluation capabilities, improving upon both\ncorrelation with GPT-4's evaluations and calibration on reference situations.\n","authors":["Sacha Muller","António Loison","Bilel Omrani","Gautier Viaud"],"pdf_url":"https://arxiv.org/pdf/2409.06595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08921v2","updated":"2024-09-10T15:38:56Z","published":"2024-08-15T12:20:24Z","title":"Graph Retrieval-Augmented Generation: A Survey","summary":"  Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.\n","authors":["Boci Peng","Yun Zhu","Yongchao Liu","Xiaohe Bo","Haizhou Shi","Chuntao Hong","Yan Zhang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2408.08921v2.pdf","comment":"Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided"},{"id":"http://arxiv.org/abs/2409.06567v1","updated":"2024-09-10T14:58:55Z","published":"2024-09-10T14:58:55Z","title":"Exploring syntactic information in sentence embeddings through\n  multilingual subject-verb agreement","summary":"  In this paper, our goal is to investigate to what degree multilingual\npretrained language models capture cross-linguistically valid abstract\nlinguistic representations. We take the approach of developing curated\nsynthetic data on a large scale, with specific properties, and using them to\nstudy sentence representations built using pretrained language models. We use a\nnew multiple-choice task and datasets, Blackbird Language Matrices (BLMs), to\nfocus on a specific grammatical structural phenomenon -- subject-verb agreement\nacross a variety of sentence structures -- in several languages. Finding a\nsolution to this task requires a system detecting complex linguistic patterns\nand paradigms in text representations. Using a two-level architecture that\nsolves the problem in two steps -- detect syntactic objects and their\nproperties in individual sentences, and find patterns across an input sequence\nof sentences -- we show that despite having been trained on multilingual texts\nin a consistent manner, multilingual pretrained language models have\nlanguage-specific differences, and syntactic structure is not shared, even\nacross closely related languages.\n","authors":["Vivi Nastase","Chunyang Jiang","Giuseppe Samo","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2409.06567v1.pdf","comment":"11 pages, 5 tables, 5 figures"},{"id":"http://arxiv.org/abs/2409.03701v2","updated":"2024-09-10T14:45:15Z","published":"2024-09-05T16:57:39Z","title":"LAST: Language Model Aware Speech Tokenization","summary":"  Speech tokenization serves as the foundation of speech language model (LM),\nenabling them to perform various tasks such as spoken language modeling,\ntext-to-speech, speech-to-text, etc. Most speech tokenizers are trained\nindependently of the LM training process, relying on separate acoustic models\nand quantization methods. Following such an approach may create a mismatch\nbetween the tokenization process and its usage afterward. In this study, we\npropose a novel approach to training a speech tokenizer by leveraging\nobjectives from pre-trained textual LMs. We advocate for the integration of\nthis objective into the process of learning discrete speech representations.\nOur aim is to transform features from a pre-trained speech model into a new\nfeature space that enables better clustering for speech LMs. We empirically\ninvestigate the impact of various model design choices, including speech\nvocabulary size and text LM size. Our results demonstrate the proposed\ntokenization method outperforms the evaluated baselines considering both spoken\nlanguage modeling and speech-to-text. More importantly, unlike prior work, the\nproposed method allows the utilization of a single pre-trained LM for\nprocessing both speech and text inputs, setting it apart from conventional\ntokenization approaches.\n","authors":["Arnon Turetzky","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2409.03701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06550v1","updated":"2024-09-10T14:26:12Z","published":"2024-09-10T14:26:12Z","title":"From LIMA to DeepLIMA: following a new path of interoperability","summary":"  In this article, we describe the architecture of the LIMA (Libre Multilingual\nAnalyzer) framework and its recent evolution with the addition of new text\nanalysis modules based on deep neural networks. We extended the functionality\nof LIMA in terms of the number of supported languages while preserving existing\nconfigurable architecture and the availability of previously developed\nrule-based and statistical analysis components. Models were trained for more\nthan 60 languages on the Universal Dependencies 2.5 corpora, WikiNer corpora,\nand CoNLL-03 dataset. Universal Dependencies allowed us to increase the number\nof supported languages and to generate models that could be integrated into\nother platforms. This integration of ubiquitous Deep Learning Natural Language\nProcessing models and the use of standard annotated collections using Universal\nDependencies can be viewed as a new path of interoperability, through the\nnormalization of models and data, that are complementary to a more standard\ntechnical interoperability, implemented in LIMA through services available in\nDocker containers on Docker Hub.\n","authors":["Victor Bocharov","Romaric Besançon","Gaël de Chalendar","Olivier Ferret","Nasredine Semmar"],"pdf_url":"https://arxiv.org/pdf/2409.06550v1.pdf","comment":"16 pages, 5 figures, submitted to Language Resources and Evaluation"},{"id":"http://arxiv.org/abs/2409.06540v1","updated":"2024-09-10T14:15:30Z","published":"2024-09-10T14:15:30Z","title":"Mapping News Narratives Using LLMs and Narrative-Structured Text\n  Embeddings","summary":"  Given the profound impact of narratives across various societal levels, from\npersonal identities to international politics, it is crucial to understand\ntheir distribution and development over time. This is particularly important in\nonline spaces. On the Web, narratives can spread rapidly and intensify societal\ndivides and conflicts. While many qualitative approaches exist, quantifying\nnarratives remains a significant challenge. Computational narrative analysis\nlacks frameworks that are both comprehensive and generalizable. To address this\ngap, we introduce a numerical narrative representation grounded in\nstructuralist linguistic theory. Chiefly, Greimas' Actantial Model represents a\nnarrative through a constellation of six functional character roles. These\nso-called actants are genre-agnostic, making the model highly generalizable. We\nextract the actants using an open-source LLM and integrate them into a\nNarrative-Structured Text Embedding that captures both the semantics and\nnarrative structure of a text. We demonstrate the analytical insights of the\nmethod on the example of 5000 full-text news articles from Al Jazeera and The\nWashington Post on the Israel-Palestine conflict. Our method successfully\ndistinguishes articles that cover the same topics but differ in narrative\nstructure.\n","authors":["Jan Elfes"],"pdf_url":"https://arxiv.org/pdf/2409.06540v1.pdf","comment":"19 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.14467v2","updated":"2024-09-10T14:08:29Z","published":"2024-07-19T17:14:16Z","title":"Check-Eval: A Checklist-based Approach for Evaluating Text Quality","summary":"  Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose \\textsc{Check-Eval}, a novel evaluation framework\nleveraging LLMs to assess the quality of generated text through a\nchecklist-based approach. \\textsc{Check-Eval} can be employed as both a\nreference-free and reference-dependent evaluation method, providing a\nstructured and interpretable assessment of text quality. The framework consists\nof two main stages: checklist generation and checklist evaluation. We validate\n\\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic\nTextual Similarity and \\textsc{SummEval}. Our results demonstrate that\n\\textsc{Check-Eval} achieves higher correlations with human judgments compared\nto existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore},\nunderscoring its potential as a more reliable and effective evaluation\nframework for natural language generation tasks. The code for our experiments\nis available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}\n","authors":["Jayr Pereira","Andre Assumpcao","Roberto Lotufo"],"pdf_url":"https://arxiv.org/pdf/2407.14467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06518v1","updated":"2024-09-10T13:54:04Z","published":"2024-09-10T13:54:04Z","title":"Questioning Internal Knowledge Structure of Large Language Models\n  Through the Lens of the Olympic Games","summary":"  Large language models (LLMs) have become a dominant approach in natural\nlanguage processing, yet their internal knowledge structures remain largely\nunexplored. In this paper, we analyze the internal knowledge structures of LLMs\nusing historical medal tallies from the Olympic Games. We task the models with\nproviding the medal counts for each team and identifying which teams achieved\nspecific rankings. Our results reveal that while state-of-the-art LLMs perform\nremarkably well in reporting medal counts for individual teams, they struggle\nsignificantly with questions about specific rankings. This suggests that the\ninternal knowledge structures of LLMs are fundamentally different from those of\nhumans, who can easily infer rankings from known medal counts. To support\nfurther research, we publicly release our code, dataset, and model outputs.\n","authors":["Juhwan Choi","YoungBin Kim"],"pdf_url":"https://arxiv.org/pdf/2409.06518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07606v2","updated":"2024-09-10T13:39:28Z","published":"2023-09-14T11:13:36Z","title":"Zero-shot Audio Topic Reranking using Large Language Models","summary":"  Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.\n","authors":["Mengjie Qian","Rao Ma","Adian Liusie","Erfan Loweimi","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2309.07606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01808v2","updated":"2024-09-10T13:33:46Z","published":"2024-09-03T11:40:38Z","title":"Dialogue You Can Trust: Human and AI Perspectives on Generated\n  Conversations","summary":"  As dialogue systems and chatbots increasingly integrate into everyday\ninteractions, the need for efficient and accurate evaluation methods becomes\nparamount. This study explores the comparative performance of human and AI\nassessments across a range of dialogue scenarios, focusing on seven key\nperformance indicators (KPIs): Coherence, Innovation, Concreteness, Goal\nContribution, Commonsense Contradiction, Incorrect Fact, and Redundancy.\nUtilizing the GPT-4o API, we generated a diverse dataset of conversations and\nconducted a two-part experimental analysis. In Experiment 1, we evaluated\nmulti-party conversations on Coherence, Innovation, Concreteness, and Goal\nContribution, revealing that GPT models align closely with human judgments.\nNotably, both human and AI evaluators exhibited a tendency towards binary\njudgment rather than linear scaling, highlighting a shared challenge in these\nassessments. Experiment 2 extended the work of Finch et al. (2023) by focusing\non dyadic dialogues and assessing Commonsense Contradiction, Incorrect Fact,\nand Redundancy. The results indicate that while GPT-4o demonstrates strong\nperformance in maintaining factual accuracy and commonsense reasoning, it still\nstruggles with reducing redundancy and self-contradiction. Our findings\nunderscore the potential of GPT models to closely replicate human evaluation in\ndialogue systems, while also pointing to areas for improvement. This research\noffers valuable insights for advancing the development and implementation of\nmore refined dialogue evaluation methodologies, contributing to the evolution\nof more effective and human-like AI communication tools.\n","authors":["Ike Ebubechukwu","Johane Takeuchi","Antonello Ceravola","Frank Joublin"],"pdf_url":"https://arxiv.org/pdf/2409.01808v2.pdf","comment":"17 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.10671v4","updated":"2024-09-10T13:25:53Z","published":"2024-07-15T12:35:42Z","title":"Qwen2 Technical Report","summary":"  This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.\n","authors":["An Yang","Baosong Yang","Binyuan Hui","Bo Zheng","Bowen Yu","Chang Zhou","Chengpeng Li","Chengyuan Li","Dayiheng Liu","Fei Huang","Guanting Dong","Haoran Wei","Huan Lin","Jialong Tang","Jialin Wang","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Ma","Jianxin Yang","Jin Xu","Jingren Zhou","Jinze Bai","Jinzheng He","Junyang Lin","Kai Dang","Keming Lu","Keqin Chen","Kexin Yang","Mei Li","Mingfeng Xue","Na Ni","Pei Zhang","Peng Wang","Ru Peng","Rui Men","Ruize Gao","Runji Lin","Shijie Wang","Shuai Bai","Sinan Tan","Tianhang Zhu","Tianhao Li","Tianyu Liu","Wenbin Ge","Xiaodong Deng","Xiaohuan Zhou","Xingzhang Ren","Xinyu Zhang","Xipin Wei","Xuancheng Ren","Xuejing Liu","Yang Fan","Yang Yao","Yichang Zhang","Yu Wan","Yunfei Chu","Yuqiong Liu","Zeyu Cui","Zhenru Zhang","Zhifang Guo","Zhihao Fan"],"pdf_url":"https://arxiv.org/pdf/2407.10671v4.pdf","comment":"26 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.05211v2","updated":"2024-09-10T13:21:08Z","published":"2024-08-09T17:59:49Z","title":"VITA: Towards Open-Source Interactive Omni Multimodal LLM","summary":"  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.\n","authors":["Chaoyou Fu","Haojia Lin","Zuwei Long","Yunhang Shen","Meng Zhao","Yifan Zhang","Shaoqi Dong","Xiong Wang","Di Yin","Long Ma","Xiawu Zheng","Ran He","Rongrong Ji","Yunsheng Wu","Caifeng Shan","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.05211v2.pdf","comment":"Project Page: https://vita-home.github.io"},{"id":"http://arxiv.org/abs/2409.00222v2","updated":"2024-09-10T12:57:19Z","published":"2024-08-30T19:26:15Z","title":"Can Large Language Models Address Open-Target Stance Detection?","summary":"  Stance detection (SD) assesses a text's position towards a target, typically\nlabeled as \"favor,\" \"against,\" or \"neutral.\" We introduce Open-Target Stance\nDetection (OTSD), where targets are neither seen during training nor provided\nas input. Evaluating Large Language Models (LLMs) like GPT-3.5, GPT-4o, Llama\n3, and Mistral, we compare their performance with the Target-Stance Extraction\n(TSE) approach, which has the advantage of using predefined targets. LLMs\nperform better than TSE in target generation when the real target is explicitly\nand not explicitly mentioned in the text. For stance detection, LLMs perform\nbetter in explicit scenarios but fail in non-explicit ones.\n","authors":["Abu Ubaida Akash","Ahmed Fahmy","Amine Trabelsi"],"pdf_url":"https://arxiv.org/pdf/2409.00222v2.pdf","comment":"12 pages; currently under submission"},{"id":"http://arxiv.org/abs/2409.02813v2","updated":"2024-09-10T12:55:31Z","published":"2024-09-04T15:31:26Z","title":"MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI.\n","authors":["Xiang Yue","Tianyu Zheng","Yuansheng Ni","Yubo Wang","Kai Zhang","Shengbang Tong","Yuxuan Sun","Botao Yu","Ge Zhang","Huan Sun","Yu Su","Wenhu Chen","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2409.02813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06468v1","updated":"2024-09-10T12:52:36Z","published":"2024-09-10T12:52:36Z","title":"An Effective Context-Balanced Adaptation Approach for Long-Tailed Speech\n  Recognition","summary":"  End-to-end (E2E) automatic speech recognition (ASR) models have become\nstandard practice for various commercial applications. However, in real-world\nscenarios, the long-tailed nature of word distribution often leads E2E ASR\nmodels to perform well on common words but fall short in recognizing uncommon\nones. Recently, the notion of a contextual adapter (CA) was proposed to infuse\nexternal knowledge represented by a context word list into E2E ASR models.\nAlthough CA can improve recognition performance on rare words, two crucial data\nimbalance problems remain. First, when using low-frequency words as context\nwords during training, since these words rarely occur in the utterance, CA\nbecomes prone to overfit on attending to the <no-context> token due to\nhigher-frequency words not being present in the context list. Second, the\nlong-tailed distribution within the context list itself still causes the model\nto perform poorly on low-frequency context words. In light of this, we explore\nin-depth the impact of altering the context list to have words with different\nfrequency distributions on model performance, and meanwhile extend CA with a\nsimple yet effective context-balanced learning objective. A series of\nexperiments conducted on the AISHELL-1 benchmark dataset suggests that using\nall vocabulary words from the training corpus as the context list and pairing\nthem with our balanced objective yields the best performance, demonstrating a\nsignificant reduction in character error rate (CER) by up to 1.21% and a more\npronounced 9.44% reduction in the error rate of zero-shot words.\n","authors":["Yi-Cheng Wang","Li-Ting Pai","Bi-Cheng Yan","Hsin-Wei Wang","Chi-Han Lin","Berlin Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06468v1.pdf","comment":"Accepted by SLT 2024"},{"id":"http://arxiv.org/abs/2409.06446v1","updated":"2024-09-10T12:01:43Z","published":"2024-09-10T12:01:43Z","title":"HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data","summary":"  Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.\n","authors":["Hossein Hajipour","Lea Schönherr","Thorsten Holz","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2409.06446v1.pdf","comment":"24 pages, 16 tables, 8 figures"},{"id":"http://arxiv.org/abs/2409.06411v1","updated":"2024-09-10T10:49:38Z","published":"2024-09-10T10:49:38Z","title":"Length Desensitization in Directed Preference Optimization","summary":"  Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.\n","authors":["Wei Liu","Yang Bai","Chengcheng Han","Rongxiang Weng","Jun Xu","Xuezhi Cao","Jingang Wang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2409.06411v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2210.11984v2","updated":"2024-09-10T10:48:57Z","published":"2022-10-21T14:19:47Z","title":"Shift-Reduce Task-Oriented Semantic Parsing with Stack-Transformers","summary":"  Intelligent voice assistants, such as Apple Siri and Amazon Alexa, are widely\nused nowadays. These task-oriented dialogue systems require a semantic parsing\nmodule in order to process user utterances and understand the action to be\nperformed. This semantic parsing component was initially implemented by\nrule-based or statistical slot-filling approaches for processing simple\nqueries; however, the appearance of more complex utterances demanded the\napplication of shift-reduce parsers or sequence-to-sequence models. Although\nshift-reduce approaches were initially considered the most promising option,\nthe emergence of sequence-to-sequence neural systems has propelled them to the\nforefront as the highest-performing method for this particular task. In this\narticle, we advance the research on shift-reduce semantic parsing for\ntask-oriented dialogue. We implement novel shift-reduce parsers that rely on\nStack-Transformers. This framework allows to adequately model transition\nsystems on the Transformer neural architecture, notably boosting shift-reduce\nparsing performance. Furthermore, our approach goes beyond the conventional\ntop-down algorithm: we incorporate alternative bottom-up and in-order\ntransition systems derived from constituency parsing into the realm of\ntask-oriented parsing. We extensively test our approach on multiple domains\nfrom the Facebook TOP benchmark, improving over existing shift-reduce parsers\nand state-of-the-art sequence-to-sequence models in both high-resource and\nlow-resource settings. We also empirically prove that the in-order algorithm\nsubstantially outperforms the commonly-used top-down strategy. Through the\ncreation of innovative transition systems and harnessing the capabilities of a\nrobust neural architecture, our study showcases the superiority of shift-reduce\nparsers over leading sequence-to-sequence methods on the main benchmark.\n","authors":["Daniel Fernández-González"],"pdf_url":"https://arxiv.org/pdf/2210.11984v2.pdf","comment":"Final peer-reviewed manuscript accepted for publication in Cognitive\n  Computation"},{"id":"http://arxiv.org/abs/2409.06386v1","updated":"2024-09-10T10:08:58Z","published":"2024-09-10T10:08:58Z","title":"Coarse-Grained Sense Inventories Based on Semantic Matching between\n  English Dictionaries","summary":"  WordNet is one of the largest handcrafted concept dictionaries visualizing\nword connections through semantic relationships. It is widely used as a word\nsense inventory in natural language processing tasks. However, WordNet's\nfine-grained senses have been criticized for limiting its usability. In this\npaper, we semantically match sense definitions from Cambridge dictionaries and\nWordNet and develop new coarse-grained sense inventories. We verify the\neffectiveness of our inventories by comparing their semantic coherences with\nthat of Coarse Sense Inventory. The advantages of the proposed inventories\ninclude their low dependency on large-scale resources, better aggregation of\nclosely related senses, CEFR-level assignments, and ease of expansion and\nimprovement.\n","authors":["Masato Kikuchi","Masatsugu Ono","Toshioki Soga","Tetsu Tanabe","Tadachika Ozono"],"pdf_url":"https://arxiv.org/pdf/2409.06386v1.pdf","comment":"The 11th International Conference on Advanced Informatics: Concepts,\n  Theory and Applications (ICAICTA 2024)"},{"id":"http://arxiv.org/abs/2409.06377v1","updated":"2024-09-10T09:58:55Z","published":"2024-09-10T09:58:55Z","title":"Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration","summary":"  Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Xiao Zhang","Ming He","Jianping Fan","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06377v1.pdf","comment":"First 3 authors contributes equally to this work"},{"id":"http://arxiv.org/abs/2409.06372v1","updated":"2024-09-10T09:56:15Z","published":"2024-09-10T09:56:15Z","title":"SpeechTaxi: On Multilingual Semantic Speech Classification","summary":"  Recent advancements in multilingual speech encoding as well as transcription\nraise the question of the most effective approach to semantic speech\nclassification. Concretely, can (1) end-to-end (E2E) classifiers obtained by\nfine-tuning state-of-the-art multilingual speech encoders (MSEs) match or\nsurpass the performance of (2) cascading (CA), where speech is first\ntranscribed into text and classification is delegated to a text-based\nclassifier. To answer this, we first construct SpeechTaxi, an 80-hour\nmultilingual dataset for semantic speech classification of Bible verses,\ncovering 28 diverse languages. We then leverage SpeechTaxi to conduct a wide\nrange of experiments comparing E2E and CA in monolingual semantic speech\nclassification as well as in cross-lingual transfer. We find that E2E based on\nMSEs outperforms CA in monolingual setups, i.e., when trained on in-language\ndata. However, MSEs seem to have poor cross-lingual transfer abilities, with\nE2E substantially lagging CA both in (1) zero-shot transfer to languages unseen\nin training and (2) multilingual training, i.e., joint training on multiple\nlanguages. Finally, we devise a novel CA approach based on transcription to\nRomanized text as a language-agnostic intermediate representation and show that\nit represents a robust solution for languages without native ASR support. Our\nSpeechTaxi dataset is publicly available at: https://huggingface.co/\ndatasets/LennartKeller/SpeechTaxi/.\n","authors":["Lennart Keller","Goran Glavaš"],"pdf_url":"https://arxiv.org/pdf/2409.06372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03454v2","updated":"2024-09-10T09:22:26Z","published":"2024-09-05T12:06:38Z","title":"How Much Data is Enough Data? Fine-Tuning Large Language Models for\n  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes","summary":"  Decoder-only LLMs have shown impressive performance in MT due to their\nability to learn from extensive datasets and generate high-quality\ntranslations. However, LLMs often struggle with the nuances and style required\nfor organisation-specific translation. In this study, we explore the\neffectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3\n8B Instruct, leveraging translation memories (TMs), as a valuable resource to\nenhance accuracy and efficiency. We investigate the impact of fine-tuning the\nLlama 3 model using TMs from a specific organisation in the software sector.\nOur experiments cover five translation directions across languages of varying\nresource levels (English to Brazilian Portuguese, Czech, German, Finnish, and\nKorean). We analyse diverse sizes of training datasets (1k to 207k segments) to\nevaluate their influence on translation quality. We fine-tune separate models\nfor each training set and evaluate their performance based on automatic\nmetrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in\ntranslation performance with larger datasets across all metrics. On average,\nBLEU and COMET scores increase by 13 and 25 points, respectively, on the\nlargest training set against the baseline model. Notably, there is a\nperformance deterioration in comparison with the baseline model when\nfine-tuning on only 1k and 2k examples; however, we observe a substantial\nimprovement as the training dataset size increases. The study highlights the\npotential of integrating TMs with LLMs to create bespoke translation models\ntailored to the specific needs of businesses, thus enhancing translation\nquality and reducing turn-around times. This approach offers a valuable insight\nfor organisations seeking to leverage TMs and LLMs for optimal translation\noutcomes, especially in narrower domains.\n","authors":["Inacio Vieira","Will Allred","Séamus Lankford","Sheila Castilho","Andy Way"],"pdf_url":"https://arxiv.org/pdf/2409.03454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08263v2","updated":"2024-09-10T09:14:49Z","published":"2024-04-12T06:23:07Z","title":"Relational Prompt-based Pre-trained Language Models for Social Event\n  Detection","summary":"  Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with missing and noisy edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.\n","authors":["Pu Li","Xiaoyan Yu","Hao Peng","Yantuan Xian","Linqin Wang","Li Sun","Jingyun Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08263v2.pdf","comment":"ACM TOIS"},{"id":"http://arxiv.org/abs/2409.06338v1","updated":"2024-09-10T08:48:05Z","published":"2024-09-10T08:48:05Z","title":"Retrieval Or Holistic Understanding? Dolce: Differentiate Our Long\n  Context Evaluation Tasks","summary":"  We argue that there are two major distinct capabilities in long context\nunderstanding: retrieval and holistic understanding. Understanding and further\nimproving LLMs' long context capabilities would not be possible without knowing\nthe tasks' focus categories. We aim to automatically identify retrieval focused\nand holistic understanding focused problems from suites of benchmarks and\nquantitatively measure the difficulty within each focus. In this paper, we\npresent the Dolce framework, which parameterizes each problem by $\\lambda$\n(complexity) and $k$ (redundancy) and assigns to one of five predefined focus\ncategories. We propose to sample short contexts from the full context and\nestimate the probability an LLM solves the problem using the sampled spans. To\nfind the $\\lambda$ and $k$ for each problem, we further propose a mixture model\nof a non-parametric background noise component and a parametric/non-parametric\nhybrid oracle component, where we derive the probability functions\nparameterized by $\\lambda$ and $k$ for both the correct-or-wrong (COW) scenario\nand the partial-point-in-grading (PIG) scenario. Our proposed methods can\nidentify 0% to 67% of the problems are retrieval focused and 0% to 90% of the\nproblems are holistic understanding focused across 44 existing long context\nevaluation tasks.\n","authors":["Zi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.06338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06328v1","updated":"2024-09-10T08:33:31Z","published":"2024-09-10T08:33:31Z","title":"Extracting Paragraphs from LLM Token Activations","summary":"  Generative large language models (LLMs) excel in natural language processing\ntasks, yet their inner workings remain underexplored beyond token-level\npredictions. This study investigates the degree to which these models decide\nthe content of a paragraph at its onset, shedding light on their contextual\nunderstanding. By examining the information encoded in single-token\nactivations, specifically the \"\\textbackslash n\\textbackslash n\" double newline\ntoken, we demonstrate that patching these activations can transfer significant\ninformation about the context of the following paragraph, providing further\ninsights into the model's capacity to plan ahead.\n","authors":["Nicholas Pochinkov","Angelo Benoit","Lovkush Agarwal","Zainab Ali Majid","Lucile Ter-Minassian"],"pdf_url":"https://arxiv.org/pdf/2409.06328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20684v2","updated":"2024-09-10T08:19:08Z","published":"2024-05-31T08:26:47Z","title":"Joint Embeddings for Graph Instruction Tuning","summary":"  Large Language Models (LLMs) have achieved impressive performance in text\nunderstanding and have become an essential tool for building smart assistants.\nOriginally focusing on text, they have been enhanced with multimodal\ncapabilities in recent works that successfully built visual instruction\nfollowing assistants. As far as the graph modality goes, however, no such\nassistants have yet been developed. Graph structures are complex in that they\nrepresent relation between different features and are permutation invariant.\nMoreover, representing them in purely textual form does not always lead to good\nLLM performance even for finetuned models. As a result, there is a need to\ndevelop a new method to integrate graphs in LLMs for general graph\nunderstanding. This work explores the integration of the graph modality in LLM\nfor general graph instruction following tasks. It aims at producing a deep\nlearning model that enhances an underlying LLM with graph embeddings and trains\nit to understand them and to produce, given an instruction, an answer grounded\nin the graph representation. The approach performs significantly better than a\ngraph to text approach and remains consistent even for larger graphs.\n","authors":["Aaron Haag","Vlad Argatu","Oliver Lohse"],"pdf_url":"https://arxiv.org/pdf/2405.20684v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.13375v2","updated":"2024-09-10T08:08:40Z","published":"2024-06-19T09:16:14Z","title":"ALiiCE: Evaluating Positional Fine-grained Citation Generation","summary":"  Large Language Models (LLMs) can enhance the credibility and verifiability by\ngenerating text with citations. However, existing tasks and evaluation methods\nare predominantly limited to sentence-level statement, neglecting the\nsignificance of positional fine-grained citations that can appear anywhere\nwithin sentences. To facilitate further exploration of the fine-grained\ncitation generation, we propose ALiiCE, the first automatic evaluation\nframework for this task. Our framework first parses the sentence claim into\natomic claims via dependency analysis and then calculates citation quality at\nthe atomic claim level. ALiiCE introduces three novel metrics for positional\nfined-grained citation quality assessment, including positional fine-grained\ncitation recall and precision, and coefficient of variation of citation\npositions. We evaluate the positional fine-grained citation generation\nperformance of several LLMs on two long-form QA datasets. Our experiments and\nanalyses demonstrate the effectiveness and reasonableness of ALiiCE. The\nresults also indicate that existing LLMs still struggle to provide positional\nfine-grained citations.\n","authors":["Yilong Xu","Jinhua Gao","Xiaoming Yu","Baolong Bi","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.13375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02897v3","updated":"2024-09-10T07:43:19Z","published":"2024-09-04T17:41:19Z","title":"LongCite: Enabling LLMs to Generate Fine-grained Citations in\n  Long-context QA","summary":"  Though current long-context large language models (LLMs) have demonstrated\nimpressive capacities in answering user questions based on extensive text, the\nlack of citations in their responses makes user verification difficult, leading\nto concerns about their trustworthiness due to their potential hallucinations.\nIn this work, we aim to enable long-context LLMs to generate responses with\nfine-grained sentence-level citations, improving their faithfulness and\nverifiability. We first introduce LongBench-Cite, an automated benchmark for\nassessing current LLMs' performance in Long-Context Question Answering with\nCitations (LQAC), revealing considerable room for improvement. To this end, we\npropose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs\nto automatically generate long-context QA instances with precise sentence-level\ncitations, and leverage this pipeline to construct LongCite-45k, a large-scale\nSFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the\nLongCite-45k dataset, successfully enabling their generation of accurate\nresponses and fine-grained sentence-level citations in a single output. The\nevaluation results on LongBench-Cite show that our trained models achieve\nstate-of-the-art citation quality, surpassing advanced proprietary models\nincluding GPT-4o.\n","authors":["Jiajie Zhang","Yushi Bai","Xin Lv","Wanjun Gu","Danqing Liu","Minhao Zou","Shulin Cao","Lei Hou","Yuxiao Dong","Ling Feng","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2409.02897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06263v1","updated":"2024-09-10T07:06:40Z","published":"2024-09-10T07:06:40Z","title":"Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking","summary":"  Dialogue State Tracking (DST) is a key part of task-oriented dialogue\nsystems, identifying important information in conversations. However, its\naccuracy drops significantly in spoken dialogue environments due to named\nentity errors from Automatic Speech Recognition (ASR) systems. We introduce a\nsimple yet effective data augmentation method that targets those entities to\nimprove the robustness of DST model. Our novel method can control the placement\nof errors using keyword-highlighted prompts while introducing phonetically\nsimilar errors. As a result, our method generated sufficient error patterns on\nkeywords, leading to improved accuracy in noised and low-accuracy ASR\nenvironments.\n","authors":["Jihyun Lee","Solee Im","Wonjun Lee","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v3","updated":"2024-09-10T06:36:25Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking Neural Networks (SNNs) have emerged as a promising alternative to\nconventional Artificial Neural Networks (ANNs), demonstrating comparable\nperformance in both visual and linguistic tasks while offering the advantage of\nimproved energy efficiency. Despite these advancements, the integration of\nlinguistic and visual features into a unified representation through spike\ntrains poses a significant challenge, and the application of SNNs to multimodal\nscenarios remains largely unexplored. This paper presents SpikeCLIP, a novel\nframework designed to bridge the modality gap in spike-based computation. Our\napproach employs a two-step recipe: an ``alignment pre-training'' to align\nfeatures across modalities, followed by a ``dual-loss fine-tuning'' to refine\nthe model's performance. Extensive experiments reveal that SNNs achieve results\non par with ANNs while substantially reducing energy consumption across various\ndatasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP\nmaintains robust image classification capabilities, even when dealing with\nclasses that fall outside predefined categories. This study marks a significant\nadvancement in the development of energy-efficient and biologically plausible\nmultimodal learning systems.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Yufei Gu","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06243v1","updated":"2024-09-10T06:24:46Z","published":"2024-09-10T06:24:46Z","title":"Inference is All You Need: Self Example Retriever for Cross-domain\n  Dialogue State Tracking with ChatGPT","summary":"  Traditional dialogue state tracking approaches heavily rely on extensive\ntraining data and handcrafted features, limiting their scalability and\nadaptability to new domains. In this paper, we propose a novel method that\nleverages inference and in-context learning with ChatGPT for domain transfer in\ndialogue state tracking, without any parameter updates. By guiding ChatGPT's\nchain of thought, we enable it to retrieve relevant examples and generalize\nknowledge to accurately infer dialogue states, solely through inference.\nExperimental results on the MultiWOZ dataset demonstrate competitive\nperformance and promising generalization across domains. Our parameter-free\napproach offers a scalable and adaptable solution, opening new research\ndirections in domain transfer learning.\n","authors":["Jihyun Lee","Gary Geunbae Lee"],"pdf_url":"https://arxiv.org/pdf/2409.06243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05385v2","updated":"2024-09-10T06:11:28Z","published":"2024-09-09T07:32:30Z","title":"Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models","summary":"  The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.\n","authors":["Hong Xingyun Hong","Shao Yan Shao","Wang Zhilin Wang","Duan Manni Duan","Jin Xiongnan"],"pdf_url":"https://arxiv.org/pdf/2409.05385v2.pdf","comment":"This paper has been accepted by NLPCC-2024"},{"id":"http://arxiv.org/abs/2409.06226v1","updated":"2024-09-10T05:41:40Z","published":"2024-09-10T05:41:40Z","title":"NLP-Powered Repository and Search Engine for Academic Papers: A Case\n  Study on Cyber Risk Literature with CyLit","summary":"  As the body of academic literature continues to grow, researchers face\nincreasing difficulties in effectively searching for relevant resources.\nExisting databases and search engines often fall short of providing a\ncomprehensive and contextually relevant collection of academic literature. To\naddress this issue, we propose a novel framework that leverages Natural\nLanguage Processing (NLP) techniques. This framework automates the retrieval,\nsummarization, and clustering of academic literature within a specific research\ndomain. To demonstrate the effectiveness of our approach, we introduce CyLit,\nan NLP-powered repository specifically designed for the cyber risk literature.\nCyLit empowers researchers by providing access to context-specific resources\nand enabling the tracking of trends in the dynamic and rapidly evolving field\nof cyber risk. Through the automatic processing of large volumes of data, our\nNLP-powered solution significantly enhances the efficiency and specificity of\nacademic literature searches. We compare the literature categorization results\nof CyLit to those presented in survey papers or generated by ChatGPT,\nhighlighting the distinctive insights this tool provides into cyber risk\nresearch literature. Using NLP techniques, we aim to revolutionize the way\nresearchers discover, analyze, and utilize academic resources, ultimately\nfostering advancements in various domains of knowledge.\n","authors":["Linfeng Zhang","Changyue Hu","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2409.06226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06223v1","updated":"2024-09-10T05:26:53Z","published":"2024-09-10T05:26:53Z","title":"Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models","summary":"  The Audio Question Answering task includes audio event classification, audio\ncaptioning, and open ended reasoning. Recently, Audio Question Answering has\ngarnered attention due to the advent of Large Audio Language Models. Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext only Large Language Models through a projection module. While Large Audio\nLanguage Models excel in general audio understanding, they are limited in\ntemporal reasoning which may hinder their commercial applications and on device\ndeployment. This paper addresses these challenges and limitations in audio\ntemporal reasoning. First, we introduce a data augmentation technique for\ngenerating reliable audio temporal questions and answers using an LLM. Second,\nwe propose a continued finetuning curriculum learning strategy to specialize in\ntemporal reasoning without compromising performance on finetuned tasks.\nFinally, we develop a reliable and transparent automated metric, assisted by an\nLLM, to measure the correlation between Large Audio Language Model responses\nand ground truth data intelligently. We demonstrate the effectiveness of our\nproposed techniques using SOTA LALMs on public audio benchmark datasets.\n","authors":["Arvind Krishna Sridhar","Yinyi Guo","Erik Visser"],"pdf_url":"https://arxiv.org/pdf/2409.06223v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.06222v1","updated":"2024-09-10T05:24:36Z","published":"2024-09-10T05:24:36Z","title":"Advancing Topic Segmentation of Broadcasted Speech with Multilingual\n  Semantic Embeddings","summary":"  Recent advancements in speech-based topic segmentation have highlighted the\npotential of pretrained speech encoders to capture semantic representations\ndirectly from speech. Traditionally, topic segmentation has relied on a\npipeline approach in which transcripts of the automatic speech recognition\nsystems are generated, followed by text-based segmentation algorithms. In this\npaper, we introduce an end-to-end scheme that bypasses this conventional\ntwo-step process by directly employing semantic speech encoders for\nsegmentation. Focused on the broadcasted news domain, which poses unique\nchallenges due to the diversity of speakers and topics within single\nrecordings, we address the challenge of accessing topic change points\nefficiently in an end-to-end manner. Furthermore, we propose a new benchmark\nfor spoken news topic segmentation by utilizing a dataset featuring\napproximately 1000 hours of publicly available recordings across six European\nlanguages and including an evaluation set in Hindi to test the model's\ncross-domain performance in a cross-lingual, zero-shot scenario. This setup\nreflects real-world diversity and the need for models adapting to various\nlinguistic settings. Our results demonstrate that while the traditional\npipeline approach achieves a state-of-the-art $P_k$ score of 0.2431 for\nEnglish, our end-to-end model delivers a competitive $P_k$ score of 0.2564.\nWhen trained multilingually, these scores further improve to 0.1988 and 0.2370,\nrespectively. To support further research, we release our model along with data\npreparation scripts, facilitating open research on multilingual spoken news\ntopic segmentation.\n","authors":["Sakshi Deo Shukla","Pavel Denisov","Tugtekin Turan"],"pdf_url":"https://arxiv.org/pdf/2409.06222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06216v1","updated":"2024-09-10T04:48:36Z","published":"2024-09-10T04:48:36Z","title":"SubRegWeigh: Effective and Efficient Annotation Weighing with Subword\n  Regularization","summary":"  Many datasets of natural language processing (NLP) sometimes include\nannotation errors. Researchers have attempted to develop methods to reduce the\nadverse effect of errors in datasets automatically. However, an existing method\nis time-consuming because it requires many trained models to detect errors. We\npropose a novel method to reduce the time of error detection. Specifically, we\nuse a tokenization technique called subword regularization to create\npseudo-multiple models which are used to detect errors. Our proposed method,\nSubRegWeigh, can perform annotation weighting four to five times faster than\nthe existing method. Additionally, SubRegWeigh improved performance in both\ndocument classification and named entity recognition tasks. In experiments with\npseudo-incorrect labels, pseudo-incorrect labels were adequately detected.\n","authors":["Kohei Tsuji","Tatsuya Hiraoka","Yuchang Cheng","Tomoya Iwakura"],"pdf_url":"https://arxiv.org/pdf/2409.06216v1.pdf","comment":"14 pages, 1 figures, 10 tables"},{"id":"http://arxiv.org/abs/2409.06211v1","updated":"2024-09-10T04:34:42Z","published":"2024-09-10T04:34:42Z","title":"STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning","summary":"  Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.\n","authors":["Jaeseong Lee","seung-won hwang","Aurick Qiao","Daniel F Campos","Zhewei Yao","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2409.06211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06205v1","updated":"2024-09-10T04:18:49Z","published":"2024-09-10T04:18:49Z","title":"SHAPE-IT: Exploring Text-to-Shape-Display for Generative Shape-Changing\n  Behaviors with LLMs","summary":"  This paper introduces text-to-shape-display, a novel approach to generating\ndynamic shape changes in pin-based shape displays through natural language\ncommands. By leveraging large language models (LLMs) and AI-chaining, our\napproach allows users to author shape-changing behaviors on demand through text\nprompts without programming. We describe the foundational aspects necessary for\nsuch a system, including the identification of key generative elements\n(primitive, animation, and interaction) and design requirements to enhance user\ninteraction, based on formative exploration and iterative design processes.\nBased on these insights, we develop SHAPE-IT, an LLM-based authoring tool for a\n24 x 24 shape display, which translates the user's textual command into\nexecutable code and allows for quick exploration through a web-based control\ninterface. We evaluate the effectiveness of SHAPE-IT in two ways: 1)\nperformance evaluation and 2) user evaluation (N= 10). The study conclusions\nhighlight the ability to facilitate rapid ideation of a wide range of\nshape-changing behaviors with AI. However, the findings also expose\naccuracy-related challenges and limitations, prompting further exploration into\nrefining the framework for leveraging AI to better suit the unique requirements\nof shape-changing systems.\n","authors":["Wanli Qian","Chenfeng Gao","Anup Sathya","Ryo Suzuki","Ken Nakagaki"],"pdf_url":"https://arxiv.org/pdf/2409.06205v1.pdf","comment":"Accepted for ACM UIST 2024"},{"id":"http://arxiv.org/abs/2409.06192v1","updated":"2024-09-10T03:43:26Z","published":"2024-09-10T03:43:26Z","title":"NOVI : Chatbot System for University Novice with BERT and LLMs","summary":"  To mitigate the difficulties of university freshmen in adapting to university\nlife, we developed NOVI, a chatbot system based on GPT-4o. This system utilizes\npost and comment data from SKKU 'Everytime', a university community site.\nDeveloped using LangChain, NOVI's performance has been evaluated with a BLEU\nscore, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR\nscore. This approach is not only limited to help university freshmen but is\nalso expected to help various people adapting to new environments with\ndifferent data. This research explores the development and potential\napplication of new educational technology tools, contributing to easier social\nadaptation for beginners and settling a foundation for future advancement in\nLLM studies.\n","authors":["Yoonji Nam","TaeWoong Seo","Gyeongcheol Shin","Sangji Lee","JaeEun Im"],"pdf_url":"https://arxiv.org/pdf/2409.06192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06185v1","updated":"2024-09-10T03:26:42Z","published":"2024-09-10T03:26:42Z","title":"Can Large Language Models Unlock Novel Scientific Research Ideas?","summary":"  \"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.\n","authors":["Sandeep Kumar","Tirthankar Ghosal","Vinayak Goyal","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2409.06185v1.pdf","comment":"24 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2409.06178v1","updated":"2024-09-10T03:14:09Z","published":"2024-09-10T03:14:09Z","title":"SQLucid: Grounding Natural Language Database Queries with Interactive\n  Explanations","summary":"  Though recent advances in machine learning have led to significant\nimprovements in natural language interfaces for databases, the accuracy and\nreliability of these systems remain limited, especially in high-stakes domains.\nThis paper introduces SQLucid, a novel user interface that bridges the gap\nbetween non-expert users and complex database querying processes. SQLucid\naddresses existing limitations by integrating visual correspondence,\nintermediate query results, and editable step-by-step SQL explanations in\nnatural language to facilitate user understanding and engagement. This unique\nblend of features empowers users to understand and refine SQL queries easily\nand precisely. Two user studies and one quantitative experiment were conducted\nto validate SQLucid's effectiveness, showing significant improvement in task\ncompletion accuracy and user confidence compared to existing interfaces. Our\ncode is available at https://github.com/magic-YuanTian/SQLucid.\n","authors":["Yuan Tian","Jonathan K. Kummerfeld","Toby Jia-Jun Li","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06178v1.pdf","comment":"Accepted to UIST'24"},{"id":"http://arxiv.org/abs/2409.06173v1","updated":"2024-09-10T03:06:17Z","published":"2024-09-10T03:06:17Z","title":"Larger Language Models Don't Care How You Think: Why Chain-of-Thought\n  Prompting Fails in Subjective Tasks","summary":"  In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the\ndominant technique for performing natural language tasks, as it does not\nrequire updating the model parameters with gradient-based methods. ICL promises\nto \"adapt\" the LLM to perform the present task at a competitive or\nstate-of-the-art level at a fraction of the computational cost. ICL can be\naugmented by incorporating the reasoning process to arrive at the final label\nexplicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting.\nHowever, recent work has found that ICL relies mostly on the retrieval of task\npriors and less so on \"learning\" to perform tasks, especially for complex\nsubjective domains like emotion and morality, where priors ossify posterior\npredictions. In this work, we examine whether \"enabling\" reasoning also creates\nthe same behavior in LLMs, wherein the format of CoT retrieves reasoning priors\nthat remain relatively unchanged despite the evidence in the prompt. We find\nthat, surprisingly, CoT indeed suffers from the same posterior collapse as ICL\nfor larger language models. Code is avalaible at\nhttps://github.com/gchochla/cot-priors.\n","authors":["Georgios Chochlakis","Niyantha Maruthu Pandiyan","Kristina Lerman","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.06173v1.pdf","comment":"5 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2409.05005v2","updated":"2024-09-10T02:50:54Z","published":"2024-09-08T07:26:13Z","title":"Towards Patronizing and Condescending Language in Chinese Videos: A\n  Multimodal Dataset and Detector","summary":"  Patronizing and Condescending Language (PCL) is a form of discriminatory\ntoxic speech targeting vulnerable groups, threatening both online and offline\nsafety. While toxic speech research has mainly focused on overt toxicity, such\nas hate speech, microaggressions in the form of PCL remain underexplored.\nAdditionally, dominant groups' discriminatory facial expressions and attitudes\ntoward vulnerable communities can be more impactful than verbal cues, yet these\nframe features are often overlooked. In this paper, we introduce the PCLMM\ndataset, the first Chinese multimodal dataset for PCL, consisting of 715\nannotated videos from Bilibili, with high-quality PCL facial frame spans. We\nalso propose the MultiPCL detector, featuring a facial expression detection\nmodule for PCL recognition, demonstrating the effectiveness of modality\ncomplementarity in this challenging task. Our work makes an important\ncontribution to advancing microaggression detection within the domain of toxic\nspeech.\n","authors":["Hongbo Wang","Junyu Lu","Yan Han","Kai Ma","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2409.05005v2.pdf","comment":"Under review in ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.02076v2","updated":"2024-09-10T02:43:36Z","published":"2024-09-03T17:25:54Z","title":"Spinning the Golden Thread: Benchmarking Long-Form Generation in\n  long-context LLMs","summary":"  The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, Spinning the Golden\nThread (SGT), which tests models' ability to identify specific events within\ngenerated long text sequences. In this benchmark, we prompt long-context LMs to\ncreate long-form text that must include particular events or constraints and\nevaluate their ability to incorporate these elements. We evaluated ten\nlong-context LMs across four distinct scenarios, three types of prompt\ninstructions, and two different generation-length settings (16K and 32K).\nAlthough these models perform well on NIAH benchmarks, none demonstrated\nsatisfactory performance on the Spinning the Golden Thread, raising concerns\nabout their ability to generate coherent long-form text that follows\ninstructions. Additionally, as the length of the generated text increases, all\nmodels exhibit a significant drop in performance.\n","authors":["Yuhao Wu","Ming Shan Hee","Zhiqing Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2409.02076v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2409.06164v1","updated":"2024-09-10T02:22:50Z","published":"2024-09-10T02:22:50Z","title":"Deep Learning and Large Language Models for Audio and Text Analysis in\n  Predicting Suicidal Acts in Chinese Psychological Support Hotlines","summary":"  Suicide is a pressing global issue, demanding urgent and effective preventive\ninterventions. Among the various strategies in place, psychological support\nhotlines had proved as a potent intervention method. Approximately two million\npeople in China attempt suicide annually, with many individuals making multiple\nattempts. Prompt identification and intervention for high-risk individuals are\ncrucial to preventing tragedies. With the rapid advancement of artificial\nintelligence (AI), especially the development of large-scale language models\n(LLMs), new technological tools have been introduced to the field of mental\nhealth. This study included 1284 subjects, and was designed to validate whether\ndeep learning models and LLMs, using audio and transcribed text from support\nhotlines, can effectively predict suicide risk. We proposed a simple LLM-based\npipeline that first summarizes transcribed text from approximately one hour of\nspeech to extract key features, and then predict suicidial bahaviours in the\nfuture. We compared our LLM-based method with the traditional manual scale\napproach in a clinical setting and with five advanced deep learning models.\nSurprisingly, the proposed simple LLM pipeline achieved strong performance on a\ntest set of 46 subjects, with an F1 score of 76\\% when combined with manual\nscale rating. This is 7\\% higher than the best speech-based deep learning\nmodels and represents a 27.82\\% point improvement in F1 score compared to using\nthe manual scale apporach alone. Our study explores new applications of LLMs\nand demonstrates their potential for future use in suicide prevention efforts.\n","authors":["Yining Chen","Jianqiang Li","Changwei Song","Qing Zhao","Yongsheng Tong","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2409.06164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09895v3","updated":"2024-09-10T02:12:29Z","published":"2024-08-19T11:09:12Z","title":"Performance Law of Large Language Models","summary":"  Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.\n","authors":["Chuhan Wu","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2408.09895v3.pdf","comment":"Personal opinions of the authors"},{"id":"http://arxiv.org/abs/2409.05591v2","updated":"2024-09-10T02:01:43Z","published":"2024-09-09T13:20:31Z","title":"MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery","summary":"  Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied.\n","authors":["Hongjin Qian","Peitian Zhang","Zheng Liu","Kelong Mao","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2409.05591v2.pdf","comment":"Technical Report. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG"},{"id":"http://arxiv.org/abs/2409.06131v1","updated":"2024-09-10T00:59:18Z","published":"2024-09-10T00:59:18Z","title":"Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn,\n  Focus, and Review","summary":"  Large Language Model (LLM) pretraining traditionally relies on autoregressive\nlanguage modeling on randomly sampled data blocks from web-scale datasets. We\ntake inspiration from human learning techniques like spaced repetition to\nhypothesize that random data sampling for LLMs leads to high training cost and\nlow quality models which tend to forget data. In order to effectively commit\nweb-scale information to long-term memory, we propose the LFR (Learn, Focus,\nand Review) pedagogy, a new dynamic training paradigm which focuses and\nrepeatedly reviews complex data blocks at systematic intervals based on the\nmodel's learning pace and progress. LFR records the model perplexities for\ndifferent data blocks and frequently revisits blocks with higher perplexity\nwhich are more likely to be forgotten. We pretrain the GPT-2 models (124M -\n1.5B) from scratch on the OpenWebText dataset using LFR. We test on downstream\ntasks from the language modeling, question answering, translation, and problem\nsolving domains to achieve consistently lower perplexity and higher accuracy\nthan the baseline OpenAI models, while obtaining a 20x pretraining speed-up.\n","authors":["Neha Prakriya","Jui-Nan Yen","Cho-Jui Hsieh","Jason Cong"],"pdf_url":"https://arxiv.org/pdf/2409.06131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02636v2","updated":"2024-09-10T00:18:02Z","published":"2024-02-04T23:04:02Z","title":"Can Large Language Models Learn Independent Causal Mechanisms?","summary":"  Despite impressive performance on language modelling and complex reasoning\ntasks, Large Language Models (LLMs) fall short on the same tasks in uncommon\nsettings or with distribution shifts, exhibiting a lack of generalisation\nability. By contrast, systems such as causal models, that learn abstract\nvariables and causal relationships, can demonstrate increased robustness\nagainst changes in the distribution. One reason for this success is the\nexistence and use of Independent Causal Mechanisms (ICMs) representing\nhigh-level concepts that only sparsely interact. In this work, we apply two\nconcepts from causality to learn ICMs within LLMs. We develop a new LLM\narchitecture composed of multiple sparsely interacting language modelling\nmodules. We show that such causal constraints can improve out-of-distribution\nperformance on abstract and causal reasoning tasks. We also investigate the\nlevel of independence and domain specialisation and show that LLMs rely on\npre-trained partially domain-invariant mechanisms resilient to fine-tuning.\n","authors":["Gaël Gendron","Bao Trung Nguyen","Alex Yuxuan Peng","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2402.02636v2.pdf","comment":"20 pages, 7 pages for the main paper and 13 pages for references and\n  appendices, 17 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.06704v1","updated":"2024-09-10T17:59:55Z","published":"2024-09-10T17:59:55Z","title":"GeoCalib: Learning Single-image Calibration with Geometric Optimization","summary":"  From a single image, visual cues can help deduce intrinsic and extrinsic\ncamera parameters like the focal length and the gravity direction. This\nsingle-image calibration can benefit various downstream applications like image\nediting and 3D mapping. Current approaches to this problem are based on either\nclassical geometry with lines and vanishing points or on deep neural networks\ntrained end-to-end. The learned approaches are more robust but struggle to\ngeneralize to new environments and are less accurate than their classical\ncounterparts. We hypothesize that they lack the constraints that 3D geometry\nprovides. In this work, we introduce GeoCalib, a deep neural network that\nleverages universal rules of 3D geometry through an optimization process.\nGeoCalib is trained end-to-end to estimate camera parameters and learns to find\nuseful visual cues from the data. Experiments on various benchmarks show that\nGeoCalib is more robust and more accurate than existing classical and learned\napproaches. Its internal optimization estimates uncertainties, which help flag\nfailure cases and benefit downstream applications like visual localization. The\ncode and trained models are publicly available at\nhttps://github.com/cvg/GeoCalib.\n","authors":["Alexander Veicht","Paul-Edouard Sarlin","Philipp Lindenberger","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2409.06704v1.pdf","comment":"Presented at ECCV 2024"},{"id":"http://arxiv.org/abs/2409.06703v1","updated":"2024-09-10T17:59:53Z","published":"2024-09-10T17:59:53Z","title":"LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation","summary":"  Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of\nstatic scenes and objects in 3D, offering unprecedented quality. However,\nextending NeRFs to model dynamic objects or object articulations remains a\nchallenging problem. Previous works have tackled this issue by focusing on\npart-level reconstruction and motion estimation for objects, but they often\nrely on heuristics regarding the number of moving parts or object categories,\nwhich can limit their practical use. In this work, we introduce LEIA, a novel\napproach for representing dynamic 3D objects. Our method involves observing the\nobject at distinct time steps or \"states\" and conditioning a hypernetwork on\nthe current state, using this to parameterize our NeRF. This approach allows us\nto learn a view-invariant latent representation for each state. We further\ndemonstrate that by interpolating between these states, we can generate novel\narticulation configurations in 3D space that were previously unseen. Our\nexperimental results highlight the effectiveness of our method in articulating\nobjects in a manner that is independent of the viewing angle and joint\nconfiguration. Notably, our approach outperforms previous methods that rely on\nmotion information for articulation registration.\n","authors":["Archana Swaminathan","Anubhav Gupta","Kamal Gupta","Shishira R. Maiya","Vatsal Agarwal","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2409.06703v1.pdf","comment":"Accepted to ECCV 2024. Project Website at\n  https://archana1998.github.io/leia/"},{"id":"http://arxiv.org/abs/2409.06702v1","updated":"2024-09-10T17:59:40Z","published":"2024-09-10T17:59:40Z","title":"Hint-AD: Holistically Aligned Interpretability in End-to-End Autonomous\n  Driving","summary":"  End-to-end architectures in autonomous driving (AD) face a significant\nchallenge in interpretability, impeding human-AI trust. Human-friendly natural\nlanguage has been explored for tasks such as driving explanation and 3D\ncaptioning. However, previous works primarily focused on the paradigm of\ndeclarative interpretability, where the natural language interpretations are\nnot grounded in the intermediate outputs of AD systems, making the\ninterpretations only declarative. In contrast, aligned interpretability\nestablishes a connection between language and the intermediate outputs of AD\nsystems. Here we introduce Hint-AD, an integrated AD-language system that\ngenerates language aligned with the holistic perception-prediction-planning\noutputs of the AD model. By incorporating the intermediate outputs and a\nholistic token mixer sub-network for effective feature adaptation, Hint-AD\nachieves desirable accuracy, achieving state-of-the-art results in driving\nlanguage tasks including driving explanation, 3D dense captioning, and command\nprediction. To facilitate further study on driving explanation task on\nnuScenes, we also introduce a human-labeled dataset, Nu-X. Codes, dataset, and\nmodels will be publicly available.\n","authors":["Kairui Ding","Boyuan Chen","Yuchen Su","Huan-ang Gao","Bu Jin","Chonghao Sima","Wuqiang Zhang","Xiaohui Li","Paul Barsch","Hongyang Li","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.06702v1.pdf","comment":"CoRL 2024, Project Page: https://air-discover.github.io/Hint-AD/"},{"id":"http://arxiv.org/abs/2409.06699v1","updated":"2024-09-10T17:58:21Z","published":"2024-09-10T17:58:21Z","title":"A study on Deep Convolutional Neural Networks, Transfer Learning and\n  Ensemble Model for Breast Cancer Detection","summary":"  In deep learning, transfer learning and ensemble models have shown promise in\nimproving computer-aided disease diagnosis. However, applying the transfer\nlearning and ensemble model is still relatively limited. Moreover, the ensemble\nmodel's development is ad-hoc, overlooks redundant layers, and suffers from\nimbalanced datasets and inadequate augmentation. Lastly, significant Deep\nConvolutional Neural Networks (D-CNNs) have been introduced to detect and\nclassify breast cancer. Still, very few comparative studies were conducted to\ninvestigate the accuracy and efficiency of existing CNN architectures.\nRealising the gaps, this study compares the performance of D-CNN, which\nincludes the original CNN, transfer learning, and an ensemble model, in\ndetecting breast cancer. The comparison study of this paper consists of\ncomparison using six CNN-based deep learning architectures (SE-ResNet152,\nMobileNetV2, VGG19, ResNet18, InceptionV3, and DenseNet-121), a transfer\nlearning, and an ensemble model on breast cancer detection. Among the\ncomparison of these models, the ensemble model provides the highest detection\nand classification accuracy of 99.94% for breast cancer detection and\nclassification. However, this study also provides a negative result in the case\nof transfer learning, as the transfer learning did not increase the accuracy of\nthe original SE-ResNet152, MobileNetV2, VGG19, ResNet18, InceptionV3, and\nDenseNet-121 model. The high accuracy in detecting and categorising breast\ncancer detection using CNN suggests that the CNN model is promising in breast\ncancer disease detection. This research is significant in biomedical\nengineering, computer-aided disease diagnosis, and ML-based disease detection.\n","authors":["Md Taimur Ahad","Sumaya Mustofa","Faruk Ahmed","Yousuf Rayhan Emon","Aunirudra Dey Anu"],"pdf_url":"https://arxiv.org/pdf/2409.06699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09394v3","updated":"2024-09-10T17:54:34Z","published":"2024-06-13T17:59:10Z","title":"WonderWorld: Interactive 3D Scene Generation from a Single Image","summary":"  We present WonderWorld, a novel framework for interactive 3D scene generation\nthat enables users to interactively specify scene contents and layout and see\nthe created scenes in low latency. The major challenge lies in achieving fast\ngeneration of 3D scenes. Existing scene generation approaches fall short of\nspeed as they often require (1) progressively generating many views and depth\nmaps, and (2) time-consuming optimization of the scene geometry\nrepresentations. We introduce the Fast Layered Gaussian Surfels (FLAGS) as our\nscene representation and an algorithm to generate it from a single view. Our\napproach does not need multiple views, and it leverages a geometry-based\ninitialization that significantly reduces optimization time. Another challenge\nis generating coherent geometry that allows all scenes to be connected. We\nintroduce the guided depth diffusion that allows partial conditioning of depth\nestimation. WonderWorld generates connected and diverse 3D scenes in less than\n10 seconds on a single A6000 GPU, enabling real-time user interaction and\nexploration. We demonstrate the potential of WonderWorld for user-driven\ncontent creation and exploration in virtual environments. We will release full\ncode and software for reproducibility. Project website:\nhttps://kovenyu.com/WonderWorld/.\n","authors":["Hong-Xing Yu","Haoyi Duan","Charles Herrmann","William T. Freeman","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.09394v3.pdf","comment":"Project website: https://kovenyu.com/WonderWorld/"},{"id":"http://arxiv.org/abs/2409.06689v1","updated":"2024-09-10T17:53:47Z","published":"2024-09-10T17:53:47Z","title":"A comprehensive study on Blood Cancer detection and classification using\n  Convolutional Neural Network","summary":"  Over the years in object detection several efficient Convolutional Neural\nNetworks (CNN) networks, such as DenseNet201, InceptionV3, ResNet152v2,\nSEresNet152, VGG19, Xception gained significant attention due to their\nperformance. Moreover, CNN paradigms have expanded to transfer learning and\nensemble models from original CNN architectures. Research studies suggest that\ntransfer learning and ensemble models are capable of increasing the accuracy of\ndeep learning (DL) models. However, very few studies have conducted\ncomprehensive experiments utilizing these techniques in detecting and\nlocalizing blood malignancies. Realizing the gap, this study conducted three\nexperiments; in the first experiment -- six original CNNs were used, in the\nsecond experiment -- transfer learning and, in the third experiment a novel\nensemble model DIX (DenseNet201, InceptionV3, and Xception) was developed to\ndetect and classify blood cancer. The statistical result suggests that DIX\noutperformed the original and transfer learning performance, providing an\naccuracy of 99.12%. However, this study also provides a negative result in the\ncase of transfer learning, as the transfer learning did not increase the\naccuracy of the original CNNs. Like many other cancers, blood cancer diseases\nrequire timely identification for effective treatment plans and increased\nsurvival possibilities. The high accuracy in detecting and categorization blood\ncancer detection using CNN suggests that the CNN model is promising in blood\ncancer disease detection. This research is significant in the fields of\nbiomedical engineering, computer-aided disease diagnosis, and ML-based disease\ndetection.\n","authors":["Md Taimur Ahad","Sajib Bin Mamun","Sumaya Mustofa","Bo Song","Yan Li"],"pdf_url":"https://arxiv.org/pdf/2409.06689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06687v1","updated":"2024-09-10T17:53:29Z","published":"2024-09-10T17:53:29Z","title":"A study on deep feature extraction to detect and classify Acute\n  Lymphoblastic Leukemia (ALL)","summary":"  Acute lymphoblastic leukaemia (ALL) is a blood malignancy that mainly affects\nadults and children. This study looks into the use of deep learning,\nspecifically Convolutional Neural Networks (CNNs), for the detection and\nclassification of ALL. Conventional techniques for ALL diagnosis, such bone\nmarrow biopsy, are costly and prone to mistakes made by hand. By utilising\nautomated technologies, the research seeks to improve diagnostic accuracy. The\nresearch uses a variety of pre-trained CNN models, such as InceptionV3,\nResNet101, VGG19, DenseNet121, MobileNetV2, and DenseNet121, to extract\ncharacteristics from pictures of blood smears. ANOVA, Recursive Feature\nElimination (RFE), Random Forest, Lasso, and Principal Component Analysis (PCA)\nare a few of the selection approaches used to find the most relevant features\nafter feature extraction. Following that, machine learning methods like Na\\\"ive\nBayes, Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbours\n(KNN) are used to classify these features. With an 87% accuracy rate, the\nResNet101 model produced the best results, closely followed by DenseNet121 and\nVGG19. According to the study, CNN-based models have the potential to decrease\nthe need for medical specialists by increasing the speed and accuracy of ALL\ndiagnosis. To improve model performance, the study also recommends expanding\nand diversifying datasets and investigating more sophisticated designs such as\ntransformers. This study highlights how well automated deep learning systems do\nmedical diagnosis.\n","authors":["Sabit Ahamed Preanto","Md. Taimur Ahad","Yousuf Rayhan Emon","Sumaya Mustofa","Md Alamin"],"pdf_url":"https://arxiv.org/pdf/2409.06687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06685v1","updated":"2024-09-10T17:51:39Z","published":"2024-09-10T17:51:39Z","title":"GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface\n  Reconstruction","summary":"  3D Gaussian Splatting (3DGS) has shown promising performance in novel view\nsynthesis. Previous methods adapt it to obtaining surfaces of either individual\n3D objects or within limited scenes. In this paper, we make the first attempt\nto tackle the challenging task of large-scale scene surface reconstruction.\nThis task is particularly difficult due to the high GPU memory consumption,\ndifferent levels of details for geometric representation, and noticeable\ninconsistencies in appearance. To this end, we propose GigaGS, the first work\nfor high-quality surface reconstruction for large-scale scenes using 3DGS.\nGigaGS first applies a partitioning strategy based on the mutual visibility of\nspatial regions, which effectively grouping cameras for parallel processing. To\nenhance the quality of the surface, we also propose novel multi-view\nphotometric and geometric consistency constraints based on Level-of-Detail\nrepresentation. In doing so, our method can reconstruct detailed surface\nstructures. Comprehensive experiments are conducted on various datasets. The\nconsistent improvement demonstrates the superiority of GigaGS.\n","authors":["Junyi Chen","Weicai Ye","Yifan Wang","Danpeng Chen","Di Huang","Wanli Ouyang","Guofeng Zhang","Yu Qiao","Tong He"],"pdf_url":"https://arxiv.org/pdf/2409.06685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06683v1","updated":"2024-09-10T17:51:03Z","published":"2024-09-10T17:51:03Z","title":"Alignist: CAD-Informed Orientation Distribution Estimation by Fusing\n  Shape and Correspondences","summary":"  Object pose distribution estimation is crucial in robotics for better path\nplanning and handling of symmetric objects. Recent distribution estimation\napproaches employ contrastive learning-based approaches by maximizing the\nlikelihood of a single pose estimate in the absence of a CAD model. We propose\na pose distribution estimation method leveraging symmetry respecting\ncorrespondence distributions and shape information obtained using a CAD model.\nContrastive learning-based approaches require an exhaustive amount of training\nimages from different viewpoints to learn the distribution properly, which is\nnot possible in realistic scenarios. Instead, we propose a pipeline that can\nleverage correspondence distributions and shape information from the CAD model,\nwhich are later used to learn pose distributions. Besides, having access to\npose distribution based on correspondences before learning pose distributions\nconditioned on images, can help formulate the loss between distributions. The\nprior knowledge of distribution also helps the network to focus on getting\nsharper modes instead. With the CAD prior, our approach converges much faster\nand learns distribution better by focusing on learning sharper distribution\nnear all the valid modes, unlike contrastive approaches, which focus on a\nsingle mode at a time. We achieve benchmark results on SYMSOL-I and T-Less\ndatasets.\n","authors":["Shishir Reddy Vutukur","Rasmus Laurvig Haugaard","Junwen Huang","Benjamin Busam","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2409.06683v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2409.06676v1","updated":"2024-09-10T17:42:14Z","published":"2024-09-10T17:42:14Z","title":"Constructing an Interpretable Deep Denoiser by Unrolling Graph Laplacian\n  Regularizer","summary":"  An image denoiser can be used for a wide range of restoration problems via\nthe Plug-and-Play (PnP) architecture. In this paper, we propose a general\nframework to build an interpretable graph-based deep denoiser (GDD) by\nunrolling a solution to a maximum a posteriori (MAP) problem equipped with a\ngraph Laplacian regularizer (GLR) as signal prior. Leveraging a recent theorem\nshowing that any (pseudo-)linear denoiser $\\boldsymbol \\Psi$, under mild\nconditions, can be mapped to a solution of a MAP denoising problem regularized\nusing GLR, we first initialize a graph Laplacian matrix $\\mathbf L$ via\ntruncated Taylor Series Expansion (TSE) of $\\boldsymbol \\Psi^{-1}$. Then, we\ncompute the MAP linear system solution by unrolling iterations of the conjugate\ngradient (CG) algorithm into a sequence of neural layers as a feed-forward\nnetwork -- one that is amenable to parameter tuning. The resulting GDD network\nis \"graph-interpretable\", low in parameter count, and easy to initialize thanks\nto $\\mathbf L$ derived from a known well-performing denoiser $\\boldsymbol\n\\Psi$. Experimental results show that GDD achieves competitive image denoising\nperformance compared to competitors, but employing far fewer parameters, and is\nmore robust to covariate shift.\n","authors":["Seyed Alireza Hosseini","Tam Thuc Do","Gene Cheung","Yuichi Tanaka"],"pdf_url":"https://arxiv.org/pdf/2409.06676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06671v1","updated":"2024-09-10T17:40:46Z","published":"2024-09-10T17:40:46Z","title":"A Semantic Segmentation Approach on Sweet Orange Leaf Diseases Detection\n  Utilizing YOLO","summary":"  This research introduces an advanced method for diagnosing diseases in sweet\norange leaves by utilising advanced artificial intelligence models like YOLOv8\n. Due to their significance as a vital agricultural product, sweet oranges\nencounter significant threats from a variety of diseases that harmfully affect\nboth their yield and quality. Conventional methods for disease detection\nprimarily depend on manual inspection which is ineffective and frequently leads\nto errors, resulting in delayed treatment and increased financial losses. In\nresponse to this challenge, the research utilized YOLOv8 , harnessing their\nproficiencies in detecting objects and analyzing images. YOLOv8 is recognized\nfor its rapid and precise performance, while VIT is acknowledged for its\ndetailed feature extraction abilities. Impressively, during both the training\nand validation stages, YOLOv8 exhibited a perfect accuracy of 80.4%, while VIT\nachieved an accuracy of 99.12%, showcasing their potential to transform disease\ndetection in agriculture. The study comprehensively examined the practical\nchallenges related to the implementation of AI technologies in agriculture,\nencompassing the computational demands and user accessibility, and offering\nviable solutions for broader usage. Moreover, it underscores the environmental\nconsiderations, particularly the potential for reduced pesticide usage, thereby\npromoting sustainable farming and environmental conservation. These findings\nprovide encouraging insights into the application of AI in agriculture,\nsuggesting a transition towards more effective, sustainable, and\ntechnologically advanced farming methods. This research not only highlights the\nefficacy of YOLOv8 within a specific agricultural domain but also lays the\nfoundation for further studies that encompass a broader application in crop\nmanagement and sustainable agricultural practices.\n","authors":["Sabit Ahamed Preanto","Md. Taimur Ahad","Yousuf Rayhan Emon","Sumaya Mustofa","Md Alamin"],"pdf_url":"https://arxiv.org/pdf/2409.06671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06665v1","updated":"2024-09-10T17:34:07Z","published":"2024-09-10T17:34:07Z","title":"Data Collection-free Masked Video Modeling","summary":"  Pre-training video transformers generally requires a large amount of data,\npresenting significant challenges in terms of data collection costs and\nconcerns related to privacy, licensing, and inherent biases. Synthesizing data\nis one of the promising ways to solve these issues, yet pre-training solely on\nsynthetic data has its own challenges. In this paper, we introduce an effective\nself-supervised learning framework for videos that leverages readily available\nand less costly static images. Specifically, we define the Pseudo Motion\nGenerator (PMG) module that recursively applies image transformations to\ngenerate pseudo-motion videos from images. These pseudo-motion videos are then\nleveraged in masked video modeling. Our approach is applicable to synthetic\nimages as well, thus entirely freeing video pre-training from data collection\ncosts and other concerns in real data. Through experiments in action\nrecognition tasks, we demonstrate that this framework allows effective learning\nof spatio-temporal features through pseudo-motion videos, significantly\nimproving over existing methods which also use static images and partially\noutperforming those using both real and synthetic videos. These results uncover\nfragments of what video transformers learn through masked video modeling.\n","authors":["Yuchi Ishikawa","Masayoshi Kondo","Yoshimitsu Aoki"],"pdf_url":"https://arxiv.org/pdf/2409.06665v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2409.06662v1","updated":"2024-09-10T17:25:47Z","published":"2024-09-10T17:25:47Z","title":"World-Grounded Human Motion Recovery via Gravity-View Coordinates","summary":"  We present a novel method for recovering world-grounded human motion from\nmonocular video. The main challenge lies in the ambiguity of defining the world\ncoordinate system, which varies between sequences. Previous approaches attempt\nto alleviate this issue by predicting relative motion in an autoregressive\nmanner, but are prone to accumulating errors. Instead, we propose estimating\nhuman poses in a novel Gravity-View (GV) coordinate system, which is defined by\nthe world gravity and the camera view direction. The proposed GV system is\nnaturally gravity-aligned and uniquely defined for each video frame, largely\nreducing the ambiguity of learning image-pose mapping. The estimated poses can\nbe transformed back to the world coordinate system using camera rotations,\nforming a global motion sequence. Additionally, the per-frame estimation avoids\nerror accumulation in the autoregressive methods. Experiments on in-the-wild\nbenchmarks demonstrate that our method recovers more realistic motion in both\nthe camera space and world-grounded settings, outperforming state-of-the-art\nmethods in both accuracy and speed. The code is available at\nhttps://zju3dv.github.io/gvhmr/.\n","authors":["Zehong Shen","Huaijin Pi","Yan Xia","Zhi Cen","Sida Peng","Zechen Hu","Hujun Bao","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.06662v1.pdf","comment":"Accepted at SIGGRAPH Asia 2024 (Conference Track). Project page:\n  https://zju3dv.github.io/gvhmr/"},{"id":"http://arxiv.org/abs/2406.05786v2","updated":"2024-09-10T17:12:03Z","published":"2024-06-09T13:53:05Z","title":"CAMS: Convolution and Attention-Free Mamba-based Cardiac Image\n  Segmentation","summary":"  Convolutional Neural Networks (CNNs) and Transformer-based self-attention\nmodels have become the standard for medical image segmentation. This paper\ndemonstrates that convolution and self-attention, while widely used, are not\nthe only effective methods for segmentation. Breaking with convention, we\npresent a Convolution and self-Attention-free Mamba-based semantic Segmentation\nNetwork named CAMS-Net. Specifically, we design Mamba-based Channel Aggregator\nand Spatial Aggregator, which are applied independently in each encoder-decoder\nstage. The Channel Aggregator extracts information across different channels,\nand the Spatial Aggregator learns features across different spatial locations.\nWe also propose a Linearly Interconnected Factorized Mamba (LIFM) block to\nreduce the computational complexity of a Mamba block and to enhance its\ndecision function by introducing a non-linearity between two factorized Mamba\nblocks. Our model outperforms the existing state-of-the-art CNN,\nself-attention, and Mamba-based methods on CMR and M&Ms-2 Cardiac segmentation\ndatasets, showing how this innovative, convolution, and self-attention-free\nmethod can inspire further research beyond CNN and Transformer paradigms,\nachieving linear complexity and reducing the number of parameters. Source code\nand pre-trained models will be publicly available upon acceptance.\n","authors":["Abbas Khan","Muhammad Asad","Martin Benning","Caroline Roney","Gregory Slabaugh"],"pdf_url":"https://arxiv.org/pdf/2406.05786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06648v1","updated":"2024-09-10T17:06:54Z","published":"2024-09-10T17:06:54Z","title":"Image Vectorization with Depth: convexified shape layers with depth\n  ordering","summary":"  Image vectorization is a process to convert a raster image into a scalable\nvector graphic format. Objective is to effectively remove the pixelization\neffect while representing boundaries of image by scaleable parameterized\ncurves. We propose new image vectorization with depth which considers depth\nordering among shapes and use curvature-based inpainting for convexifying\nshapes in vectorization process.From a given color quantized raster image, we\nfirst define each connected component of the same color as a shape layer, and\nconstruct depth ordering among them using a newly proposed depth ordering\nenergy. Global depth ordering among all shapes is described by a directed\ngraph, and we propose an energy to remove cycle within the graph. After\nconstructing depth ordering of shapes, we convexify occluded regions by Euler's\nelastica curvature-based variational inpainting, and leverage on the stability\nof Modica-Mortola double-well potential energy to inpaint large regions. This\nis following human vision perception that boundaries of shapes extend smoothly,\nand we assume shapes are likely to be convex. Finally, we fit B\\'{e}zier curves\nto the boundaries and save vectorization as a SVG file which allows\nsuperposition of curvature-based inpainted shapes following the depth ordering.\nThis is a new way to vectorize images, by decomposing an image into scalable\nshape layers with computed depth ordering. This approach makes editing shapes\nand images more natural and intuitive. We also consider grouping shape layers\nfor semantic vectorization. We present various numerical results and\ncomparisons against recent layer-based vectorization methods to validate the\nproposed model.\n","authors":["Ho Law","Sung Ha Kang"],"pdf_url":"https://arxiv.org/pdf/2409.06648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09947v3","updated":"2024-09-10T17:03:26Z","published":"2023-09-18T17:12:43Z","title":"Deep Visual Odometry with Events and Frames","summary":"  Visual Odometry (VO) is crucial for autonomous robotic navigation, especially\nin GPS-denied environments like planetary terrains. To improve robustness,\nrecent model-based VO systems have begun combining standard and event-based\ncameras. While event cameras excel in low-light and high-speed motion, standard\ncameras provide dense and easier-to-track features. However, the field of\nimage- and event-based VO still predominantly relies on model-based methods and\nis yet to fully integrate recent image-only advancements leveraging end-to-end\nlearning-based architectures. Seamlessly integrating the two modalities remains\nchallenging due to their different nature, one asynchronous, the other not,\nlimiting the potential for a more effective image- and event-based VO. We\nintroduce RAMP-VO, the first end-to-end learned image- and event-based VO\nsystem. It leverages novel Recurrent, Asynchronous, and Massively Parallel\n(RAMP) encoders capable of fusing asynchronous events with image data,\nproviding 8x faster inference and 33% more accurate predictions than existing\nsolutions. Despite being trained only in simulation, RAMP-VO outperforms\nprevious methods on the newly introduced Apollo and Malapert datasets, and on\nexisting benchmarks, where it improves image- and event-based methods by 58.8%\nand 30.6%, paving the way for robust and asynchronous VO in space.\n","authors":["Roberto Pellerito","Marco Cannici","Daniel Gehrig","Joris Belhadj","Olivier Dubois-Matra","Massimo Casasco","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2309.09947v3.pdf","comment":"IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2024"},{"id":"http://arxiv.org/abs/2409.06644v1","updated":"2024-09-10T17:00:19Z","published":"2024-09-10T17:00:19Z","title":"EyeCLIP: A visual-language foundation model for multi-modal ophthalmic\n  image analysis","summary":"  Early detection of eye diseases like glaucoma, macular degeneration, and\ndiabetic retinopathy is crucial for preventing vision loss. While artificial\nintelligence (AI) foundation models hold significant promise for addressing\nthese challenges, existing ophthalmic foundation models primarily focus on a\nsingle modality, whereas diagnosing eye diseases requires multiple modalities.\nA critical yet often overlooked aspect is harnessing the multi-view information\nacross various modalities for the same patient. Additionally, due to the\nlong-tail nature of ophthalmic diseases, standard fully supervised or\nunsupervised learning approaches often struggle. Therefore, it is essential to\nintegrate clinical text to capture a broader spectrum of diseases. We propose\nEyeCLIP, a visual-language foundation model developed using over 2.77 million\nmulti-modal ophthalmology images with partial text data. To fully leverage the\nlarge multi-modal unlabeled and labeled data, we introduced a pretraining\nstrategy that combines self-supervised reconstructions, multi-modal image\ncontrastive learning, and image-text contrastive learning to learn a shared\nrepresentation of multiple modalities. Through evaluation using 14 benchmark\ndatasets, EyeCLIP can be transferred to a wide range of downstream tasks\ninvolving ocular and systemic diseases, achieving state-of-the-art performance\nin disease classification, visual question answering, and cross-modal\nretrieval. EyeCLIP represents a significant advancement over previous methods,\nespecially showcasing few-shot, even zero-shot capabilities in real-world\nlong-tail scenarios.\n","authors":["Danli Shi","Weiyi Zhang","Jiancheng Yang","Siyu Huang","Xiaolan Chen","Mayinuer Yusufu","Kai Jin","Shan Lin","Shunming Liu","Qing Zhang","Mingguang He"],"pdf_url":"https://arxiv.org/pdf/2409.06644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05388v5","updated":"2024-09-10T16:59:37Z","published":"2023-09-11T11:35:17Z","title":"Robust Single Rotation Averaging Revisited","summary":"  In this work, we propose a novel method for robust single rotation averaging\nthat can efficiently handle an extremely large fraction of outliers. Our\napproach is to minimize the total truncated least unsquared deviations (TLUD)\ncost of geodesic distances. The proposed algorithm consists of three steps:\nFirst, we consider each input rotation as a potential initial solution and\nchoose the one that yields the least sum of truncated chordal deviations. Next,\nwe obtain the inlier set using the initial solution and compute its chordal\n$L_2$-mean. Finally, starting from this estimate, we iteratively compute the\ngeodesic $L_1$-mean of the inliers using the Weiszfeld algorithm on $SO(3)$. An\nextensive evaluation shows that our method is robust against up to 99% outliers\ngiven a sufficient number of accurate inliers, outperforming the current state\nof the art.\n","authors":["Seong Hun Lee","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2309.05388v5.pdf","comment":"Accepted to ECCV 2024 Workshop on Recovering 6D Object Pose (R6D)"},{"id":"http://arxiv.org/abs/2409.06633v1","updated":"2024-09-10T16:44:47Z","published":"2024-09-10T16:44:47Z","title":"SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse\n  Low-Rank Adaptation","summary":"  In recent years, the development of diffusion models has led to significant\nprogress in image and video generation tasks, with pre-trained models like the\nStable Diffusion series playing a crucial role. Inspired by model pruning which\nlightens large pre-trained models by removing unimportant parameters, we\npropose a novel model fine-tuning method to make full use of these ineffective\nparameters and enable the pre-trained model with new task-specified\ncapabilities. In this work, we first investigate the importance of parameters\nin pre-trained diffusion models, and discover that the smallest 10% to 20% of\nparameters by absolute values do not contribute to the generation process.\nBased on this observation, we propose a method termed SaRA that re-utilizes\nthese temporarily ineffective parameters, equating to optimizing a sparse\nweight matrix to learn the task-specific knowledge. To mitigate overfitting, we\npropose a nuclear-norm-based low-rank sparse training scheme for efficient\nfine-tuning. Furthermore, we design a new progressive parameter adjustment\nstrategy to make full use of the re-trained/finetuned parameters. Finally, we\npropose a novel unstructural backpropagation strategy, which significantly\nreduces memory costs during fine-tuning. Our method enhances the generative\ncapabilities of pre-trained models in downstream applications and outperforms\ntraditional fine-tuning methods like LoRA in maintaining model's generalization\nability. We validate our approach through fine-tuning experiments on SD models,\ndemonstrating significant improvements. SaRA also offers a practical advantage\nthat requires only a single line of code modification for efficient\nimplementation and is seamlessly compatible with existing methods.\n","authors":["Teng Hu","Jiangning Zhang","Ran Yi","Hongrui Huang","Yabiao Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2409.06633v1.pdf","comment":"Parameter efficient finetuning method"},{"id":"http://arxiv.org/abs/2305.17644v3","updated":"2024-09-10T16:42:38Z","published":"2023-05-28T06:19:36Z","title":"Caterpillar: A Pure-MLP Architecture with Shifted-Pillars-Concatenation","summary":"  Modeling in Computer Vision has evolved to MLPs. Vision MLPs naturally lack\nlocal modeling capability, to which the simplest treatment is combined with\nconvolutional layers. Convolution, famous for its sliding window scheme, also\nsuffers from this scheme of redundancy and lower parallel computation. In this\npaper, we seek to dispense with the windowing scheme and introduce a more\nelaborate and parallelizable method to exploit locality. To this end, we\npropose a new MLP module, namely Shifted-Pillars-Concatenation (SPC), that\nconsists of two steps of processes: (1) Pillars-Shift, which generates four\nneighboring maps by shifting the input image along four directions, and (2)\nPillars-Concatenation, which applies linear transformations and concatenation\non the maps to aggregate local features. SPC module offers superior local\nmodeling power and performance gains, making it a promising alternative to the\nconvolutional layer. Then, we build a pure-MLP architecture called Caterpillar\nby replacing the convolutional layer with the SPC module in a hybrid model of\nsMLPNet. Extensive experiments show Caterpillar's excellent performance on both\nsmall-scale and ImageNet-1k classification benchmarks, with remarkable\nscalability and transfer capability possessed as well. The code is available at\nhttps://github.com/sunjin19126/Caterpillar.\n","authors":["Jin Sun","Xiaoshuang Shi","Zhiyuan Wang","Kaidi Xu","Heng Tao Shen","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2305.17644v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06625v1","updated":"2024-09-10T16:28:09Z","published":"2024-09-10T16:28:09Z","title":"Towards Localizing Structural Elements: Merging Geometrical Detection\n  with Semantic Verification in RGB-D Data","summary":"  RGB-D cameras supply rich and dense visual and spatial information for\nvarious robotics tasks such as scene understanding, map reconstruction, and\nlocalization. Integrating depth and visual information can aid robots in\nlocalization and element mapping, advancing applications like 3D scene graph\ngeneration and Visual Simultaneous Localization and Mapping (VSLAM). While\npoint cloud data containing such information is primarily used for enhanced\nscene understanding, exploiting their potential to capture and represent rich\nsemantic information has yet to be adequately targeted. This paper presents a\nreal-time pipeline for localizing building components, including wall and\nground surfaces, by integrating geometric calculations for pure 3D plane\ndetection followed by validating their semantic category using point cloud data\nfrom RGB-D cameras. It has a parallel multi-thread architecture to precisely\nestimate poses and equations of all the planes detected in the environment,\nfilters the ones forming the map structure using a panoptic segmentation\nvalidation, and keeps only the validated building components. Incorporating the\nproposed method into a VSLAM framework confirmed that constraining the map with\nthe detected environment-driven semantic elements can improve scene\nunderstanding and map reconstruction accuracy. It can also ensure\n(re-)association of these detected components into a unified 3D scene graph,\nbridging the gap between geometric accuracy and semantic understanding.\nAdditionally, the pipeline allows for the detection of potential higher-level\nstructural entities, such as rooms, by identifying the relationships between\nbuilding components based on their layout.\n","authors":["Ali Tourani","Saad Ejaz","Hriday Bavle","Jose Luis Sanchez-Lopez","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2409.06625v1.pdf","comment":"6 pages, 5 figures. 3 tables"},{"id":"http://arxiv.org/abs/2409.06620v1","updated":"2024-09-10T16:16:34Z","published":"2024-09-10T16:16:34Z","title":"MVGaussian: High-Fidelity text-to-3D Content Generation with Multi-View\n  Guidance and Surface Densification","summary":"  The field of text-to-3D content generation has made significant progress in\ngenerating realistic 3D objects, with existing methodologies like Score\nDistillation Sampling (SDS) offering promising guidance. However, these methods\noften encounter the \"Janus\" problem-multi-face ambiguities due to imprecise\nguidance. Additionally, while recent advancements in 3D gaussian splitting have\nshown its efficacy in representing 3D volumes, optimization of this\nrepresentation remains largely unexplored. This paper introduces a unified\nframework for text-to-3D content generation that addresses these critical gaps.\nOur approach utilizes multi-view guidance to iteratively form the structure of\nthe 3D model, progressively enhancing detail and accuracy. We also introduce a\nnovel densification algorithm that aligns gaussians close to the surface,\noptimizing the structural integrity and fidelity of the generated models.\nExtensive experiments validate our approach, demonstrating that it produces\nhigh-quality visual outputs with minimal time cost. Notably, our method\nachieves high-quality results within half an hour of training, offering a\nsubstantial efficiency gain over most existing methods, which require hours of\ntraining time to achieve comparable results.\n","authors":["Phu Pham","Aradhya N. Mathur","Ojaswa Sharma","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2409.06620v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.06618v1","updated":"2024-09-10T16:15:01Z","published":"2024-09-10T16:15:01Z","title":"Hierarchical Multi-Label Classification with Missing Information for\n  Benthic Habitat Imagery","summary":"  In this work, we apply state-of-the-art self-supervised learning techniques\non a large dataset of seafloor imagery, \\textit{BenthicNet}, and study their\nperformance for a complex hierarchical multi-label (HML) classification\ndownstream task. In particular, we demonstrate the capacity to conduct HML\ntraining in scenarios where there exist multiple levels of missing annotation\ninformation, an important scenario for handling heterogeneous real-world data\ncollected by multiple research groups with differing data collection protocols.\nWe find that, when using smaller one-hot image label datasets typical of local\nor regional scale benthic science projects, models pre-trained with\nself-supervision on a larger collection of in-domain benthic data outperform\nmodels pre-trained on ImageNet. In the HML setting, we find the model can\nattain a deeper and more precise classification if it is pre-trained with\nself-supervision on in-domain data. We hope this work can establish a benchmark\nfor future models in the field of automated underwater image annotation tasks\nand can guide work in other domains with hierarchical annotations of mixed\nresolution.\n","authors":["Isaac Xu","Benjamin Misiuk","Scott C. Lowe","Martin Gillis","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2409.06618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06617v1","updated":"2024-09-10T16:14:46Z","published":"2024-09-10T16:14:46Z","title":"When to Extract ReID Features: A Selective Approach for Improved\n  Multiple Object Tracking","summary":"  Extracting and matching Re-Identification (ReID) features is used by many\nstate-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly\neffective against frequent and long-term occlusions. While end-to-end object\ndetection and tracking have been the main focus of recent research, they have\nyet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus,\nfrom an application standpoint, methods with separate detection and embedding\nremain the best option for accuracy, modularity, and ease of implementation,\nthough they are impractical for edge devices due to the overhead involved. In\nthis paper, we investigate a selective approach to minimize the overhead of\nfeature extraction while preserving accuracy, modularity, and ease of\nimplementation. This approach can be integrated into various SOTA methods. We\ndemonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT.\nExperiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism\nretains the advantages of feature extraction during occlusions while\nsignificantly reducing runtime. Additionally, it improves accuracy by\npreventing confusion in the feature-matching stage, particularly in cases of\ndeformation and appearance similarity, which are common in DanceTrack.\nhttps://github.com/emirhanbayar/Fast-StrongSORT,\nhttps://github.com/emirhanbayar/Fast-Deep-OC-SORT\n","authors":["Emirhan Bayar","Cemal Aker"],"pdf_url":"https://arxiv.org/pdf/2409.06617v1.pdf","comment":"8 pages, 5 figures. Presents a selective approach for ReID feature\n  extraction in Multiple Object Tracking, reducing computational overhead while\n  maintaining accuracy. Tested on StrongSORT and Deep OC-SORT using MOT17,\n  MOT20, and DanceTrack datasets. Code:\n  https://github.com/emirhanbayar/Fast-StrongSORT,\n  https://github.com/emirhanbayar/Fast-Deep-OC-SORT"},{"id":"http://arxiv.org/abs/2305.11891v2","updated":"2024-09-10T16:04:28Z","published":"2023-05-12T09:54:21Z","title":"Unlocking the Use of Raw Multispectral Earth Observation Imagery for\n  Onboard Artificial Intelligence","summary":"  Nowadays, there is growing interest in applying Artificial Intelligence (AI)\non board Earth Observation (EO) satellites for time-critical applications, such\nas natural disaster response. However, the unavailability of raw satellite data\ncurrently hinders research on lightweight pre-processing techniques and limits\nthe exploration of end-to-end pipelines, which could offer more efficient and\naccurate extraction of insights directly from the source data. To fill this\ngap, this work presents a novel methodology to automate the creation of\ndatasets for the detection of target events (e.g., warm thermal hotspots) or\nobjects (e.g., vessels) from Sentinel-2 raw data and other multispectral EO\npushbroom raw imagery. The presented approach first processes the raw data by\napplying a pipeline consisting of spatial band registration and georeferencing\nof the raw data pixels. Then, it detects the target events by leveraging\nevent-specific state-of-the-art algorithms on the Level-1C products, which are\nmosaicked and cropped on the georeferenced correspondent raw granule area. The\ndetected events are finally re-projected back onto the corresponding raw\nimages. We apply the proposed methodology to realize THRawS (Thermal Hotspots\nin Raw Sentinel-2 data), the first dataset of Sentinel-2 raw data containing\nwarm thermal hotspots. THRawS includes 1090 samples containing wildfires,\nvolcanic eruptions, and 33,335 event-free acquisitions to enable thermal\nhotspot detection and general classification applications. This dataset and\nassociated toolkits provide the community with both an immediately useful\nresource as well as a framework and methodology acting as a template for future\nadditions. With this work, we hope to pave the way for research on\nenergy-efficient pre-processing algorithms and AI-based end-to-end processing\nsystems on board EO satellites.\n","authors":["Gabriele Meoni","Roberto Del Prete","Federico Serva","Alix De Beussche","Olivier Colin","Nicolas Longépé"],"pdf_url":"https://arxiv.org/pdf/2305.11891v2.pdf","comment":"17 Pages"},{"id":"http://arxiv.org/abs/2409.06609v1","updated":"2024-09-10T16:02:12Z","published":"2024-09-10T16:02:12Z","title":"Improving the Precision of CNNs for Magnetic Resonance Spectral Modeling","summary":"  Magnetic resonance spectroscopic imaging is a widely available imaging\nmodality that can non-invasively provide a metabolic profile of the tissue of\ninterest, yet is challenging to integrate clinically. One major reason is the\nexpensive, expert data processing and analysis that is required. Using machine\nlearning to predict MRS-related quantities offers avenues around this problem,\nbut deep learning models bring their own challenges, especially model trust.\nCurrent research trends focus primarily on mean error metrics, but\ncomprehensive precision metrics are also needed, e.g. standard deviations,\nconfidence intervals, etc.. This work highlights why more comprehensive error\ncharacterization is important and how to improve the precision of CNNs for\nspectral modeling, a quantitative task. The results highlight advantages and\ntrade-offs of these techniques that should be considered when addressing such\nregression tasks with CNNs. Detailed insights into the underlying mechanisms of\neach technique, and how they interact with other techniques, are discussed in\ndepth.\n","authors":["John LaMaster","Dhritiman Das","Florian Kofler","Jason Crane","Yan Li","Tobias Lasser","Bjoern H Menze"],"pdf_url":"https://arxiv.org/pdf/2409.06609v1.pdf","comment":"11 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2409.06605v1","updated":"2024-09-10T15:58:21Z","published":"2024-09-10T15:58:21Z","title":"Interactive 3D Segmentation for Primary Gross Tumor Volume in\n  Oropharyngeal Cancer","summary":"  The main treatment modality for oropharyngeal cancer (OPC) is radiotherapy,\nwhere accurate segmentation of the primary gross tumor volume (GTVp) is\nessential. However, accurate GTVp segmentation is challenging due to\nsignificant interobserver variability and the time-consuming nature of manual\nannotation, while fully automated methods can occasionally fail. An interactive\ndeep learning (DL) model offers the advantage of automatic high-performance\nsegmentation with the flexibility for user correction when necessary. In this\nstudy, we examine interactive DL for GTVp segmentation in OPC. We implement\nstate-of-the-art algorithms and propose a novel two-stage Interactive Click\nRefinement (2S-ICR) framework. Using the 2021 HEad and neCK TumOR (HECKTOR)\ndataset for development and an external dataset from The University of Texas MD\nAnderson Cancer Center for evaluation, the 2S-ICR framework achieves a Dice\nsimilarity coefficient of 0.713 $\\pm$ 0.152 without user interaction and 0.824\n$\\pm$ 0.099 after five interactions, outperforming existing methods in both\ncases.\n","authors":["Mikko Saukkoriipi","Jaakko Sahlsten","Joel Jaskari","Lotta Orasmaa","Jari Kangas","Nastaran Rasouli","Roope Raisamo","Jussi Hirvonen","Helena Mehtonen","Jorma Järnstedt","Antti Mäkitie","Mohamed Naser","Clifton Fuller","Benjamin Kann","Kimmo Kaski"],"pdf_url":"https://arxiv.org/pdf/2409.06605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06603v1","updated":"2024-09-10T15:55:53Z","published":"2024-09-10T15:55:53Z","title":"A Practical Gated Recurrent Transformer Network Incorporating Multiple\n  Fusions for Video Denoising","summary":"  State-of-the-art (SOTA) video denoising methods employ multi-frame\nsimultaneous denoising mechanisms, resulting in significant delays (e.g., 16\nframes), making them impractical for real-time cameras. To overcome this\nlimitation, we propose a multi-fusion gated recurrent Transformer network\n(GRTN) that achieves SOTA denoising performance with only a single-frame delay.\nSpecifically, the spatial denoising module extracts features from the current\nframe, while the reset gate selects relevant information from the previous\nframe and fuses it with current frame features via the temporal denoising\nmodule. The update gate then further blends this result with the previous frame\nfeatures, and the reconstruction module integrates it with the current frame.\nTo robustly compute attention for noisy features, we propose a residual\nsimplified Swin Transformer with Euclidean distance (RSSTE) in the spatial and\ntemporal denoising modules. Comparative objective and subjective results show\nthat our GRTN achieves denoising performance comparable to SOTA multi-frame\ndelay networks, with only a single-frame delay.\n","authors":["Kai Guo","Seungwon Choi","Jongseong Choi","Lae-Hoon Kim"],"pdf_url":"https://arxiv.org/pdf/2409.06603v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.06590v1","updated":"2024-09-10T15:31:37Z","published":"2024-09-10T15:31:37Z","title":"Lightweight Multiscale Feature Fusion Super-Resolution Network Based on\n  Two-branch Convolution and Transformer","summary":"  The single image super-resolution(SISR) algorithms under deep learning\ncurrently have two main models, one based on convolutional neural networks and\nthe other based on Transformer. The former uses the stacking of convolutional\nlayers with different convolutional kernel sizes to design the model, which\nenables the model to better extract the local features of the image; the latter\nuses the self-attention mechanism to design the model, which allows the model\nto establish long-distance dependencies between image pixel points through the\nself-attention mechanism and then better extract the global features of the\nimage. However, both of the above methods face their problems. Based on this,\nthis paper proposes a new lightweight multi-scale feature fusion network model\nbased on two-way complementary convolutional and Transformer, which integrates\nthe respective features of Transformer and convolutional neural networks\nthrough a two-branch network architecture, to realize the mutual fusion of\nglobal and local information. Meanwhile, considering the partial loss of\ninformation caused by the low-pixel images trained by the deep neural network,\nthis paper designs a modular connection method of multi-stage feature\nsupplementation to fuse the feature maps extracted from the shallow stage of\nthe model with those extracted from the deep stage of the model, to minimize\nthe loss of the information in the feature images that is beneficial to the\nimage restoration as much as possible, to facilitate the obtaining of a\nhigher-quality restored image. The practical results finally show that the\nmodel proposed in this paper is optimal in image recovery performance when\ncompared with other lightweight models with the same amount of parameters.\n","authors":["Li Ke","Liu Yukai"],"pdf_url":"https://arxiv.org/pdf/2409.06590v1.pdf","comment":"11 pages,12 figures"},{"id":"http://arxiv.org/abs/2409.06589v1","updated":"2024-09-10T15:30:20Z","published":"2024-09-10T15:30:20Z","title":"Seg-HGNN: Unsupervised and Light-Weight Image Segmentation with\n  Hyperbolic Graph Neural Networks","summary":"  Image analysis in the euclidean space through linear hyperspaces is well\nstudied. However, in the quest for more effective image representations, we\nturn to hyperbolic manifolds. They provide a compelling alternative to capture\ncomplex hierarchical relationships in images with remarkably small\ndimensionality. To demonstrate hyperbolic embeddings' competence, we introduce\na light-weight hyperbolic graph neural network for image segmentation,\nencompassing patch-level features in a very small embedding size. Our solution,\nSeg-HGNN, surpasses the current best unsupervised method by 2.5\\%, 4\\% on\nVOC-07, VOC-12 for localization, and by 0.8\\%, 1.3\\% on CUB-200, ECSSD for\nsegmentation, respectively. With less than 7.5k trainable parameters, Seg-HGNN\ndelivers effective and fast ($\\approx 2$ images/second) results on very\nstandard GPUs like the GTX1650. This empirical evaluation presents compelling\nevidence of the efficacy and potential of hyperbolic representations for vision\ntasks.\n","authors":["Debjyoti Mondal","Rahul Mishra","Chandan Pandey"],"pdf_url":"https://arxiv.org/pdf/2409.06589v1.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2409.06584v1","updated":"2024-09-10T15:26:38Z","published":"2024-09-10T15:26:38Z","title":"Transtreaming: Adaptive Delay-aware Transformer for Real-time Streaming\n  Perception","summary":"  Real-time object detection is critical for the decision-making process for\nmany real-world applications, such as collision avoidance and path planning in\nautonomous driving. This work presents an innovative real-time streaming\nperception method, Transtreaming, which addresses the challenge of real-time\nobject detection with dynamic computational delay. The core innovation of\nTranstreaming lies in its adaptive delay-aware transformer, which can\nconcurrently predict multiple future frames and select the output that best\nmatches the real-world present time, compensating for any system-induced\ncomputation delays. The proposed model outperforms the existing\nstate-of-the-art methods, even in single-frame detection scenarios, by\nleveraging a transformer-based methodology. It demonstrates robust performance\nacross a range of devices, from powerful V100 to modest 2080Ti, achieving the\nhighest level of perceptual accuracy on all platforms. Unlike most\nstate-of-the-art methods that struggle to complete computation within a single\nframe on less powerful devices, Transtreaming meets the stringent real-time\nprocessing requirements on all kinds of devices. The experimental results\nemphasize the system's adaptability and its potential to significantly improve\nthe safety and reliability for many real-world systems, such as autonomous\ndriving.\n","authors":["Xiang Zhang","Yufei Cui","Chenchen Fu","Weiwei Wu","Zihao Wang","Yuyang Sun","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2409.06584v1.pdf","comment":"Submitted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.06583v1","updated":"2024-09-10T15:22:05Z","published":"2024-09-10T15:22:05Z","title":"Semi-Supervised 3D Object Detection with Chanel Augmentation using\n  Transformation Equivariance","summary":"  Accurate 3D object detection is crucial for autonomous vehicles and robots to\nnavigate and interact with the environment safely and effectively. Meanwhile,\nthe performance of 3D detector relies on the data size and annotation which is\nexpensive. Consequently, the demand of training with limited labeled data is\ngrowing. We explore a novel teacher-student framework employing channel\naugmentation for 3D semi-supervised object detection. The teacher-student SSL\ntypically adopts a weak augmentation and strong augmentation to teacher and\nstudent, respectively. In this work, we apply multiple channel augmentations to\nboth networks using the transformation equivariance detector (TED). The TED\nallows us to explore different combinations of augmentation on point clouds and\nefficiently aggregates multi-channel transformation equivariance features. In\nprinciple, by adopting fixed channel augmentations for the teacher network, the\nstudent can train stably on reliable pseudo-labels. Adopting strong channel\naugmentations can enrich the diversity of data, fostering robustness to\ntransformations and enhancing generalization performance of the student\nnetwork. We use SOTA hierarchical supervision as a baseline and adapt its\ndual-threshold to TED, which is called channel IoU consistency. We evaluate our\nmethod with KITTI dataset, and achieved a significant performance leap,\nsurpassing SOTA 3D semi-supervised object detection models.\n","authors":["Minju Kang","Taehun Kong","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2409.06583v1.pdf","comment":"Accepted to 2024 IEEE International Conference on Image Processing\n  (ICIP)"},{"id":"http://arxiv.org/abs/2409.06579v1","updated":"2024-09-10T15:19:40Z","published":"2024-09-10T15:19:40Z","title":"Quantifying and Enabling the Interpretability of CLIP-like Models","summary":"  CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. To bridge this gap we propose a study to quantify the interpretability\nin CLIP like models. We conduct this study on six different CLIP models from\nOpenAI and OpenCLIP which vary by size, type of pre-training data and patch\nsize. Our approach begins with using the TEXTSPAN algorithm and in-context\nlearning to break down individual attention heads into specific properties. We\nthen evaluate how easily these heads can be interpreted using new metrics which\nmeasure property consistency within heads and property disentanglement across\nheads. Our findings reveal that larger CLIP models are generally more\ninterpretable than their smaller counterparts. To further assist users in\nunderstanding the inner workings of CLIP models, we introduce CLIP-InterpreT, a\ntool designed for interpretability analysis. CLIP-InterpreT offers five types\nof analyses: property-based nearest neighbor search, per-head topic\nsegmentation, contrastive segmentation, per-head nearest neighbors of an image,\nand per-head nearest neighbors of text.\n","authors":["Avinash Madasu","Yossi Gandelsman","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2409.06579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11836v4","updated":"2024-09-10T15:00:14Z","published":"2024-08-06T22:09:50Z","title":"Analysis of Unstructured High-Density Crowded Scenes for Crowd\n  Monitoring","summary":"  We are interested in developing an automated system for detection of\norganized movements in human crowds. Computer vision algorithms can extract\ninformation from videos of crowded scenes and automatically detect and track\ngroups of individuals undergoing organized motion that represents an anomalous\nbehavior in the context of conflict aversion. Our system can detect organized\ncohorts against the background of randomly moving objects and we can estimate\nthe number of participants in an organized cohort, the speed and direction of\nmotion in real time, within three to four video frames, which is less than one\nsecond from the onset of motion captured on a CCTV. We have performed\npreliminary analysis in this context in biological cell data containing up to\nfour thousand objects per frame and will extend this numerically to a\nhundred-fold for public safety applications.\n  We envisage using the existing infrastructure of video cameras for acquiring\nimage datasets on-the-fly and deploying an easy-to-use data-driven software\nsystem for parsing of significant events by analyzing image sequences taken\ninside and outside of sports stadiums or other public venues. Other prospective\nusers are organizers of political rallies, civic and wildlife organizations,\nsecurity firms, and the military. We will optimize the performance of the\nsoftware by implementing a classification method able to distinguish between\nactivities posing a threat and those not posing a threat.\n","authors":["Alexandre Matov"],"pdf_url":"https://arxiv.org/pdf/2408.11836v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13342v2","updated":"2024-09-10T14:56:38Z","published":"2024-07-18T09:40:24Z","title":"Implicit Filtering for Learning Neural Signed Distance Functions from 3D\n  Point Clouds","summary":"  Neural signed distance functions (SDFs) have shown powerful ability in\nfitting the shape geometry. However, inferring continuous signed distance\nfields from discrete unoriented point clouds still remains a challenge. The\nneural network typically fits the shape with a rough surface and omits\nfine-grained geometric details such as shape edges and corners. In this paper,\nwe propose a novel non-linear implicit filter to smooth the implicit field\nwhile preserving high-frequency geometry details. Our novelty lies in that we\ncan filter the surface (zero level set) by the neighbor input points with\ngradients of the signed distance field. By moving the input raw point clouds\nalong the gradient, our proposed implicit filtering can be extended to non-zero\nlevel sets to keep the promise consistency between different level sets, which\nconsequently results in a better regularization of the zero level set. We\nconduct comprehensive experiments in surface reconstruction from objects and\ncomplex scene point clouds, the numerical and visual comparisons demonstrate\nour improvements over the state-of-the-art methods under the widely used\nbenchmarks.\n","authors":["Shengtao Li","Ge Gao","Yudong Liu","Ming Gu","Yu-Shen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13342v2.pdf","comment":"Accepted by ECCV 2024. Project page:\n  https://list17.github.io/ImplicitFilter"},{"id":"http://arxiv.org/abs/2407.07805v4","updated":"2024-09-10T14:49:18Z","published":"2024-07-10T16:25:26Z","title":"SUMix: Mixup with Semantic and Uncertain Information","summary":"  Mixup data augmentation approaches have been applied for various tasks of\ndeep learning to improve the generalization ability of deep neural networks.\nSome existing approaches CutMix, SaliencyMix, etc. randomly replace a patch in\none image with patches from another to generate the mixed image. Similarly, the\ncorresponding labels are linearly combined by a fixed ratio $\\lambda$ by l. The\nobjects in two images may be overlapped during the mixing process, so some\nsemantic information is corrupted in the mixed samples. In this case, the mixed\nimage does not match the mixed label information. Besides, such a label may\nmislead the deep learning model training, which results in poor performance. To\nsolve this problem, we proposed a novel approach named SUMix to learn the\nmixing ratio as well as the uncertainty for the mixed samples during the\ntraining process. First, we design a learnable similarity function to compute\nan accurate mix ratio. Second, an approach is investigated as a regularized\nterm to model the uncertainty of the mixed samples. We conduct experiments on\nfive image benchmarks, and extensive experimental results imply that our method\nis capable of improving the performance of classifiers with different\ncutting-based mixup approaches. The source code is available at\nhttps://github.com/JinXins/SUMix.\n","authors":["Huafeng Qin","Xin Jin","Hongyu Zhu","Hongchao Liao","Mounîm A. El-Yacoubi","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2407.07805v4.pdf","comment":"Accepted by ECCV2024 [Camera Ready] (19 pages, 7 figures) with the\n  source code at https://github.com/JinXins/SUMix"},{"id":"http://arxiv.org/abs/2407.09733v2","updated":"2024-09-10T14:34:44Z","published":"2024-07-13T00:45:37Z","title":"Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity","summary":"  In this paper, we introduce Textured-GS, an innovative method for rendering\nGaussian splatting that incorporates spatially defined color and opacity\nvariations using Spherical Harmonics (SH). This approach enables each Gaussian\nto exhibit a richer representation by accommodating varying colors and\nopacities across its surface, significantly enhancing rendering quality\ncompared to traditional methods. To demonstrate the merits of our approach, we\nhave adapted the Mini-Splatting architecture to integrate textured Gaussians\nwithout increasing the number of Gaussians. Our experiments across multiple\nreal-world datasets show that Textured-GS consistently outperforms both the\nbaseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The\nresults highlight the potential of Textured-GS to advance Gaussian-based\nrendering technologies, promising more efficient and high-quality scene\nreconstructions.\n","authors":["Zhentao Huang","Minglun Gong"],"pdf_url":"https://arxiv.org/pdf/2407.09733v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2403.14465v2","updated":"2024-09-10T14:21:28Z","published":"2024-03-21T15:13:36Z","title":"CathFlow: Self-Supervised Segmentation of Catheters in Interventional\n  Ultrasound Using Optical Flow and Transformers","summary":"  In minimally invasive endovascular procedures, contrast-enhanced angiography\nremains the most robust imaging technique. However, it is at the expense of the\npatient and clinician's health due to prolonged radiation exposure. As an\nalternative, interventional ultrasound has notable benefits such as being\nradiation-free, fast to deploy, and having a small footprint in the operating\nroom. Yet, ultrasound is hard to interpret, and highly prone to artifacts and\nnoise. Additionally, interventional radiologists must undergo extensive\ntraining before they become qualified to diagnose and treat patients\neffectively, leading to a shortage of staff, and a lack of open-source\ndatasets. In this work, we seek to address both problems by introducing a\nself-supervised deep learning architecture to segment catheters in longitudinal\nultrasound images, without demanding any labeled data. The network architecture\nbuilds upon AiAReSeg, a segmentation transformer built with the Attention in\nAttention mechanism, and is capable of learning feature changes across time and\nspace. To facilitate training, we used synthetic ultrasound data based on\nphysics-driven catheter insertion simulations, and translated the data into a\nunique CT-Ultrasound common domain, CACTUSS, to improve the segmentation\nperformance. We generated ground truth segmentation masks by computing the\noptical flow between adjacent frames using FlowNet2, and performed thresholding\nto obtain a binary map estimate. Finally, we validated our model on a test\ndataset, consisting of unseen synthetic data and images collected from silicon\naorta phantoms, thus demonstrating its potential for applications to clinical\ndata in the future.\n","authors":["Alex Ranne","Liming Kuang","Yordanka Velikova","Nassir Navab","Ferdinando Rodriguez y Baena"],"pdf_url":"https://arxiv.org/pdf/2403.14465v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2409.06535v1","updated":"2024-09-10T14:09:39Z","published":"2024-09-10T14:09:39Z","title":"PoseEmbroider: Towards a 3D, Visual, Semantic-aware Human Pose\n  Representation","summary":"  Aligning multiple modalities in a latent space, such as images and texts, has\nshown to produce powerful semantic visual representations, fueling tasks like\nimage captioning, text-to-image generation, or image grounding. In the context\nof human-centric vision, albeit CLIP-like representations encode most standard\nhuman poses relatively well (such as standing or sitting), they lack sufficient\nacuteness to discern detailed or uncommon ones. Actually, while 3D human poses\nhave been often associated with images (e.g. to perform pose estimation or\npose-conditioned image generation), or more recently with text (e.g. for\ntext-to-pose generation), they have seldom been paired with both. In this work,\nwe combine 3D poses, person's pictures and textual pose descriptions to produce\nan enhanced 3D-, visual- and semantic-aware human pose representation. We\nintroduce a new transformer-based model, trained in a retrieval fashion, which\ncan take as input any combination of the aforementioned modalities. When\ncomposing modalities, it outperforms a standard multi-modal alignment retrieval\nmodel, making it possible to sort out partial information (e.g. image with the\nlower body occluded). We showcase the potential of such an embroidered pose\nrepresentation for (1) SMPL regression from image with optional text cue; and\n(2) on the task of fine-grained instruction generation, which consists in\ngenerating a text that describes how to move from one 3D pose to another (as a\nfitness coach). Unlike prior works, our model can take any kind of input (image\nand/or pose) without retraining.\n","authors":["Ginger Delmas","Philippe Weinzaepfel","Francesc Moreno-Noguer","Grégory Rogez"],"pdf_url":"https://arxiv.org/pdf/2409.06535v1.pdf","comment":"Published in ECCV 2024"},{"id":"http://arxiv.org/abs/2409.06520v1","updated":"2024-09-10T13:55:47Z","published":"2024-09-10T13:55:47Z","title":"In Flight Boresight Rectification for Lightweight Airborne Pushbroom\n  Imaging Spectrometry","summary":"  Hyperspectral cameras have recently been miniaturized for operation on\nlightweight airborne platforms such as UAV or small aircraft. Unlike frame\ncameras (RGB or Multispectral), many hyperspectral sensors use a linear array\nor 'push-broom' scanning design. This design presents significant challenges\nfor image rectification and the calibration of the intrinsic and extrinsic\ncamera parameters. Typically, methods employed to address such tasks rely on a\nprecise GPS/INS estimate of the airborne platform trajectory and a detailed\nterrain model. However, inaccuracies in the trajectory or surface model\ninformation can introduce systematic errors and complicate geometric modeling\nwhich ultimately degrade the quality of the rectification. To overcome these\nchallenges, we propose a method for tie point extraction and camera calibration\nfor 'push-broom' hyperspectral sensors using only the raw spectral imagery and\nraw, possibly low quality, GPS/INS trajectory. We demonstrate that our approach\nallows for the automatic calibration of airborne systems with hyperspectral\ncameras, outperforms other state-of-the-art automatic rectification methods and\nreaches an accuracy on par with manual calibration methods.\n","authors":["Julien Yuuki Burkhard","Jesse Ray Murray Lahaye","Laurent Valentin Jospin","Jan Skaloud"],"pdf_url":"https://arxiv.org/pdf/2409.06520v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.05610v2","updated":"2024-09-10T13:51:00Z","published":"2024-02-08T12:08:52Z","title":"Extending 6D Object Pose Estimators for Stereo Vision","summary":"  Estimating the 6D pose of objects accurately, quickly, and robustly remains a\ndifficult task. However, recent methods for directly regressing poses from RGB\nimages using dense features have achieved state-of-the-art results. Stereo\nvision, which provides an additional perspective on the object, can help reduce\npose ambiguity and occlusion. Moreover, stereo can directly infer the distance\nof an object, while mono-vision requires internalized knowledge of the object's\nsize. To extend the state-of-the-art in 6D object pose estimation to stereo, we\ncreated a BOP compatible stereo version of the YCB-V dataset. Our method\noutperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo\nvision and can easily be adopted for other dense feature-based algorithms.\n","authors":["Thomas Pöllabauer","Jan Emrich","Volker Knauthe","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2402.05610v2.pdf","comment":"4th International Conference on Pattern Recognition and Artificial\n  Intelligence (ICPRAI)"},{"id":"http://arxiv.org/abs/2407.18038v3","updated":"2024-09-10T13:48:23Z","published":"2024-07-25T13:31:55Z","title":"TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework","summary":"  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n","authors":["Guanfeng Tang","Zhiyuan Wu","Jiahang Li","Ping Zhong","Xieyuanli Chen","Huiming Lu","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2407.18038v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06509v1","updated":"2024-09-10T13:41:08Z","published":"2024-09-10T13:41:08Z","title":"Aligning Machine and Human Visual Representations across Abstraction\n  Levels","summary":"  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n","authors":["Lukas Muttenthaler","Klaus Greff","Frieda Born","Bernhard Spitzer","Simon Kornblith","Michael C. Mozer","Klaus-Robert Müller","Thomas Unterthiner","Andrew K. Lampinen"],"pdf_url":"https://arxiv.org/pdf/2409.06509v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2406.08113v2","updated":"2024-09-10T13:41:00Z","published":"2024-06-12T11:50:51Z","title":"Valeo4Cast: A Modular Approach to End-to-End Forecasting","summary":"  Motion forecasting is crucial in autonomous driving systems to anticipate the\nfuture trajectories of surrounding agents such as pedestrians, vehicles, and\ntraffic signals. In end-to-end forecasting, the model must jointly detect and\ntrack from sensor data (cameras or LiDARs) the past trajectories of the\ndifferent elements of the scene and predict their future locations. We depart\nfrom the current trend of tackling this task via end-to-end training from\nperception to forecasting, and instead use a modular approach. We individually\nbuild and train detection, tracking and forecasting modules. We then only use\nconsecutive finetuning steps to integrate the modules better and alleviate\ncompounding errors. We conduct an in-depth study on the finetuning strategies\nand it reveals that our simple yet effective approach significantly improves\nperformance on the end-to-end forecasting benchmark. Consequently, our solution\nranks first in the Argoverse 2 End-to-end Forecasting Challenge, with 63.82\nmAPf. We surpass forecasting results by +17.1 points over last year's winner\nand by +13.3 points over this year's runner-up. This remarkable performance in\nforecasting can be explained by our modular paradigm, which integrates\nfinetuning strategies and significantly outperforms the end-to-end-trained\ncounterparts.\n","authors":["Yihong Xu","Éloi Zablocki","Alexandre Boulch","Gilles Puy","Mickael Chen","Florent Bartoccioni","Nermin Samet","Oriane Siméoni","Spyros Gidaris","Tuan-Hung Vu","Andrei Bursuc","Eduardo Valle","Renaud Marlet","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08113v2.pdf","comment":"Winning solution of the Argoverse 2 \"Unified Detection, Tracking, and\n  Forecasting\" challenge; work accepted at Road++ ECCVW 2024"},{"id":"http://arxiv.org/abs/2409.06506v1","updated":"2024-09-10T13:40:34Z","published":"2024-09-10T13:40:34Z","title":"Neural Laplacian Operator for 3D Point Clouds","summary":"  The discrete Laplacian operator holds a crucial role in 3D geometry\nprocessing, yet it is still challenging to define it on point clouds. Previous\nworks mainly focused on constructing a local triangulation around each point to\napproximate the underlying manifold for defining the Laplacian operator, which\nmay not be robust or accurate. In contrast, we simply use the K-nearest\nneighbors (KNN) graph constructed from the input point cloud and learn the\nLaplacian operator on the KNN graph with graph neural networks (GNNs). However,\nthe ground-truth Laplacian operator is defined on a manifold mesh with a\ndifferent connectivity from the KNN graph and thus cannot be directly used for\ntraining. To train the GNN, we propose a novel training scheme by imitating the\nbehavior of the ground-truth Laplacian operator on a set of probe functions so\nthat the learned Laplacian operator behaves similarly to the ground-truth\nLaplacian operator. We train our network on a subset of ShapeNet and evaluate\nit across a variety of point clouds. Compared with previous methods, our method\nreduces the error by an order of magnitude and excels in handling sparse point\nclouds with thin structures or sharp features. Our method also demonstrates a\nstrong generalization ability to unseen shapes. With our learned Laplacian\noperator, we further apply a series of Laplacian-based geometry processing\nalgorithms directly to point clouds and achieve accurate results, enabling many\nexciting possibilities for geometry processing on point clouds. The code and\ntrained models are available at https://github.com/IntelligentGeometry/NeLo.\n","authors":["Bo Pang","Zhongtian Zheng","Yilong Li","Guoping Wang","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2409.06506v1.pdf","comment":"SIGGRAPH Asia 2024 (Journal Track)"},{"id":"http://arxiv.org/abs/2401.07951v2","updated":"2024-09-10T13:33:37Z","published":"2024-01-15T20:23:05Z","title":"Image Similarity using An Ensemble of Context-Sensitive Models","summary":"  Image similarity has been extensively studied in computer vision. In recent\nyears, machine-learned models have shown their ability to encode more semantics\nthan traditional multivariate metrics. However, in labelling semantic\nsimilarity, assigning a numerical score to a pair of images is impractical,\nmaking the improvement and comparisons on the task difficult. In this work, we\npresent a more intuitive approach to build and compare image similarity models\nbased on labelled data in the form of A:R vs B:R, i.e., determining if an image\nA is closer to a reference image R than another image B. We address the\nchallenges of sparse sampling in the image space (R, A, B) and biases in the\nmodels trained with context-based data by using an ensemble model. Our testing\nresults show that the ensemble model constructed performs ~5% better than the\nbest individual context-sensitive models. They also performed better than the\nmodels that were directly fine-tuned using mixed imagery data as well as\nexisting deep embeddings, e.g., CLIP and DINO. This work demonstrates that\ncontext-based labelling and model training can be effective when an appropriate\nensemble approach is used to alleviate the limitation due to sparse sampling.\n","authors":["Zukang Liao","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2401.07951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11239v2","updated":"2024-09-10T13:28:58Z","published":"2023-10-17T13:08:24Z","title":"LiDAR-based 4D Occupancy Completion and Forecasting","summary":"  Scene completion and forecasting are two popular perception problems in\nresearch for mobile agents like autonomous vehicles. Existing approaches treat\nthe two problems in isolation, resulting in a separate perception of the two\naspects. In this paper, we introduce a novel LiDAR perception task of Occupancy\nCompletion and Forecasting (OCF) in the context of autonomous driving to unify\nthese aspects into a cohesive framework. This task requires new algorithms to\naddress three challenges altogether: (1) sparse-to-dense reconstruction, (2)\npartial-to-complete hallucination, and (3) 3D-to-4D prediction. To enable\nsupervision and evaluation, we curate a large-scale dataset termed OCFBench\nfrom public autonomous driving datasets. We analyze the performance of closely\nrelated existing baseline models and our own ones on our dataset. We envision\nthat this research will inspire and call for further investigation in this\nevolving and crucial area of 4D perception. Our code for data curation and\nbaseline implementation is available at https://github.com/ai4ce/Occ4cast.\n","authors":["Xinhao Liu","Moonjun Gong","Qi Fang","Haoyu Xie","Yiming Li","Hang Zhao","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2310.11239v2.pdf","comment":"IROS 2024"},{"id":"http://arxiv.org/abs/2409.03767v2","updated":"2024-09-10T13:23:57Z","published":"2024-08-21T02:15:26Z","title":"EMCNet : Graph-Nets for Electron Micrographs Classification","summary":"  Characterization of materials via electron micrographs is an important and\nchallenging task in several materials processing industries. Classification of\nelectron micrographs is complex due to the high intra-class dissimilarity, high\ninter-class similarity, and multi-spatial scales of patterns. However, existing\nmethods are ineffective in learning complex image patterns. We propose an\neffective end-to-end electron micrograph representation learning-based\nframework for nanomaterial identification to overcome the challenges. We\ndemonstrate that our framework outperforms the popular baselines on the\nopen-source datasets in nanomaterials-based identification tasks. The ablation\nstudies are reported in great detail to support the efficacy of our approach.\n","authors":["Sakhinana Sagar Srinivas","Rajat Kumar Sarkar","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2409.03767v2.pdf","comment":"12 pages, 10 figures, Accepted in a ACM SIGKDD 2022 Workshop on\n  Machine Learning for Materials"},{"id":"http://arxiv.org/abs/2408.05211v2","updated":"2024-09-10T13:21:08Z","published":"2024-08-09T17:59:49Z","title":"VITA: Towards Open-Source Interactive Omni Multimodal LLM","summary":"  The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. VITA is the first step for the open-source community to\nexplore the seamless integration of multimodal understanding and interaction.\nWhile there is still lots of work to be done on VITA to get close to\nclose-source counterparts, we hope that its role as a pioneer can serve as a\ncornerstone for subsequent research. Project Page: https://vita-home.github.io.\n","authors":["Chaoyou Fu","Haojia Lin","Zuwei Long","Yunhang Shen","Meng Zhao","Yifan Zhang","Shaoqi Dong","Xiong Wang","Di Yin","Long Ma","Xiawu Zheng","Ran He","Rongrong Ji","Yunsheng Wu","Caifeng Shan","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.05211v2.pdf","comment":"Project Page: https://vita-home.github.io"},{"id":"http://arxiv.org/abs/2210.11795v3","updated":"2024-09-10T13:19:28Z","published":"2022-10-21T08:18:49Z","title":"PoseScript: Linking 3D Human Poses and Natural Language","summary":"  Natural language plays a critical role in many computer vision applications,\nsuch as image captioning, visual question answering, and cross-modal retrieval,\nto provide fine-grained semantic information. Unfortunately, while human pose\nis key to human understanding, current 3D human pose datasets lack detailed\nlanguage descriptions. To address this issue, we have introduced the PoseScript\ndataset. This dataset pairs more than six thousand 3D human poses from AMASS\nwith rich human-annotated descriptions of the body parts and their spatial\nrelationships. Additionally, to increase the size of the dataset to a scale\nthat is compatible with data-hungry learning algorithms, we have proposed an\nelaborate captioning process that generates automatic synthetic descriptions in\nnatural language from given 3D keypoints. This process extracts low-level pose\ninformation, known as \"posecodes\", using a set of simple but generic rules on\nthe 3D keypoints. These posecodes are then combined into higher level textual\ndescriptions using syntactic rules. With automatic annotations, the amount of\navailable data significantly scales up (100k), making it possible to\neffectively pretrain deep models for finetuning on human captions. To showcase\nthe potential of annotated poses, we present three multi-modal learning tasks\nthat utilize the PoseScript dataset. Firstly, we develop a pipeline that maps\n3D poses and textual descriptions into a joint embedding space, allowing for\ncross-modal retrieval of relevant poses from large-scale datasets. Secondly, we\nestablish a baseline for a text-conditioned model generating 3D poses. Thirdly,\nwe present a learned process for generating pose descriptions. These\napplications demonstrate the versatility and usefulness of annotated poses in\nvarious tasks and pave the way for future research in the field.\n","authors":["Ginger Delmas","Philippe Weinzaepfel","Thomas Lucas","Francesc Moreno-Noguer","Grégory Rogez"],"pdf_url":"https://arxiv.org/pdf/2210.11795v3.pdf","comment":"TPAMI 2024, extended version of the ECCV 2022 paper"},{"id":"http://arxiv.org/abs/2409.06485v1","updated":"2024-09-10T13:13:14Z","published":"2024-09-10T13:13:14Z","title":"Mitigating Hallucination in Visual-Language Models via Re-Balancing\n  Contrastive Decoding","summary":"  Although Visual-Language Models (VLMs) have shown impressive capabilities in\ntasks like visual question answering and image captioning, they still struggle\nwith hallucinations. Analysis of attention distribution in these models shows\nthat VLMs tend to processing textual tokens rather than visual tokens. This\nimbalance of attention distribution causes VLMs to favor textual knowledge in\nthe case of multimodal knowledge conflicts, resulting in differences from the\nimage information. In this paper, we propose Re-Balancing Contrastive Decoding\n(RBD) method, which employs textual and visual branches to recalibrate\nattention distribution in VLMs. Specifically, the textual branch injects image\nnoise to stimulate the model's dependency on text, thereby reducing textual\nbias. Concurrently, the visual branch focuses on the selection of significant\ntokens, refining the attention mechanism to highlight the primary subject. This\ndual-branch strategy enables the RBD method to diminish textual bias while\nenhancing visual information. Experimental results demonstrate that our method,\nRBD, outperforms the existing methods by the CHAIR and POPE metrics, mitigate\nhallucinations without reducing the model's general capabilities.\n","authors":["Xiaoyu Liang","Jiayuan Yu","Lianrui Mu","Jiedong Zhuang","Jiaqi Hu","Yuchen Yang","Jiangnan Ye","Lu Lu","Jian Chen","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2409.06485v1.pdf","comment":"PRCV"},{"id":"http://arxiv.org/abs/2409.06476v1","updated":"2024-09-10T13:05:31Z","published":"2024-09-10T13:05:31Z","title":"Multi-scale Cycle Tracking in Dynamic Planar Graphs","summary":"  This paper presents a nested tracking framework for analyzing cycles in 2D\nforce networks within granular materials. These materials are composed of\ninteracting particles, whose interactions are described by a force network.\nUnderstanding the cycles within these networks at various scales and their\nevolution under external loads is crucial, as they significantly contribute to\nthe mechanical and kinematic properties of the system. Our approach involves\ncomputing a cycle hierarchy by partitioning the 2D domain into segments bounded\nby cycles in the force network. We can adapt concepts from nested tracking\ngraphs originally developed for merge trees by leveraging the duality between\nthis partitioning and the cycles. We demonstrate the effectiveness of our\nmethod on two force networks derived from experiments with photoelastic disks.\n","authors":["Farhan Rasheed","Abrar Naseer","Emma Nilsson","Talha Bin Masood","Ingrid Hotz"],"pdf_url":"https://arxiv.org/pdf/2409.06476v1.pdf","comment":"TopoInVis 2024, 11 pages"},{"id":"http://arxiv.org/abs/2407.06315v3","updated":"2024-09-10T12:59:47Z","published":"2024-07-08T18:31:19Z","title":"Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models","summary":"  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n","authors":["Mujtaba Hussain Mirza","Maria Rosaria Briglia","Senad Beadini","Iacopo Masi"],"pdf_url":"https://arxiv.org/pdf/2407.06315v3.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2409.06471v1","updated":"2024-09-10T12:57:16Z","published":"2024-09-10T12:57:16Z","title":"Weakly-supervised Camera Localization by Ground-to-satellite Image\n  Registration","summary":"  The ground-to-satellite image matching/retrieval was initially proposed for\ncity-scale ground camera localization. This work addresses the problem of\nimproving camera pose accuracy by ground-to-satellite image matching after a\ncoarse location and orientation have been obtained, either from the city-scale\nretrieval or from consumer-level GPS and compass sensors. Existing\nlearning-based methods for solving this task require accurate GPS labels of\nground images for network training. However, obtaining such accurate GPS labels\nis difficult, often requiring an expensive {\\color{black}Real Time Kinematics\n(RTK)} setup and suffering from signal occlusion, multi-path signal\ndisruptions, \\etc. To alleviate this issue, this paper proposes a weakly\nsupervised learning strategy for ground-to-satellite image registration when\nonly noisy pose labels for ground images are available for network training. It\nderives positive and negative satellite images for each ground image and\nleverages contrastive learning to learn feature representations for ground and\nsatellite images useful for translation estimation. We also propose a\nself-supervision strategy for cross-view image relative rotation estimation,\nwhich trains the network by creating pseudo query and reference image pairs.\nExperimental results show that our weakly supervised learning strategy achieves\nthe best performance on cross-area evaluation compared to recent\nstate-of-the-art methods that are reliant on accurate pose labels for\nsupervision.\n","authors":["Yujiao Shi","Hongdong Li","Akhil Perincherry","Ankit Vora"],"pdf_url":"https://arxiv.org/pdf/2409.06471v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2409.02813v2","updated":"2024-09-10T12:55:31Z","published":"2024-09-04T15:31:26Z","title":"MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  This paper introduces MMMU-Pro, a robust version of the Massive\nMulti-discipline Multimodal Understanding and Reasoning (MMMU) benchmark.\nMMMU-Pro rigorously assesses multimodal models' true understanding and\nreasoning capabilities through a three-step process based on MMMU: (1)\nfiltering out questions answerable by text-only models, (2) augmenting\ncandidate options, and (3) introducing a vision-only input setting where\nquestions are embedded within images. This setting challenges AI to truly \"see\"\nand \"read\" simultaneously, testing a fundamental human cognitive skill of\nseamlessly integrating visual and textual information. Results show that model\nperformance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8%\nto 26.9% across models. We explore the impact of OCR prompts and Chain of\nThought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT\ngenerally improves performance. MMMU-Pro provides a more rigorous evaluation\ntool, closely mimicking real-world scenarios and offering valuable directions\nfor future research in multimodal AI.\n","authors":["Xiang Yue","Tianyu Zheng","Yuansheng Ni","Yubo Wang","Kai Zhang","Shengbang Tong","Yuxuan Sun","Botao Yu","Ge Zhang","Huan Sun","Yu Su","Wenhu Chen","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2409.02813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06455v1","updated":"2024-09-10T12:21:54Z","published":"2024-09-10T12:21:54Z","title":"Continual Domain Incremental Learning for Privacy-aware Digital\n  Pathology","summary":"  In recent years, there has been remarkable progress in the field of digital\npathology, driven by the ability to model complex tissue patterns using\nadvanced deep-learning algorithms. However, the robustness of these models is\noften severely compromised in the presence of data shifts (e.g., different\nstains, organs, centers, etc.). Alternatively, continual learning (CL)\ntechniques aim to reduce the forgetting of past data when learning new data\nwith distributional shift conditions. Specifically, rehearsal-based CL\ntechniques, which store some past data in a buffer and then replay it with new\ndata, have proven effective in medical image analysis tasks. However, privacy\nconcerns arise as these approaches store past data, prompting the development\nof our novel Generative Latent Replay-based CL (GLRCL) approach. GLRCL captures\nthe previous distribution through Gaussian Mixture Models instead of storing\npast samples, which are then utilized to generate features and perform latent\nreplay with new data. We systematically evaluate our proposed framework under\ndifferent shift conditions in histopathology data, including stain and organ\nshift. Our approach significantly outperforms popular buffer-free CL approaches\nand performs similarly to rehearsal-based CL approaches that require large\nbuffers causing serious privacy violations.\n","authors":["Pratibha Kumari","Daniel Reisenbüchler","Lucas Luttner","Nadine S. Schaadt","Friedrich Feuerhake","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2409.06455v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2409.06445v1","updated":"2024-09-10T12:00:40Z","published":"2024-09-10T12:00:40Z","title":"Learning Generative Interactive Environments By Trained Agent\n  Exploration","summary":"  World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .\n","authors":["Naser Kazemi","Nedko Savov","Danda Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2409.06445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09654v2","updated":"2024-09-10T11:58:23Z","published":"2024-04-15T10:42:22Z","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection","summary":"  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Fang Deng","Beng Chin Ooi","Junran Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09654v2.pdf","comment":"Accepted by MM'24 (Oral)"},{"id":"http://arxiv.org/abs/2409.06443v1","updated":"2024-09-10T11:49:28Z","published":"2024-09-10T11:49:28Z","title":"Knowledge Distillation via Query Selection for Detection Transformer","summary":"  Transformers have revolutionized the object detection landscape by\nintroducing DETRs, acclaimed for their simplicity and efficacy. Despite their\nadvantages, the substantial size of these models poses significant challenges\nfor practical deployment, particularly in resource-constrained environments.\nThis paper addresses the challenge of compressing DETR by leveraging knowledge\ndistillation, a technique that holds promise for maintaining model performance\nwhile reducing size. A critical aspect of DETRs' performance is their reliance\non queries to interpret object representations accurately. Traditional\ndistillation methods often focus exclusively on positive queries, identified\nthrough bipartite matching, neglecting the rich information present in\nhard-negative queries. Our visual analysis indicates that hard-negative\nqueries, focusing on foreground elements, are crucial for enhancing\ndistillation outcomes. To this end, we introduce a novel Group Query Selection\nstrategy, which diverges from traditional query selection in DETR distillation\nby segmenting queries based on their Generalized Intersection over Union (GIoU)\nwith ground truth objects, thereby uncovering valuable hard-negative queries\nfor distillation. Furthermore, we present the Knowledge Distillation via Query\nSelection for DETR (QSKD) framework, which incorporates Attention-Guided\nFeature Distillation (AGFD) and Local Alignment Prediction Distillation (LAPD).\nThese components optimize the distillation process by focusing on the most\ninformative aspects of the teacher model's intermediate features and output.\nOur comprehensive experimental evaluation of the MS-COCO dataset demonstrates\nthe effectiveness of our approach, significantly improving average precision\n(AP) across various DETR architectures without incurring substantial\ncomputational costs. Specifically, the AP of Conditional DETR ResNet-18\nincreased from 35.8 to 39.9.\n","authors":["Yi Liu","Luting Wang","Zongheng Tang","Yue Liao","Yifan Sun","Lijun Zhang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2409.06443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06442v1","updated":"2024-09-10T11:48:05Z","published":"2024-09-10T11:48:05Z","title":"Prompt2Fashion: An automatically generated fashion dataset","summary":"  Despite the rapid evolution and increasing efficacy of language and vision\ngenerative models, there remains a lack of comprehensive datasets that bridge\nthe gap between personalized fashion needs and AI-driven design, limiting the\npotential for truly inclusive and customized fashion solutions. In this work,\nwe leverage generative models to automatically construct a fashion image\ndataset tailored to various occasions, styles, and body types as instructed by\nusers. We use different Large Language Models (LLMs) and prompting strategies\nto offer personalized outfits of high aesthetic quality, detail, and relevance\nto both expert and non-expert users' requirements, as demonstrated by\nqualitative analysis. Up until now the evaluation of the generated outfits has\nbeen conducted by non-expert human subjects. Despite the provided fine-grained\ninsights on the quality and relevance of generation, we extend the discussion\non the importance of expert knowledge for the evaluation of artistic\nAI-generated datasets such as this one. Our dataset is publicly available on\nGitHub at https://github.com/georgiarg/Prompt2Fashion.\n","authors":["Georgia Argyro","Angeliki Dimitriou","Maria Lymperaiou","Giorgos Filandrianos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.06442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06424v1","updated":"2024-09-10T11:10:32Z","published":"2024-09-10T11:10:32Z","title":"A Likelihood Ratio-Based Approach to Segmenting Unknown Objects","summary":"  Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite\nfor perception systems operating in an open-world environment. Large\nfoundational models are frequently used in downstream tasks, however, their\npotential for OoD remains mostly unexplored. We seek to leverage a large\nfoundational model to achieve robust representation. Outlier supervision is a\nwidely used strategy for improving OoD detection of the existing segmentation\nnetworks. However, current approaches for outlier supervision involve\nretraining parts of the original network, which is typically disruptive to the\nmodel's learned feature representation. Furthermore, retraining becomes\ninfeasible in the case of large foundational models. Our goal is to retrain for\noutlier segmentation without compromising the strong representation space of\nthe foundational model. To this end, we propose an adaptive, lightweight\nunknown estimation module (UEM) for outlier supervision that significantly\nenhances the OoD segmentation performance without affecting the learned feature\nrepresentation of the original network. UEM learns a distribution for outliers\nand a generic distribution for known classes. Using the learned distributions,\nwe propose a likelihood-ratio-based outlier scoring function that fuses the\nconfidence of UEM with that of the pixel-wise segmentation inlier network to\ndetect unknown objects. We also propose an objective to optimize this score\ndirectly. Our approach achieves a new state-of-the-art across multiple\ndatasets, outperforming the previous best method by 5.74% average precision\npoints while having a lower false-positive rate. Importantly, strong inlier\nperformance remains unaffected.\n","authors":["Nazir Nayal","Youssef Shoeb","Fatma Güney"],"pdf_url":"https://arxiv.org/pdf/2409.06424v1.pdf","comment":"13 pages, 2 figures, and 4 tables"},{"id":"http://arxiv.org/abs/2409.06420v1","updated":"2024-09-10T11:02:35Z","published":"2024-09-10T11:02:35Z","title":"Unrevealed Threats: A Comprehensive Study of the Adversarial Robustness\n  of Underwater Image Enhancement Models","summary":"  Learning-based methods for underwater image enhancement (UWIE) have undergone\nextensive exploration. However, learning-based models are usually vulnerable to\nadversarial examples so as the UWIE models. To the best of our knowledge, there\nis no comprehensive study on the adversarial robustness of UWIE models, which\nindicates that UWIE models are potentially under the threat of adversarial\nattacks. In this paper, we propose a general adversarial attack protocol. We\nmake a first attempt to conduct adversarial attacks on five well-designed UWIE\nmodels on three common underwater image benchmark datasets. Considering the\nscattering and absorption of light in the underwater environment, there exists\na strong correlation between color correction and underwater image enhancement.\nOn the basis of that, we also design two effective UWIE-oriented adversarial\nattack methods Pixel Attack and Color Shift Attack targeting different color\nspaces. The results show that five models exhibit varying degrees of\nvulnerability to adversarial attacks and well-designed small perturbations on\ndegraded images are capable of preventing UWIE models from generating enhanced\nresults. Further, we conduct adversarial training on these models and\nsuccessfully mitigated the effectiveness of adversarial attacks. In summary, we\nreveal the adversarial vulnerability of UWIE models and propose a new\nevaluation dimension of UWIE models.\n","authors":["Siyu Zhai","Zhibo He","Xiaofeng Cong","Junming Hou","Jie Gui","Jian Wei You","Xin Gong","James Tin-Yau Kwok","Yuan Yan Tang"],"pdf_url":"https://arxiv.org/pdf/2409.06420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.02315v2","updated":"2024-09-10T10:57:14Z","published":"2023-01-05T22:10:16Z","title":"TempSAL -- Uncovering Temporal Information for Deep Saliency Prediction","summary":"  Deep saliency prediction algorithms complement the object recognition\nfeatures, they typically rely on additional information, such as scene context,\nsemantic relationships, gaze direction, and object dissimilarity. However, none\nof these models consider the temporal nature of gaze shifts during image\nobservation. We introduce a novel saliency prediction model that learns to\noutput saliency maps in sequential time intervals by exploiting human temporal\nattention patterns. Our approach locally modulates the saliency predictions by\ncombining the learned temporal maps. Our experiments show that our method\noutperforms the state-of-the-art models, including a multi-duration saliency\nmodel, on the SALICON benchmark. Our code will be publicly available on GitHub.\n","authors":["Bahar Aydemir","Ludo Hoffstetter","Tong Zhang","Mathieu Salzmann","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2301.02315v2.pdf","comment":"10 pages, 7 figures, published in CVPR 2023"},{"id":"http://arxiv.org/abs/2409.06407v1","updated":"2024-09-10T10:43:53Z","published":"2024-09-10T10:43:53Z","title":"Sources of Uncertainty in 3D Scene Reconstruction","summary":"  The process of 3D scene reconstruction can be affected by numerous\nuncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)\nand 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack\nbuilt-in mechanisms to directly address or quantify uncertainties arising from\nthe presence of noise, occlusions, confounding outliers, and imprecise camera\npose inputs. In this paper, we introduce a taxonomy that categorizes different\nsources of uncertainty inherent in these methods. Moreover, we extend NeRF- and\nGS-based methods with uncertainty estimation techniques, including learning\nuncertainty outputs and ensembles, and perform an empirical study to assess\ntheir ability to capture the sensitivity of the reconstruction. Our study\nhighlights the need for addressing various uncertainty aspects when designing\nNeRF/GS-based methods for uncertainty-aware 3D reconstruction.\n","authors":["Marcus Klasson","Riccardo Mereu","Juho Kannala","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2409.06407v1.pdf","comment":"To appear in ECCV 2024 Workshop Proceedings. Project page at\n  https://aaltoml.github.io/uncertainty-nerf-gs/"},{"id":"http://arxiv.org/abs/2408.16662v2","updated":"2024-09-10T10:19:16Z","published":"2024-08-29T16:05:22Z","title":"Space3D-Bench: Spatial 3D Question Answering Benchmark","summary":"  Answering questions about the spatial properties of the environment poses\nchallenges for existing language and vision foundation models due to a lack of\nunderstanding of the 3D world notably in terms of relationships between\nobjects. To push the field forward, multiple 3D Q&A datasets were proposed\nwhich, overall, provide a variety of questions, but they individually focus on\nparticular aspects of 3D reasoning or are limited in terms of data modalities.\nTo address this, we present Space3D-Bench - a collection of 1000 general\nspatial questions and answers related to scenes of the Replica dataset which\noffers a variety of data modalities: point clouds, posed RGB-D images,\nnavigation meshes and 3D object detections. To ensure that the questions cover\na wide range of 3D objectives, we propose an indoor spatial questions taxonomy\ninspired by geographic information systems and use it to balance the dataset\naccordingly. Moreover, we provide an assessment system that grades natural\nlanguage responses based on predefined ground-truth answers by leveraging a\nVision Language Model's comprehension of both text and images to compare the\nresponses with ground-truth textual information or relevant visual data.\nFinally, we introduce a baseline called RAG3D-Chat integrating the world\nunderstanding of foundation models with rich context retrieval, achieving an\naccuracy of 67% on the proposed dataset.\n","authors":["Emilia Szymanska","Mihai Dusmanu","Jan-Willem Buurlage","Mahdi Rad","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2408.16662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06385v1","updated":"2024-09-10T10:08:01Z","published":"2024-09-10T10:08:01Z","title":"AMNS: Attention-Weighted Selective Mask and Noise Label Suppression for\n  Text-to-Image Person Retrieval","summary":"  Text-to-image person retrieval aims to retrieve images of person given\ntextual descriptions, and most methods implicitly assume that the training\nimage-text pairs are correctly aligned, but in practice, under-correlated and\nfalse-correlated problems arise for image-text pairs due to poor image quality\nand mislabeling. Meanwhile, the random masking augmentation strategy may\nincorrectly discard semantic content resulting in the problem of generating\nnoisy pairings between image lexical elements and text descriptions. To solve\nthese two problems, we propose a new noise label suppression method and\nalleviate the problem generated by random mask through an attention-weighted\nselective mask strategy. In the proposed noise label suppression method, the\neffect of noise labels is suppressed by preventing the model from being\noverconfident by considering the inverse KL scatter loss, which is combined\nwith the weight adjustment focus loss to further improve the model's\nrecognition ability on difficult samples. On the other hand, Attention-Weighted\nSelective Mask processes the raw image through the EMA version of the image\nencoder, retaining some of the tokens with strong semantic associations with\nthe corresponding text descriptions in order to extract better features.\nNumerous experiments validate the effectiveness of our approach in terms of\ndealing with noisy problems. The code will be available soon at\nhttps://github.com/RunQing715/AMNS.git.\n","authors":["Runqing Zhang","Xue Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06381v1","updated":"2024-09-10T10:04:58Z","published":"2024-09-10T10:04:58Z","title":"A Cross-Font Image Retrieval Network for Recognizing Undeciphered Oracle\n  Bone Inscriptions","summary":"  Oracle Bone Inscription (OBI) is the earliest mature writing system known in\nChina to date, which represents a crucial stage in the development of\nhieroglyphs. Nevertheless, the substantial quantity of undeciphered OBI\ncharacters continues to pose a persistent challenge for scholars, while\nconventional methods of ancient script research are both time-consuming and\nlabor-intensive. In this paper, we propose a cross-font image retrieval network\n(CFIRN) to decipher OBI characters by establishing associations between OBI\ncharacters and other script forms, simulating the interpretive behavior of\npaleography scholars. Concretely, our network employs a siamese framework to\nextract deep features from character images of various fonts, fully exploring\nstructure clues with different resolution by designed multiscale feature\nintegration (MFI) module and multiscale refinement classifier (MRC). Extensive\nexperiments on three challenging cross-font image retrieval datasets\ndemonstrate that, given undeciphered OBI characters, our CFIRN can effectively\nachieve accurate matches with characters from other gallery fonts.\n","authors":["Zhicong Wu","Qifeng Su","Ke Gu","Xiaodong Shi"],"pdf_url":"https://arxiv.org/pdf/2409.06381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06371v1","updated":"2024-09-10T09:53:06Z","published":"2024-09-10T09:53:06Z","title":"Distilling Generative-Discriminative Representations for Very\n  Low-Resolution Face Recognition","summary":"  Very low-resolution face recognition is challenging due to the serious loss\nof informative facial details in resolution degradation. In this paper, we\npropose a generative-discriminative representation distillation approach that\ncombines generative representation with cross-resolution aligned knowledge\ndistillation. This approach facilitates very low-resolution face recognition by\njointly distilling generative and discriminative models via two distillation\nmodules. Firstly, the generative representation distillation takes the encoder\nof a diffusion model pretrained for face super-resolution as the generative\nteacher to supervise the learning of the student backbone via feature\nregression, and then freezes the student backbone. After that, the\ndiscriminative representation distillation further considers a pretrained face\nrecognizer as the discriminative teacher to supervise the learning of the\nstudent head via cross-resolution relational contrastive distillation. In this\nway, the general backbone representation can be transformed into discriminative\nhead representation, leading to a robust and discriminative student model for\nvery low-resolution face recognition. Our approach improves the recovery of the\nmissing details in very low-resolution faces and achieves better knowledge\ntransfer. Extensive experiments on face datasets demonstrate that our approach\nenhances the recognition accuracy of very low-resolution faces, showcasing its\neffectiveness and adaptability.\n","authors":["Junzheng Zhang","Weijia Guo","Bochao Liu","Ruixin Shi","Yong Li","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2409.06371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04388v3","updated":"2024-09-10T09:46:58Z","published":"2024-09-06T16:27:52Z","title":"Question-Answering Dense Video Events","summary":"  Multimodal Large Language Models (MLLMs) have shown excellent performance in\nquestion-answering of single-event videos. In this paper, we present\nquestion-answering dense video events, a novel task that requires answering and\ngrounding the dense-event questions in long videos, thus challenging MLLMs to\nfaithfully comprehend and reason about multiple events occurring over extended\ntime periods. To facilitate the study, we construct DeVE-QA - a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. We then\nbenchmark and show that existing MLLMs excelling at single-event QA struggle to\nperform well in DeVE-QA. For improvement, we propose DeVi, a novel\ntraining-free MLLM approach that highlights a hierarchical captioning module, a\ntemporal event memory module, and a self-consistency checking module to\nrespectively detect, contextualize and memorize, and ground dense-events in\nlong videos for question answering. Extensive experiments show that DeVi is\nsuperior at answering dense-event questions and grounding relevant video\nmoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1\npercent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA\nrespectively.\n","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2409.04388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06367v1","updated":"2024-09-10T09:44:38Z","published":"2024-09-10T09:44:38Z","title":"Texture-AD: An Anomaly Detection Dataset and Benchmark for Real\n  Algorithm Development","summary":"  Anomaly detection is a crucial process in industrial manufacturing and has\nmade significant advancements recently. However, there is a large variance\nbetween the data used in the development and the data collected by the\nproduction environment. Therefore, we present the Texture-AD benchmark based on\nrepresentative texture-based anomaly detection to evaluate the effectiveness of\nunsupervised anomaly detection algorithms in real-world applications. This\ndataset includes images of 15 different cloth, 14 semiconductor wafers and 10\nmetal plates acquired under different optical schemes. In addition, it includes\nmore than 10 different types of defects produced during real manufacturing\nprocesses, such as scratches, wrinkles, color variations and point defects,\nwhich are often more difficult to detect than existing datasets. All anomalous\nareas are provided with pixel-level annotations to facilitate comprehensive\nevaluation using anomaly detection models. Specifically, to adapt to diverse\nproducts in automated pipelines, we present a new evaluation method and results\nof baseline algorithms. The experimental results show that Texture-AD is a\ndifficult challenge for state-of-the-art algorithms. To our knowledge,\nTexture-AD is the first dataset to be devoted to evaluating industrial defect\ndetection algorithms in the real world. The dataset is available at\nhttps://XXX.\n","authors":["Tianwu Lei","Bohan Wang","Silin Chen","Shurong Cao","Ningmu Zou"],"pdf_url":"https://arxiv.org/pdf/2409.06367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18197v3","updated":"2024-09-10T09:22:39Z","published":"2024-06-26T09:29:05Z","title":"Human-Free Automated Prompting for Vision-Language Anomaly Detection:\n  Prompt Optimization with Meta-guiding Prompt Scheme","summary":"  Pre-trained vision-language models (VLMs) are highly adaptable to various\ndownstream tasks through few-shot learning, making prompt-based anomaly\ndetection a promising approach. Traditional methods depend on human-crafted\nprompts that require prior knowledge of specific anomaly types. Our goal is to\ndevelop a human-free prompt-based anomaly detection framework that optimally\nlearns prompts through data-driven methods, eliminating the need for human\nintervention. The primary challenge in this approach is the lack of anomalous\nsamples during the training phase. Additionally, the Vision Transformer\n(ViT)-based image encoder in VLMs is not ideal for pixel-wise anomaly\nsegmentation due to a locality feature mismatch between the original image and\nthe output feature map. To tackle the first challenge, we have developed the\nObject-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples\nfor training. Furthermore, our Meta-Guiding Prompt-Tuning Scheme (MPTS)\niteratively adjusts the gradient-based optimization direction of learnable\nprompts to avoid overfitting to the synthesized anomalies. For the second\nchallenge, we propose Locality-Aware Attention, which ensures that each local\npatch feature attends only to nearby patch features, preserving the locality\nfeatures corresponding to their original locations. This framework allows for\nthe optimal prompt embeddings by searching in the continuous latent space via\nbackpropagation, free from human semantic constraints. Additionally, the\nmodified locality-aware attention improves the precision of pixel-wise anomaly\nsegmentation.\n","authors":["Pi-Wei Chen","Jerry Chun-Wei Lin","Jia Ji","Feng-Hao Yeh","Zih-Ching Chen","Chao-Chun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06355v1","updated":"2024-09-10T09:22:35Z","published":"2024-09-10T09:22:35Z","title":"DiffQRCoder: Diffusion-based Aesthetic QR Code Generation with Scanning\n  Robustness Guided Iterative Refinement","summary":"  With the success of Diffusion Models for image generation, the technologies\nalso have revolutionized the aesthetic Quick Response (QR) code generation.\nDespite significant improvements in visual attractiveness for the beautified\ncodes, their scannabilities are usually sacrificed and thus hinder their\npractical uses in real-world scenarios. To address this issue, we propose a\nnovel Diffusion-based QR Code generator (DiffQRCoder) to effectively craft both\nscannable and visually pleasing QR codes. The proposed approach introduces\nScanning-Robust Perceptual Guidance (SRPG), a new diffusion guidance for\nDiffusion Models to guarantee the generated aesthetic codes to obey the\nground-truth QR codes while maintaining their attractiveness during the\ndenoising process. Additionally, we present another post-processing technique,\nScanning Robust Manifold Projected Gradient Descent (SR-MPGD), to further\nenhance their scanning robustness through iterative latent space optimization.\nWith extensive experiments, the results demonstrate that our approach not only\noutperforms other compared methods in Scanning Success Rate (SSR) with better\nor comparable CLIP aesthetic score (CLIP-aes.) but also significantly improves\nthe SSR of the ControlNet-only approach from 60% to 99%. The subjective\nevaluation indicates that our approach achieves promising visual attractiveness\nto users as well. Finally, even with different scanning angles and the most\nrigorous error tolerance settings, our approach robustly achieves over 95% SSR,\ndemonstrating its capability for real-world applications.\n","authors":["Jia-Wei Liao","Winston Wang","Tzu-Sian Wang","Li-Xuan Peng","Ju-Hsuan Weng","Cheng-Fu Chou","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.07610v5","updated":"2024-09-10T09:09:25Z","published":"2022-01-19T14:09:14Z","title":"Nonlinear Unknown Input Observability and Unknown Input Reconstruction:\n  The General Analytical Solution","summary":"  Observability is a fundamental structural property of any dynamic system and\ndescribes the possibility of reconstructing the state that characterizes the\nsystem from observing its inputs and outputs. Despite the huge effort made to\nstudy this property and to introduce analytical criteria able to check whether\na dynamic system satisfies this property or not, there is no general analytical\ncriterion to automatically check the state observability when the dynamics are\nalso driven by unknown inputs. Here, we introduce the general analytical\nsolution of this fundamental problem, often called the unknown input\nobservability problem. This paper provides the general analytical solution of\nthis problem, namely, it provides the systematic procedure, based on automatic\ncomputation (differentiation and matrix rank determination), that allows us to\nautomatically check the state observability even in the presence of unknown\ninputs (Algorithm 6.1). A first solution of this problem was presented in the\nsecond part of the book: \"Observability: A New Theory Based on the Group of\nInvariance\" [45]. The solution presented by this paper completes the previous\nsolution in [45]. In particular, the new solution exhaustively accounts for the\nsystems that do not belong to the category of the systems that are \"canonic\nwith respect to their unknown inputs\". The analytical derivations largely\nexploit several new concepts and analytical results introduced in [45].\nFinally, as a simple consequence of the results here obtained, we also provide\nthe answer to the problem of unknown input reconstruction which is intimately\nrelated to the problem of state observability. We illustrate the implementation\nof the new algorithm by studying the observability properties of a nonlinear\nsystem in the framework of visual-inertial sensor fusion, whose dynamics are\ndriven by two unknown inputs and one known input.\n","authors":["Agostino Martinelli"],"pdf_url":"https://arxiv.org/pdf/2201.07610v5.pdf","comment":"This paper was published by the journal of Information Fusion"},{"id":"http://arxiv.org/abs/2408.05743v4","updated":"2024-09-10T09:07:35Z","published":"2024-08-11T10:42:22Z","title":"Neural Architecture Search based Global-local Vision Mamba for Palm-Vein\n  Recognition","summary":"  Due to the advantages such as high security, high privacy, and liveness\nrecognition, vein recognition has been received more and more attention in past\nyears. Recently, deep learning models, e.g., Mamba has shown robust feature\nrepresentation with linear computational complexity and successfully applied\nfor visual tasks. However, vision Manba can capture long-distance feature\ndependencies but unfortunately deteriorate local feature details. Besides,\nmanually designing a Mamba architecture based on human priori knowledge is very\ntime-consuming and error-prone. In this paper, first, we propose a hybrid\nnetwork structure named Global-local Vision Mamba (GLVM), to learn the local\ncorrelations in images explicitly and global dependencies among tokens for vein\nfeature representation. Secondly, we design a Multi-head Mamba to learn the\ndependencies along different directions, so as to improve the feature\nrepresentation ability of vision Mamba. Thirdly, to learn the complementary\nfeatures, we propose a ConvMamba block consisting of three branches, named\nMulti-head Mamba branch (MHMamba), Feature Iteration Unit branch (FIU), and\nConvolutional Neural Network (CNN) branch, where the Feature Iteration Unit\nbranch aims to fuse convolutional local features with Mamba-based global\nrepresentations. Finally, a Globallocal Alternate Neural Architecture Search\n(GLNAS) method is proposed to search the optimal architecture of GLVM\nalternately with the evolutionary algorithm, thereby improving the recognition\nperformance for vein recognition tasks. We conduct rigorous experiments on\nthree public palm-vein databases to estimate the performance. The experimental\nresults demonstrate that the proposed method outperforms the representative\napproaches and achieves state-of-the-art recognition accuracy.\n","authors":["Huafeng Qin","Yuming Fu","Jing Chen","Mounim A. El-Yacoubi","Xinbo Gao","Feng Xi"],"pdf_url":"https://arxiv.org/pdf/2408.05743v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08116v4","updated":"2024-09-10T09:07:23Z","published":"2023-10-12T08:17:57Z","title":"Multimodal Active Measurement for Human Mesh Recovery in Close Proximity","summary":"  For physical human-robot interactions (pHRI), a robot needs to estimate the\naccurate body pose of a target person. However, in these pHRI scenarios, the\nrobot cannot fully observe the target person's body with equipped cameras\nbecause the target person must be close to the robot for physical interaction.\nThis close distance leads to severe truncation and occlusions and thus results\nin poor accuracy of human pose estimation. For better accuracy in this\nchallenging environment, we propose an active measurement and sensor fusion\nframework of the equipped cameras with touch and ranging sensors such as 2D\nLiDAR. Touch and ranging sensor measurements are sparse but reliable and\ninformative cues for localizing human body parts. In our active measurement\nprocess, camera viewpoints and sensor placements are dynamically optimized to\nmeasure body parts with higher estimation uncertainty, which is closely related\nto truncation or occlusion. In our sensor fusion process, assuming that the\nmeasurements of touch and ranging sensors are more reliable than the\ncamera-based estimations, we fuse the sensor measurements to the camera-based\nestimated pose by aligning the estimated pose towards the measured points. Our\nproposed method outperformed previous methods on the standard occlusion\nbenchmark with simulated active measurement. Furthermore, our method reliably\nestimated human poses using a real robot, even with practical constraints such\nas occlusion by blankets.\n","authors":["Takahiro Maeda","Keisuke Takeshita","Norimichi Ukita","Kazuhito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2310.08116v4.pdf","comment":"Accepted at Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2402.18307v2","updated":"2024-09-10T08:50:11Z","published":"2024-02-28T13:07:16Z","title":"Multi-Scale Denoising in the Feature Space for Low-Light Instance\n  Segmentation","summary":"  Instance segmentation for low-light imagery remains largely unexplored due to\nthe challenges imposed by such conditions, for example shot noise due to low\nphoton count, color distortions and reduced contrast. In this paper, we propose\nan end-to-end solution to address this challenging task. Our proposed method\nimplements weighted non-local blocks (wNLB) in the feature extractor. This\nintegration enables an inherent denoising process at the feature level. As a\nresult, our method eliminates the need for aligned ground truth images during\ntraining, thus supporting training on real-world low-light datasets. We\nintroduce additional learnable weights at each layer in order to enhance the\nnetwork's adaptability to real-world noise characteristics, which affect\ndifferent feature scales in different ways. Experimental results on several\nobject detectors show that the proposed method outperforms the pretrained\nnetworks with an Average Precision (AP) improvement of at least +7.6, with the\nintroduction of wNLB further enhancing AP by upto +1.3.\n","authors":["Joanne Lin","Nantheera Anantrasirichai","David Bull"],"pdf_url":"https://arxiv.org/pdf/2402.18307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06334v1","updated":"2024-09-10T08:47:03Z","published":"2024-09-10T08:47:03Z","title":"Multi-Weather Image Restoration via Histogram-Based Transformer Feature\n  Enhancement","summary":"  Currently, the mainstream restoration tasks under adverse weather conditions\nhave predominantly focused on single-weather scenarios. However, in reality,\nmultiple weather conditions always coexist and their degree of mixing is\nusually unknown. Under such complex and diverse weather conditions,\nsingle-weather restoration models struggle to meet practical demands. This is\nparticularly critical in fields such as autonomous driving, where there is an\nurgent need for a model capable of effectively handling mixed weather\nconditions and enhancing image quality in an automated manner. In this paper,\nwe propose a Task Sequence Generator module that, in conjunction with the Task\nIntra-patch Block, effectively extracts task-specific features embedded in\ndegraded images. The Task Intra-patch Block introduces an external learnable\nsequence that aids the network in capturing task-specific information.\nAdditionally, we employ a histogram-based transformer module as the backbone of\nour network, enabling the capture of both global and local dynamic range\nfeatures. Our proposed model achieves state-of-the-art performance on public\ndatasets.\n","authors":["Yang Wen","Anyu Lai","Bo Qian","Hao Wang","Wuzhen Shi","Wenming Cao"],"pdf_url":"https://arxiv.org/pdf/2409.06334v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.03249"},{"id":"http://arxiv.org/abs/2409.06324v1","updated":"2024-09-10T08:27:44Z","published":"2024-09-10T08:27:44Z","title":"SDF-Net: A Hybrid Detection Network for Mediastinal Lymph Node Detection\n  on Contrast CT Images","summary":"  Accurate lymph node detection and quantification are crucial for cancer\ndiagnosis and staging on contrast-enhanced CT images, as they impact treatment\nplanning and prognosis. However, detecting lymph nodes in the mediastinal area\nposes challenges due to their low contrast, irregular shapes and dispersed\ndistribution. In this paper, we propose a Swin-Det Fusion Network (SDF-Net) to\neffectively detect lymph nodes. SDF-Net integrates features from both\nsegmentation and detection to enhance the detection capability of lymph nodes\nwith various shapes and sizes. Specifically, an auto-fusion module is designed\nto merge the feature maps of segmentation and detection networks at different\nlevels. To facilitate effective learning without mask annotations, we introduce\na shape-adaptive Gaussian kernel to represent lymph node in the training stage\nand provide more anatomical information for effective learning. Comparative\nresults demonstrate promising performance in addressing the complex lymph node\ndetection problem.\n","authors":["Jiuli Xiong","Lanzhuju Mei","Jiameng Liu","Dinggang Shen","Zhong Xue","Xiaohuan Cao"],"pdf_url":"https://arxiv.org/pdf/2409.06324v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.06322v1","updated":"2024-09-10T08:27:19Z","published":"2024-09-10T08:27:19Z","title":"G3PT: Unleash the power of Autoregressive Modeling in 3D Generation via\n  Cross-scale Querying Transformer","summary":"  Autoregressive transformers have revolutionized generative models in language\nprocessing and shown substantial promise in image and video generation.\nHowever, these models face significant challenges when extended to 3D\ngeneration tasks due to their reliance on next-token prediction to learn token\nsequences, which is incompatible with the unordered nature of 3D data. Instead\nof imposing an artificial order on 3D data, in this paper, we introduce G3PT, a\nscalable coarse-to-fine 3D generative model utilizing a cross-scale querying\ntransformer. The key is to map point-based 3D data into discrete tokens with\ndifferent levels of detail, naturally establishing a sequential relationship\nbetween different levels suitable for autoregressive modeling. Additionally,\nthe cross-scale querying transformer connects tokens globally across different\nlevels of detail without requiring an ordered sequence. Benefiting from this\napproach, G3PT features a versatile 3D generation pipeline that effortlessly\nsupports diverse conditional structures, enabling the generation of 3D shapes\nfrom various types of conditions. Extensive experiments demonstrate that G3PT\nachieves superior generation quality and generalization ability compared to\nprevious 3D generation methods. Most importantly, for the first time in 3D\ngeneration, scaling up G3PT reveals distinct power-law scaling behaviors.\n","authors":["Jinzhi Zhang","Feng Xiong","Mu Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06322v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.02453v2","updated":"2024-09-10T08:20:36Z","published":"2024-09-04T05:19:57Z","title":"FrameCorr: Adaptive, Autoencoder-based Neural Compression for Video\n  Reconstruction in Resource and Timing Constrained Network Settings","summary":"  Despite the growing adoption of video processing via Internet of Things (IoT)\ndevices due to their cost-effectiveness, transmitting captured data to nearby\nservers poses challenges due to varying timing constraints and scarcity of\nnetwork bandwidth. Existing video compression methods face difficulties in\nrecovering compressed data when incomplete data is provided. Here, we introduce\nFrameCorr, a deep-learning based solution that utilizes previously received\ndata to predict the missing segments of a frame, enabling the reconstruction of\na frame from partially received data.\n","authors":["John Li","Shehab Sarar Ahmed","Deepak Nair"],"pdf_url":"https://arxiv.org/pdf/2409.02453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06311v1","updated":"2024-09-10T08:11:14Z","published":"2024-09-10T08:11:14Z","title":"Seam Carving as Feature Pooling in CNN","summary":"  This work investigates the potential of seam carving as a feature pooling\ntechnique within Convolutional Neural Networks (CNNs) for image classification\ntasks. We propose replacing the traditional max pooling layer with a seam\ncarving operation. Our experiments on the Caltech-UCSD Birds 200-2011 dataset\ndemonstrate that the seam carving-based CNN achieves better performance\ncompared to the model utilizing max pooling, based on metrics such as accuracy,\nprecision, recall, and F1-score. We further analyze the behavior of both\napproaches through feature map visualizations, suggesting that seam carving\nmight preserve more structural information during the pooling process.\nAdditionally, we discuss the limitations of our approach and propose potential\nfuture directions for research.\n","authors":["Mohammad Imrul Jubair"],"pdf_url":"https://arxiv.org/pdf/2409.06311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06309v1","updated":"2024-09-10T08:08:50Z","published":"2024-09-10T08:08:50Z","title":"PPMamba: A Pyramid Pooling Local Auxiliary SSM-Based Model for Remote\n  Sensing Image Semantic Segmentation","summary":"  Semantic segmentation is a vital task in the field of remote sensing (RS).\nHowever, conventional convolutional neural network (CNN) and transformer-based\nmodels face limitations in capturing long-range dependencies or are often\ncomputationally intensive. Recently, an advanced state space model (SSM),\nnamely Mamba, was introduced, offering linear computational complexity while\neffectively establishing long-distance dependencies. Despite their advantages,\nMamba-based methods encounter challenges in preserving local semantic\ninformation. To cope with these challenges, this paper proposes a novel network\ncalled Pyramid Pooling Mamba (PPMamba), which integrates CNN and Mamba for RS\nsemantic segmentation tasks. The core structure of PPMamba, the Pyramid\nPooling-State Space Model (PP-SSM) block, combines a local auxiliary mechanism\nwith an omnidirectional state space model (OSS) that selectively scans feature\nmaps from eight directions, capturing comprehensive feature information.\nAdditionally, the auxiliary mechanism includes pyramid-shaped convolutional\nbranches designed to extract features at multiple scales. Extensive experiments\non two widely-used datasets, ISPRS Vaihingen and LoveDA Urban, demonstrate that\nPPMamba achieves competitive performance compared to state-of-the-art models.\n","authors":["Yin Hu","Xianping Ma","Jialu Sui","Man-On Pun"],"pdf_url":"https://arxiv.org/pdf/2409.06309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06305v1","updated":"2024-09-10T08:04:11Z","published":"2024-09-10T08:04:11Z","title":"High-Performance Few-Shot Segmentation with Foundation Models: An\n  Empirical Study","summary":"  Existing few-shot segmentation (FSS) methods mainly focus on designing novel\nsupport-query matching and self-matching mechanisms to exploit implicit\nknowledge in pre-trained backbones. However, the performance of these methods\nis often constrained by models pre-trained on classification tasks. The\nexploration of what types of pre-trained models can provide more beneficial\nimplicit knowledge for FSS remains limited. In this paper, inspired by the\nrepresentation consistency of foundational computer vision models, we develop a\nFSS framework based on foundation models. To be specific, we propose a simple\napproach to extract implicit knowledge from foundation models to construct\ncoarse correspondence and introduce a lightweight decoder to refine coarse\ncorrespondence for fine-grained segmentation. We systematically summarize the\nperformance of various foundation models on FSS and discover that the implicit\nknowledge within some of these models is more beneficial for FSS than models\npre-trained on classification tasks. Extensive experiments on two widely used\ndatasets demonstrate the effectiveness of our approach in leveraging the\nimplicit knowledge of foundation models. Notably, the combination of DINOv2 and\nDFN exceeds previous state-of-the-art methods by 17.5% on COCO-20i. Code is\navailable at https://github.com/DUT-CSJ/FoundationFSS.\n","authors":["Shijie Chang","Lihe Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2409.06305v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2409.06300v1","updated":"2024-09-10T07:53:32Z","published":"2024-09-10T07:53:32Z","title":"An Attribute-Enriched Dataset and Auto-Annotated Pipeline for Open\n  Detection","summary":"  Detecting objects of interest through language often presents challenges,\nparticularly with objects that are uncommon or complex to describe, due to\nperceptual discrepancies between automated models and human annotators. These\nchallenges highlight the need for comprehensive datasets that go beyond\nstandard object labels by incorporating detailed attribute descriptions. To\naddress this need, we introduce the Objects365-Attr dataset, an extension of\nthe existing Objects365 dataset, distinguished by its attribute annotations.\nThis dataset reduces inconsistencies in object detection by integrating a broad\nspectrum of attributes, including color, material, state, texture and tone. It\ncontains an extensive collection of 5.6M object-level attribute descriptions,\nmeticulously annotated across 1.4M bounding boxes. Additionally, to validate\nthe dataset's effectiveness, we conduct a rigorous evaluation of YOLO-World at\ndifferent scales, measuring their detection performance and demonstrating the\ndataset's contribution to advancing object detection.\n","authors":["Pengfei Qi","Yifei Zhang","Wenqiang Li","Youwen Hu","Kunlong Bai"],"pdf_url":"https://arxiv.org/pdf/2409.06300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06299v1","updated":"2024-09-10T07:53:10Z","published":"2024-09-10T07:53:10Z","title":"Enhancing Long Video Understanding via Hierarchical Event-Based Memory","summary":"  Recently, integrating visual foundation models into large language models\n(LLMs) to form video understanding systems has attracted widespread attention.\nMost of the existing models compress diverse semantic information within the\nwhole video and feed it into LLMs for content comprehension. While this method\nexcels in short video understanding, it may result in a blend of multiple event\ninformation in long videos due to coarse compression, which causes information\nredundancy. Consequently, the semantics of key events might be obscured within\nthe vast information that hinders the model's understanding capabilities. To\naddress this issue, we propose a Hierarchical Event-based Memory-enhanced LLM\n(HEM-LLM) for better understanding of long videos. Firstly, we design a novel\nadaptive sequence segmentation scheme to divide multiple events within long\nvideos. In this way, we can perform individual memory modeling for each event\nto establish intra-event contextual connections, thereby reducing information\nredundancy. Secondly, while modeling current event, we compress and inject the\ninformation of the previous event to enhance the long-term inter-event\ndependencies in videos. Finally, we perform extensive experiments on various\nvideo understanding tasks and the results show that our model achieves\nstate-of-the-art performances.\n","authors":["Dingxin Cheng","Mingda Li","Jingyu Liu","Yongxin Guo","Bin Jiang","Qingbin Liu","Xi Chen","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.06299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06290v1","updated":"2024-09-10T07:42:47Z","published":"2024-09-10T07:42:47Z","title":"EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for\n  Image Classification","summary":"  Data augmentation (DA) has been widely used to improve the generalization of\ndeep neural networks. While existing DA methods have proven effective, they\noften rely on augmentation operations with random magnitudes to each sample.\nHowever, this approach can inadvertently introduce noise, induce distribution\nshifts, and increase the risk of overfitting. In this paper, we propose\nEntAugment, a tuning-free and adaptive DA framework. Unlike previous work,\nEntAugment dynamically assesses and adjusts the augmentation magnitudes for\neach sample during training, leveraging insights into both the inherent\ncomplexities of training samples and the evolving status of deep models.\nSpecifically, in EntAugment, the magnitudes are determined by the information\nentropy derived from the probability distribution obtained by applying the\nsoftmax function to the model's output. In addition, to further enhance the\nefficacy of EntAugment, we introduce a novel entropy regularization term,\nEntLoss, which complements the EntAugment approach. Theoretical analysis\nfurther demonstrates that EntLoss, compared to traditional cross-entropy loss,\nachieves closer alignment between the model distributions and underlying\ndataset distributions. Moreover, EntAugment and EntLoss can be utilized\nseparately or jointly. We conduct extensive experiments across multiple image\nclassification tasks and network architectures with thorough comparisons of\nexisting DA methods. Importantly, the proposed methods outperform others\nwithout introducing any auxiliary models or noticeable extra computational\ncosts, highlighting both effectiveness and efficiency. Code is available at\nhttps://github.com/Jackbrocp/EntAugment.\n","authors":["Suorong Yang","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.06290v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2409.00487v2","updated":"2024-09-10T07:41:08Z","published":"2024-08-31T15:45:56Z","title":"TrackSSM: A General Motion Predictor by State-Space Model","summary":"  Temporal motion modeling has always been a key component in multiple object\ntracking (MOT) which can ensure smooth trajectory movement and provide accurate\npositional information to enhance association precision. However, current\nmotion models struggle to be both efficient and effective across different\napplication scenarios. To this end, we propose TrackSSM inspired by the\nrecently popular state space models (SSM), a unified encoder-decoder motion\nframework that uses data-dependent state space model to perform temporal motion\nof trajectories. Specifically, we propose Flow-SSM, a module that utilizes the\nposition and motion information from historical trajectories to guide the\ntemporal state transition of object bounding boxes. Based on Flow-SSM, we\ndesign a flow decoder. It is composed of a cascaded motion decoding module\nemploying Flow-SSM, which can use the encoded flow information to complete the\ntemporal position prediction of trajectories. Additionally, we propose a\nStep-by-Step Linear (S$^2$L) training strategy. By performing linear\ninterpolation between the positions of the object in the previous frame and the\ncurrent frame, we construct the pseudo labels of step-by-step linear training,\nensuring that the trajectory flow information can better guide the object\nbounding box in completing temporal transitions. TrackSSM utilizes a simple\nMamba-Block to build a motion encoder for historical trajectories, forming a\ntemporal motion model with an encoder-decoder structure in conjunction with the\nflow decoder. TrackSSM is applicable to various tracking scenarios and achieves\nexcellent tracking performance across multiple benchmarks, further extending\nthe potential of SSM-like temporal motion models in multi-object tracking\ntasks. Code and models are publicly available at\n\\url{https://github.com/Xavier-Lin/TrackSSM}.\n","authors":["Bin Hu","Run Luo","Zelin Liu","Cheng Wang","Wenyu Liu"],"pdf_url":"https://arxiv.org/pdf/2409.00487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04494v2","updated":"2024-09-10T07:40:06Z","published":"2024-09-06T14:21:23Z","title":"Diff-INR: Generative Regularization for Electrical Impedance Tomography","summary":"  Electrical Impedance Tomography (EIT) is a non-invasive imaging technique\nthat reconstructs conductivity distributions within a body from boundary\nmeasurements. However, EIT reconstruction is hindered by its ill-posed\nnonlinear inverse problem, which complicates accurate results. To tackle this,\nwe propose Diff-INR, a novel method that combines generative regularization\nwith Implicit Neural Representations (INR) through a diffusion model. Diff-INR\nintroduces geometric priors to guide the reconstruction, effectively addressing\nthe shortcomings of traditional regularization methods. By integrating a\npre-trained diffusion regularizer with INR, our approach achieves\nstate-of-the-art reconstruction accuracy in both simulation and experimental\ndata. The method demonstrates robust performance across various mesh densities\nand hyperparameter settings, highlighting its flexibility and efficiency. This\nadvancement represents a significant improvement in managing the ill-posed\nnature of EIT. Furthermore, the method's principles are applicable to other\nimaging modalities facing similar challenges with ill-posed inverse problems.\n","authors":["Bowen Tong","Junwu Wang","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2409.04494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06285v1","updated":"2024-09-10T07:37:58Z","published":"2024-09-10T07:37:58Z","title":"Context Enhancement with Reconstruction as Sequence for Unified\n  Unsupervised Anomaly Detection","summary":"  Unsupervised anomaly detection (AD) aims to train robust detection models\nusing only normal samples, while can generalize well to unseen anomalies.\nRecent research focuses on a unified unsupervised AD setting in which only one\nmodel is trained for all classes, i.e., n-class-one-model paradigm.\nFeature-reconstruction-based methods achieve state-of-the-art performance in\nthis scenario. However, existing methods often suffer from a lack of sufficient\ncontextual awareness, thereby compromising the quality of the reconstruction.\nTo address this issue, we introduce a novel Reconstruction as Sequence (RAS)\nmethod, which enhances the contextual correspondence during feature\nreconstruction from a sequence modeling perspective. In particular, based on\nthe transformer technique, we integrate a specialized RASFormer block into RAS.\nThis block enables the capture of spatial relationships among different image\nregions and enhances sequential dependencies throughout the reconstruction\nprocess. By incorporating the RASFormer block, our RAS method achieves superior\ncontextual awareness capabilities, leading to remarkable performance.\nExperimental results show that our RAS significantly outperforms competing\nmethods, well demonstrating the effectiveness and superiority of our method.\nOur code is available at https://github.com/Nothingtolose9979/RAS.\n","authors":["Hui-Yue Yang","Hui Chen","Lihao Liu","Zijia Lin","Kai Chen","Liejun Wang","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2409.06285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06270v1","updated":"2024-09-10T07:18:57Z","published":"2024-09-10T07:18:57Z","title":"Towards Robust Uncertainty-Aware Incomplete Multi-View Classification","summary":"  Handling incomplete data in multi-view classification is challenging,\nespecially when traditional imputation methods introduce biases that compromise\nuncertainty estimation. Existing Evidential Deep Learning (EDL) based\napproaches attempt to address these issues, but they often struggle with\nconflicting evidence due to the limitations of the Dempster-Shafer combination\nrule, leading to unreliable decisions. To address these challenges, we propose\nthe Alternating Progressive Learning Network (APLN), specifically designed to\nenhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates\nbias from corrupted observed data by first applying coarse imputation, followed\nby mapping the data to a latent space. In this latent space, we progressively\nlearn an evidence distribution aligned with the target domain, incorporating\nuncertainty considerations through EDL. Additionally, we introduce a\nconflict-aware Dempster-Shafer combination rule (DSCR) to better handle\nconflicting evidence. By sampling from the learned distribution, we optimize\nthe latent representations of missing views, reducing bias and enhancing\ndecision-making robustness. Extensive experiments demonstrate that APLN,\ncombined with DSCR, significantly outperforms traditional methods, particularly\nin environments characterized by high uncertainty and conflicting evidence,\nestablishing it as a promising solution for incomplete multi-view\nclassification.\n","authors":["Mulin Chen","Haojian Huang","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2409.06270v1.pdf","comment":"Ongoing work: 9 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.06267v1","updated":"2024-09-10T07:12:18Z","published":"2024-09-10T07:12:18Z","title":"Mahalanobis k-NN: A Statistical Lens for Robust Point-Cloud\n  Registrations","summary":"  In this paper, we discuss Mahalanobis k-NN: a statistical lens designed to\naddress the challenges of feature matching in learning-based point cloud\nregistration when confronted with an arbitrary density of point clouds, either\nin the source or target point cloud. We tackle this by adopting Mahalanobis\nk-NN's inherent property to capture the distribution of the local neighborhood\nand surficial geometry. Our method can be seamlessly integrated into any\nlocal-graph-based point cloud analysis method. In this paper, we focus on two\ndistinct methodologies: Deep Closest Point (DCP) and Deep Universal Manifold\nEmbedding (DeepUME). Our extensive benchmarking on the ModelNet40 and Faust\ndatasets highlights the efficacy of the proposed method in point cloud\nregistration tasks. Moreover, we establish for the first time that the features\nacquired through point cloud registration inherently can possess discriminative\ncapabilities. This is evident by a substantial improvement of about 20\\% in the\naverage accuracy observed in the point cloud few-shot classification task\nbenchmarked on ModelNet40 and ScanObjectNN. The code is publicly available at\nhttps://github.com/TejasAnvekar/Mahalanobis-k-NN\n","authors":["Tejas Anvekar","Shivanand Venkanna Sheshappanavar"],"pdf_url":"https://arxiv.org/pdf/2409.06267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09498v2","updated":"2024-09-10T07:03:32Z","published":"2024-06-12T18:30:03Z","title":"OT-VP: Optimal Transport-guided Visual Prompting for Test-Time\n  Adaptation","summary":"  Vision Transformers (ViTs) have demonstrated remarkable capabilities in\nlearning representations, but their performance is compromised when applied to\nunseen domains. Previous methods either engage in prompt learning during the\ntraining phase or modify model parameters at test time through entropy\nminimization. The former often overlooks unlabeled target data, while the\nlatter doesn't fully address domain shifts. In this work, our approach, Optimal\nTransport-guided Test-Time Visual Prompting (OT-VP), handles these problems by\nleveraging prompt learning at test time to align the target and source domains\nwithout accessing the training process or altering pre-trained model\nparameters. This method involves learning a universal visual prompt for the\ntarget domain by optimizing the Optimal Transport distance.OT-VP, with only\nfour learned prompt tokens, exceeds state-of-the-art performance across three\nstylistic datasets-PACS, VLCS, OfficeHome, and one corrupted dataset\nImageNet-C. Additionally, OT-VP operates efficiently, both in terms of memory\nand computation, and is adaptable for extension to online settings.\n","authors":["Yunbei Zhang","Akshay Mehra","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2407.09498v2.pdf","comment":"WACV2025"},{"id":"http://arxiv.org/abs/2409.06259v1","updated":"2024-09-10T07:02:01Z","published":"2024-09-10T07:02:01Z","title":"ALSS-YOLO: An Adaptive Lightweight Channel Split and Shuffling Network\n  for TIR Wildlife Detection in UAV Imagery","summary":"  Unmanned aerial vehicles (UAVs) equipped with thermal infrared (TIR) cameras\nplay a crucial role in combating nocturnal wildlife poaching. However, TIR\nimages often face challenges such as jitter, and wildlife overlap,\nnecessitating UAVs to possess the capability to identify blurred and\noverlapping small targets. Current traditional lightweight networks deployed on\nUAVs struggle to extract features from blurry small targets. To address this\nissue, we developed ALSS-YOLO, an efficient and lightweight detector optimized\nfor TIR aerial images. Firstly, we propose a novel Adaptive Lightweight Channel\nSplit and Shuffling (ALSS) module. This module employs an adaptive channel\nsplit strategy to optimize feature extraction and integrates a channel\nshuffling mechanism to enhance information exchange between channels. This\nimproves the extraction of blurry features, crucial for handling jitter-induced\nblur and overlapping targets. Secondly, we developed a Lightweight Coordinate\nAttention (LCA) module that employs adaptive pooling and grouped convolution to\nintegrate feature information across dimensions. This module ensures\nlightweight operation while maintaining high detection precision and robustness\nagainst jitter and target overlap. Additionally, we developed a single-channel\nfocus module to aggregate the width and height information of each channel into\nfour-dimensional channel fusion, which improves the feature representation\nefficiency of infrared images. Finally, we modify the localization loss\nfunction to emphasize the loss value associated with small objects to improve\nlocalization accuracy. Extensive experiments on the BIRDSAI and ISOD TIR UAV\nwildlife datasets show that ALSS-YOLO achieves state-of-the-art performance,\nOur code is openly available at\nhttps://github.com/helloworlder8/computer_vision.\n","authors":["Ang He","Xiaobo Li","Ximei Wu","Chengyue Su","Jing Chen","Sheng Xu","Xiaobin Guo"],"pdf_url":"https://arxiv.org/pdf/2409.06259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12877v6","updated":"2024-09-10T06:53:24Z","published":"2023-10-19T16:32:18Z","title":"Perceptual Assessment and Optimization of HDR Image Rendering","summary":"  High dynamic range (HDR) rendering has the ability to faithfully reproduce\nthe wide luminance ranges in natural scenes, but how to accurately assess the\nrendering quality is relatively underexplored. Existing quality models are\nmostly designed for low dynamic range (LDR) images, and do not align well with\nhuman perception of HDR image quality. To fill this gap, we propose a family of\nHDR quality metrics, in which the key step is employing a simple inverse\ndisplay model to decompose an HDR image into a stack of LDR images with varying\nexposures. Subsequently, these decomposed images are assessed through\nwell-established LDR quality metrics. Our HDR quality models present three\ndistinct benefits. First, they directly inherit the recent advancements of LDR\nquality metrics. Second, they do not rely on human perceptual data of HDR image\nquality for re-calibration. Third, they facilitate the alignment and\nprioritization of specific luminance ranges for more accurate and detailed\nquality assessment. Experimental results show that our HDR quality metrics\nconsistently outperform existing models in terms of quality assessment on four\nHDR image quality datasets and perceptual optimization of HDR novel view\nsynthesis.\n","authors":["Peibei Cao","Rafal K. Mantiuk","Kede Ma"],"pdf_url":"https://arxiv.org/pdf/2310.12877v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04760v2","updated":"2024-09-10T06:40:45Z","published":"2024-09-07T08:20:02Z","title":"Training-Free Point Cloud Recognition Based on Geometric and Semantic\n  Information Fusion","summary":"  The trend of employing training-free methods for point cloud recognition is\nbecoming increasingly popular due to its significant reduction in computational\nresources and time costs. However, existing approaches are limited as they\ntypically extract either geometric or semantic features. To address this\nlimitation, we are the first to propose a novel training-free method that\nintegrates both geometric and semantic features. For the geometric branch, we\nadopt a non-parametric strategy to extract geometric features. In the semantic\nbranch, we leverage a model aligned with text features to obtain semantic\nfeatures. Additionally, we introduce the GFE module to complement the geometric\ninformation of point clouds and the MFF module to improve performance in\nfew-shot settings. Experimental results demonstrate that our method outperforms\nexisting state-of-the-art training-free approaches on mainstream benchmark\ndatasets, including ModelNet and ScanObiectNN.\n","authors":["Yan Chen","Di Huang","Zhichao Liao","Xi Cheng","Xinghui Li","Lone Zeng"],"pdf_url":"https://arxiv.org/pdf/2409.04760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v3","updated":"2024-09-10T06:36:25Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking Neural Networks (SNNs) have emerged as a promising alternative to\nconventional Artificial Neural Networks (ANNs), demonstrating comparable\nperformance in both visual and linguistic tasks while offering the advantage of\nimproved energy efficiency. Despite these advancements, the integration of\nlinguistic and visual features into a unified representation through spike\ntrains poses a significant challenge, and the application of SNNs to multimodal\nscenarios remains largely unexplored. This paper presents SpikeCLIP, a novel\nframework designed to bridge the modality gap in spike-based computation. Our\napproach employs a two-step recipe: an ``alignment pre-training'' to align\nfeatures across modalities, followed by a ``dual-loss fine-tuning'' to refine\nthe model's performance. Extensive experiments reveal that SNNs achieve results\non par with ANNs while substantially reducing energy consumption across various\ndatasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP\nmaintains robust image classification capabilities, even when dealing with\nclasses that fall outside predefined categories. This study marks a significant\nadvancement in the development of energy-efficient and biologically plausible\nmultimodal learning systems.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Yufei Gu","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16340v3","updated":"2024-09-10T06:35:59Z","published":"2024-08-29T08:23:57Z","title":"Learned Image Transmission with Hierarchical Variational Autoencoder","summary":"  In this paper, we introduce an innovative hierarchical joint source-channel\ncoding (HJSCC) framework for image transmission, utilizing a hierarchical\nvariational autoencoder (VAE). Our approach leverages a combination of\nbottom-up and top-down paths at the transmitter to autoregressively generate\nmultiple hierarchical representations of the original image. These\nrepresentations are then directly mapped to channel symbols for transmission by\nthe JSCC encoder. We extend this framework to scenarios with a feedback link,\nmodeling transmission over a noisy channel as a probabilistic sampling process\nand deriving a novel generative formulation for JSCC with feedback. Compared\nwith existing approaches, our proposed HJSCC provides enhanced adaptability by\ndynamically adjusting transmission bandwidth, encoding these representations\ninto varying amounts of channel symbols. Extensive experiments on images of\nvarying resolutions demonstrate that our proposed model outperforms existing\nbaselines in rate-distortion performance and maintains robustness against\nchannel noise. The source code will be made available upon acceptance.\n","authors":["Guangyi Zhang","Hanlei Li","Yunlong Cai","Qiyu Hu","Guanding Yu","Runmin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.16340v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06240v1","updated":"2024-09-10T06:17:07Z","published":"2024-09-10T06:17:07Z","title":"Test-Time Certifiable Self-Supervision to Bridge the Sim2Real Gap in\n  Event-Based Satellite Pose Estimation","summary":"  Deep learning plays a critical role in vision-based satellite pose\nestimation. However, the scarcity of real data from the space environment means\nthat deep models need to be trained using synthetic data, which raises the\nSim2Real domain gap problem. A major cause of the Sim2Real gap are novel\nlighting conditions encountered during test time. Event sensors have been shown\nto provide some robustness against lighting variations in vision-based pose\nestimation. However, challenging lighting conditions due to strong directional\nlight can still cause undesirable effects in the output of commercial\noff-the-shelf event sensors, such as noisy/spurious events and inhomogeneous\nevent densities on the object. Such effects are non-trivial to simulate in\nsoftware, thus leading to Sim2Real gap in the event domain. To close the\nSim2Real gap in event-based satellite pose estimation, the paper proposes a\ntest-time self-supervision scheme with a certifier module. Self-supervision is\nenabled by an optimisation routine that aligns a dense point cloud of the\npredicted satellite pose with the event data to attempt to rectify the\ninaccurately estimated pose. The certifier attempts to verify the corrected\npose, and only certified test-time inputs are backpropagated via implicit\ndifferentiation to refine the predicted landmarks, thus improving the pose\nestimates and closing the Sim2Real gap. Results show that the our method\noutperforms established test-time adaptation schemes.\n","authors":["Mohsi Jawaid","Rajat Talak","Yasir Latif","Luca Carlone","Tat-Jun Chin"],"pdf_url":"https://arxiv.org/pdf/2409.06240v1.pdf","comment":"This work has been accepted for publication at IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2024). Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible"},{"id":"http://arxiv.org/abs/2409.05494v2","updated":"2024-09-10T06:15:55Z","published":"2024-09-09T10:47:39Z","title":"An Atmospheric Correction Integrated LULC Segmentation Model for\n  High-Resolution Satellite Imagery","summary":"  The integration of fine-scale multispectral imagery with deep learning models\nhas revolutionized land use and land cover (LULC) classification. However, the\natmospheric effects present in Top-of-Atmosphere sensor measured Digital Number\nvalues must be corrected to retrieve accurate Bottom-of-Atmosphere surface\nreflectance for reliable analysis. This study employs look-up-table-based\nradiative transfer simulations to estimate the atmospheric path reflectance and\ntransmittance for atmospherically correcting high-resolution CARTOSAT-3\nMultispectral (MX) imagery for several Indian cities. The corrected surface\nreflectance data were subsequently used in supervised and semi-supervised\nsegmentation models, demonstrating stability in multi-class (buildings, roads,\ntrees and water bodies) LULC segmentation accuracy, particularly in scenarios\nwith sparsely labelled data.\n","authors":["Soham Mukherjee","Yash Dixit","Naman Srivastava","Joel D Joy","Rohan Olikara","Koesha Sinha","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2409.05494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06235v1","updated":"2024-09-10T06:07:20Z","published":"2024-09-10T06:07:20Z","title":"Recurrent Neural Networks for Still Images","summary":"  In this paper, we explore the application of Recurrent Neural Network (RNN)\nfor still images. Typically, Convolutional Neural Networks (CNNs) are the\nprevalent method applied for this type of data, and more recently, transformers\nhave gained popularity, although they often require large models. Unlike these\nmethods, RNNs are generally associated with processing sequences over time\nrather than single images. We argue that RNNs can effectively handle still\nimages by interpreting the pixels as a sequence. This approach could be\nparticularly advantageous for compact models designed for embedded systems,\nwhere resources are limited. Additionally, we introduce a novel RNN design\ntailored for two-dimensional inputs, such as images, and a custom version of\nBiDirectional RNN (BiRNN) that is more memory-efficient than traditional\nimplementations. In our research, we have tested these layers in Convolutional\nRecurrent Neural Networks (CRNNs), predominantly composed of Conv2D layers,\nwith RNN layers at or close to the end. Experiments on the COCO and CIFAR100\ndatasets show better results, particularly for small networks.\n","authors":[" Dmitri"," Lvov","Yair Smadar","Ran Bezen"],"pdf_url":"https://arxiv.org/pdf/2409.06235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06231v1","updated":"2024-09-10T05:57:58Z","published":"2024-09-10T05:57:58Z","title":"A Latent Implicit 3D Shape Model for Multiple Levels of Detail","summary":"  Implicit neural representations map a shape-specific latent code and a 3D\ncoordinate to its corresponding signed distance (SDF) value. However, this\napproach only offers a single level of detail. Emulating low levels of detail\ncan be achieved with shallow networks, but the generated shapes are typically\nnot smooth. Alternatively, some network designs offer multiple levels of\ndetail, but are limited to overfitting a single object.\n  To address this, we propose a new shape modeling approach, which enables\nmultiple levels of detail and guarantees a smooth surface at each level. At the\ncore, we introduce a novel latent conditioning for a multiscale and\nbandwith-limited neural architecture. This results in a deep parameterization\nof multiple shapes, where early layers quickly output approximated SDF values.\nThis allows to balance speed and accuracy within a single network and enhance\nthe efficiency of implicit scene rendering. We demonstrate that by limiting the\nbandwidth of the network, we can maintain smooth surfaces across all levels of\ndetail. At finer levels, reconstruction quality is on par with the state of the\nart models, which are limited to a single level of detail.\n","authors":["Benoit Guillard","Marc Habermann","Christian Theobalt","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2409.06231v1.pdf","comment":"Published in GCPR 2024 proceedings"},{"id":"http://arxiv.org/abs/2404.00785v2","updated":"2024-09-10T05:43:34Z","published":"2024-03-31T20:08:23Z","title":"Disentangling Hippocampal Shape Variations: A Study of Neurological\n  Disorders Using Mesh Variational Autoencoder with Contrastive Learning","summary":"  This paper presents a comprehensive study focused on disentangling\nhippocampal shape variations from diffusion tensor imaging (DTI) datasets\nwithin the context of neurological disorders. Leveraging a Graph Variational\nAutoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach\naims to improve interpretability by disentangling two distinct latent variables\ncorresponding to age and the presence of diseases. In our ablation study, we\ninvestigate a range of VAE architectures and contrastive loss functions,\nshowcasing the enhanced disentanglement capabilities of our approach. This\nevaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh\ndatasets derived from the DTI hippocampal dataset. Our supervised\ndisentanglement model outperforms several state-of-the-art (SOTA) methods like\nattribute and guided VAEs in terms of disentanglement scores. Our model\ndistinguishes between age groups and disease status in patients with Multiple\nSclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised\nContrastive Learning shows the volume changes of the hippocampus of MS\npopulations at different ages, and the result is consistent with the current\nneuroimaging literature. This research provides valuable insights into the\nrelationship between neurological disorder and hippocampal shape changes in\ndifferent age groups of MS populations using a Graph VAE with Supervised\nContrastive loss.\n","authors":["Jakaria Rabbi","Johannes Kiechle","Christian Beaulieu","Nilanjan Ray","Dana Cobzas"],"pdf_url":"https://arxiv.org/pdf/2404.00785v2.pdf","comment":"Length: 25 pages and submitted to the journal: MELBA (Machine\n  Learning for Biomedical Imaging)"},{"id":"http://arxiv.org/abs/2409.06224v1","updated":"2024-09-10T05:28:38Z","published":"2024-09-10T05:28:38Z","title":"MIP-GAF: A MLLM-annotated Benchmark for Most Important Person\n  Localization and Group Context Understanding","summary":"  Estimating the Most Important Person (MIP) in any social event setup is a\nchallenging problem mainly due to contextual complexity and scarcity of labeled\ndata. Moreover, the causality aspects of MIP estimation are quite subjective\nand diverse. To this end, we aim to address the problem by annotating a\nlarge-scale `in-the-wild' dataset for identifying human perceptions about the\n`Most Important Person (MIP)' in an image. The paper provides a thorough\ndescription of our proposed Multimodal Large Language Model (MLLM) based data\nannotation strategy, and a thorough data quality analysis. Further, we perform\na comprehensive benchmarking of the proposed dataset utilizing state-of-the-art\nMIP localization methods, indicating a significant drop in performance compared\nto existing datasets. The performance drop shows that the existing MIP\nlocalization algorithms must be more robust with respect to `in-the-wild'\nsituations. We believe the proposed dataset will play a vital role in building\nthe next-generation social situation understanding methods. The code and data\nis available at https://github.com/surbhimadan92/MIP-GAF.\n","authors":["Surbhi Madan","Shreya Ghosh","Lownish Rai Sookha","M. A. Ganaie","Ramanathan Subramanian","Abhinav Dhall","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2409.06224v1.pdf","comment":"Accepted for publication at WACV 2025"},{"id":"http://arxiv.org/abs/2409.06220v1","updated":"2024-09-10T05:08:26Z","published":"2024-09-10T05:08:26Z","title":"CerviXpert: A Multi-Structural Convolutional Neural Network for\n  Predicting Cervix Type and Cervical Cell Abnormalities","summary":"  Cervical cancer affects millions of women worldwide and has a significantly\nhigher survival rate when diagnosed early. Pap smears and cervical biopsies are\nvital screening tools for detecting such cancer. However, the success of these\nscreening processes depends on the skills of cytologists. A recent trend in\ndiagnostic cytology is to apply machine-learning-based models to classify\ncancer using cell images. These automated models have been shown to perform\njust as well as, or even better than, expert cytologists. Some notable methods\nfor classifying cervix cancers include ResNet50, VGG16, MobileNetV2, and\nInceptionV3, based on deep convolutional neural networks (CNN). However, these\nmethods are computationally expensive. We present CerviXpert, a\nmulti-structural Convolutional Neural Network, to identify cervix cancer. We\nperform extensive experiments on a publicly available dataset, SiPaKMeD, to\nshow the efficacy of our method. CerviXpert presents a promising solution for\nefficient cervical cancer screening and diagnosis by striking a balance between\naccuracy and practical feasibility.\n","authors":["Rashik Shahriar Akash","Radiful Islam","S. M. Saiful Islam Badhon","K. S. M. Tozammel Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.06220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06219v1","updated":"2024-09-10T05:05:34Z","published":"2024-09-10T05:05:34Z","title":"Denoising: A Powerful Building-Block for Imaging, Inverse Problems, and\n  Machine Learning","summary":"  Denoising, the process of reducing random fluctuations in a signal to\nemphasize essential patterns, has been a fundamental problem of interest since\nthe dawn of modern scientific inquiry. Recent denoising techniques,\nparticularly in imaging, have achieved remarkable success, nearing theoretical\nlimits by some measures. Yet, despite tens of thousands of research papers, the\nwide-ranging applications of denoising beyond noise removal have not been fully\nrecognized. This is partly due to the vast and diverse literature, making a\nclear overview challenging.\n  This paper aims to address this gap. We present a comprehensive perspective\non denoisers, their structure, and desired properties. We emphasize the\nincreasing importance of denoising and showcase its evolution into an essential\nbuilding block for complex tasks in imaging, inverse problems, and machine\nlearning. Despite its long history, the community continues to uncover\nunexpected and groundbreaking uses for denoising, further solidifying its place\nas a cornerstone of scientific and engineering practice.\n","authors":["Peyman Milanfar","Mauricio Delbracio"],"pdf_url":"https://arxiv.org/pdf/2409.06219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06217v1","updated":"2024-09-10T04:58:48Z","published":"2024-09-10T04:58:48Z","title":"DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition","summary":"  Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.\n","authors":["Kaixiang Yang","Qiang Li","Zhiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.06217v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07511v2","updated":"2024-09-10T04:52:17Z","published":"2023-10-11T14:07:05Z","title":"Learning a Cross-modality Anomaly Detector for Remote Sensing Imagery","summary":"  Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets for Earth monitoring. Given the diversity in\nearth anomaly types, designing a transferring model with cross-modality\ndetection ability should be cost-effective and flexible to new earth\nobservation sources and anomaly types. However, the current anomaly detectors\naim to learn the certain background distribution, the trained model cannot be\ntransferred to unseen images. Inspired by the fact that the deviation metric\nfor score ranking is consistent and independent from the image distribution,\nthis study exploits the learning target conversion from the varying background\ndistribution to the consistent deviation metric. We theoretically prove that\nthe large-margin condition in labeled samples ensures the transferring ability\nof learned deviation metric. To satisfy this condition, two large margin losses\nfor pixel-level and feature-level deviation ranking are proposed respectively.\nSince the real anomalies are difficult to acquire, anomaly simulation\nstrategies are designed to compute the model loss. With the large-margin\nlearning for deviation metric, the trained model achieves cross-modality\ndetection ability in five modalities including hyperspectral, visible light,\nsynthetic aperture radar (SAR), infrared and low-light in zero-shot manner.\n","authors":["Jingtao Li","Xinyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.07511v2.pdf","comment":"Journal paper"},{"id":"http://arxiv.org/abs/2409.06214v1","updated":"2024-09-10T04:45:25Z","published":"2024-09-10T04:45:25Z","title":"Towards Generalizable Scene Change Detection","summary":"  Scene Change Detection (SCD) is vital for applications such as visual\nsurveillance and mobile robotics. However, current SCD methods exhibit a bias\nto the temporal order of training datasets and limited performance on unseen\ndomains; coventional SCD benchmarks are not able to evaluate generalization or\ntemporal consistency. To tackle these limitations, we introduce a Generalizable\nScene Change Detection Framework (GeSCF) in this work. The proposed GeSCF\nleverages localized semantics of a foundation model without any re-training or\nfine-tuning -- for generalization over unseen domains. Specifically, we design\nan adaptive thresholding of the similarity distribution derived from facets of\nthe pre-trained foundation model to generate initial pseudo-change mask. We\nfurther utilize Segment Anything Model's (SAM) class-agnostic masks to refine\npseudo-masks. Moreover, our proposed framework maintains commutative operations\nin all settings to ensure complete temporal consistency. Finally, we define new\nmetrics, evaluation dataset, and evaluation protocol for Generalizable Scene\nChange Detection (GeSCD). Extensive experiments demonstrate that GeSCF excels\nacross diverse and challenging environments -- establishing a new benchmark for\nSCD performance.\n","authors":["Jaewoo Kim","Uehwan Kim"],"pdf_url":"https://arxiv.org/pdf/2409.06214v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.06210v1","updated":"2024-09-10T04:31:51Z","published":"2024-09-10T04:31:51Z","title":"INTRA: Interaction Relationship-aware Weakly Supervised Affordance\n  Grounding","summary":"  Affordance denotes the potential interactions inherent in objects. The\nperception of affordance can enable intelligent agents to navigate and interact\nwith new environments efficiently. Weakly supervised affordance grounding\nteaches agents the concept of affordance without costly pixel-level\nannotations, but with exocentric images. Although recent advances in weakly\nsupervised affordance grounding yielded promising results, there remain\nchallenges including the requirement for paired exocentric and egocentric image\ndataset, and the complexity in grounding diverse affordances for a single\nobject. To address them, we propose INTeraction Relationship-aware weakly\nsupervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this\nproblem as representation learning to identify unique features of interactions\nthrough contrastive learning with exocentric images only, eliminating the need\nfor paired datasets. Moreover, we leverage vision-language model embeddings for\nperforming affordance grounding flexibly with any text, designing\ntext-conditioned affordance map generation to reflect interaction relationship\nfor contrastive learning and enhancing robustness with our text synonym\naugmentation. Our method outperformed prior arts on diverse datasets such as\nAGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate\nthat our method has remarkable domain scalability for synthesized images /\nillustrations and is capable of performing affordance grounding for novel\ninteractions and objects.\n","authors":["Ji Ha Jang","Hoigi Seo","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2409.06210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06206v1","updated":"2024-09-10T04:20:59Z","published":"2024-09-10T04:20:59Z","title":"AgileIR: Memory-Efficient Group Shifted Windows Attention for Agile\n  Image Restoration","summary":"  Image Transformers show a magnificent success in Image Restoration tasks.\nNevertheless, most of transformer-based models are strictly bounded by\nexorbitant memory occupancy. Our goal is to reduce the memory consumption of\nSwin Transformer and at the same time speed up the model during training\nprocess. Thus, we introduce AgileIR, group shifted attention mechanism along\nwith window attention, which sparsely simplifies the model in architecture. We\npropose Group Shifted Window Attention (GSWA) to decompose Shift Window\nMulti-head Self Attention (SW-MSA) and Window Multi-head Self Attention (W-MSA)\ninto groups across their attention heads, contributing to shrinking memory\nusage in back propagation. In addition to that, we keep shifted window masking\nand its shifted learnable biases during training, in order to induce the model\ninteracting across windows within the channel. We also re-allocate projection\nparameters to accelerate attention matrix calculation, which we found a\nnegligible decrease in performance. As a result of experiment, compared with\nour baseline SwinIR and other efficient quantization models, AgileIR keeps the\nperformance still at 32.20 dB on Set5 evaluation dataset, exceeding other\nmethods with tailor-made efficient methods and saves over 50% memory while a\nlarge batch size is employed.\n","authors":["Hongyi Cai","Mohammad Mahdinur Rahman","Mohammad Shahid Akhtar","Jie Li","Jingyu Wu","Zhili Fang"],"pdf_url":"https://arxiv.org/pdf/2409.06206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06202v1","updated":"2024-09-10T04:14:11Z","published":"2024-09-10T04:14:11Z","title":"RealisDance: Equip controllable character animation with realistic hands","summary":"  Controllable character animation is an emerging task that generates character\nvideos controlled by pose sequences from given character images. Although\ncharacter consistency has made significant progress via reference UNet, another\ncrucial factor, pose control, has not been well studied by existing methods\nyet, resulting in several issues: 1) The generation may fail when the input\npose sequence is corrupted. 2) The hands generated using the DWPose sequence\nare blurry and unrealistic. 3) The generated video will be shaky if the pose\nsequence is not smooth enough. In this paper, we present RealisDance to handle\nall the above issues. RealisDance adaptively leverages three types of poses,\navoiding failed generation caused by corrupted pose sequences. Among these pose\ntypes, HaMeR provides accurate 3D and depth information of hands, enabling\nRealisDance to generate realistic hands even for complex gestures. Besides\nusing temporal attention in the main UNet, RealisDance also inserts temporal\nattention into the pose guidance network, smoothing the video from the pose\ncondition aspect. Moreover, we introduce pose shuffle augmentation during\ntraining to further improve generation robustness and video smoothness.\nQualitative experiments demonstrate the superiority of RealisDance over other\nexisting methods, especially in hand quality.\n","authors":["Jingkai Zhou","Benzhi Wang","Weihua Chen","Jingqi Bai","Dongyang Li","Aixi Zhang","Hao Xu","Mingyang Yang","Fan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.06202v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2409.06198v1","updated":"2024-09-10T03:57:31Z","published":"2024-09-10T03:57:31Z","title":"Deep kernel representations of latent space features for low-dose PET-MR\n  imaging robust to variable dose reduction","summary":"  Low-dose positron emission tomography (PET) image reconstruction methods have\npotential to significantly improve PET as an imaging modality. Deep learning\nprovides a promising means of incorporating prior information into the image\nreconstruction problem to produce quantitatively accurate images from\ncompromised signal. Deep learning-based methods for low-dose PET are generally\npoorly conditioned and perform unreliably on images with features not present\nin the training distribution. We present a method which explicitly models deep\nlatent space features using a robust kernel representation, providing robust\nperformance on previously unseen dose reduction factors. Additional constraints\non the information content of deep latent features allow for tuning\nin-distribution accuracy and generalisability. Tests with out-of-distribution\ndose reduction factors ranging from $\\times 10$ to $\\times 1000$ and with both\npaired and unpaired MR, demonstrate significantly improved performance relative\nto conventional deep-learning methods trained using the same data.\nCode:https://github.com/cameronPain\n","authors":["Cameron Dennis Pain","Yasmeen George","Alex Fornito","Gary Egan","Zhaolin Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06198v1.pdf","comment":"19 pages, 15 figures, 4 tables, Submitted to IEEE Transactions on\n  Medical Imaging"},{"id":"http://arxiv.org/abs/2409.06197v1","updated":"2024-09-10T03:57:30Z","published":"2024-09-10T03:57:30Z","title":"UdeerLID+: Integrating LiDAR, Image, and Relative Depth with\n  Semi-Supervised","summary":"  Road segmentation is a critical task for autonomous driving systems,\nrequiring accurate and robust methods to classify road surfaces from various\nenvironmental data. Our work introduces an innovative approach that integrates\nLiDAR point cloud data, visual image, and relative depth maps derived from\nimages. The integration of multiple data sources in road segmentation presents\nboth opportunities and challenges. One of the primary challenges is the\nscarcity of large-scale, accurately labeled datasets that are necessary for\ntraining robust deep learning models. To address this, we have developed the\n[UdeerLID+] framework under a semi-supervised learning paradigm. Experiments\nresults on KITTI datasets validate the superior performance.\n","authors":["Tao Ni","Xin Zhan","Tao Luo","Wenbin Liu","Zhan Shi","JunBo Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05239v6","updated":"2024-09-10T03:53:55Z","published":"2023-12-08T18:44:09Z","title":"SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational\n  Score Distillation","summary":"  Despite their ability to generate high-resolution and diverse images from\ntext prompts, text-to-image diffusion models often suffer from slow iterative\nsampling processes. Model distillation is one of the most effective directions\nto accelerate these models. However, previous distillation methods fail to\nretain the generation quality while requiring a significant amount of images\nfor training, either from real data or synthetically generated by the teacher\nmodel. In response to this limitation, we present a novel image-free\ndistillation scheme named $\\textbf{SwiftBrush}$. Drawing inspiration from\ntext-to-3D synthesis, in which a 3D neural radiance field that aligns with the\ninput prompt can be obtained from a 2D text-to-image diffusion prior via a\nspecialized loss without the use of any 3D data ground-truth, our approach\nre-purposes that same loss for distilling a pretrained multi-step text-to-image\nmodel to a student network that can generate high-fidelity images with just a\nsingle inference step. In spite of its simplicity, our model stands as one of\nthe first one-step text-to-image generators that can produce images of\ncomparable quality to Stable Diffusion without reliance on any training image\ndata. Remarkably, SwiftBrush achieves an FID score of $\\textbf{16.67}$ and a\nCLIP score of $\\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive\nresults or even substantially surpassing existing state-of-the-art distillation\ntechniques.\n","authors":["Thuan Hoang Nguyen","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2312.05239v6.pdf","comment":"Accepted to CVPR 2024; Github:\n  https://github.com/VinAIResearch/SwiftBrush"},{"id":"http://arxiv.org/abs/2403.10094v2","updated":"2024-09-10T03:50:09Z","published":"2024-03-15T08:19:57Z","title":"RangeLDM: Fast Realistic LiDAR Point Cloud Generation","summary":"  Autonomous driving demands high-quality LiDAR data, yet the cost of physical\nLiDAR sensors presents a significant scaling-up challenge. While recent efforts\nhave explored deep generative models to address this issue, they often consume\nsubstantial computational resources with slow generation speeds while suffering\nfrom a lack of realism. To address these limitations, we introduce RangeLDM, a\nnovel approach for rapidly generating high-quality range-view LiDAR point\nclouds via latent diffusion models. We achieve this by correcting range-view\ndata distribution for accurate projection from point clouds to range images via\nHough voting, which has a critical impact on generative learning. We then\ncompress the range images into a latent space with a variational autoencoder,\nand leverage a diffusion model to enhance expressivity. Additionally, we\ninstruct the model to preserve 3D structural fidelity by devising a\nrange-guided discriminator. Experimental results on KITTI-360 and nuScenes\ndatasets demonstrate both the robust expressiveness and fast speed of our LiDAR\npoint cloud generation.\n","authors":["Qianjiang Hu","Zhimin Zhang","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2403.10094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05463v2","updated":"2024-09-10T03:39:24Z","published":"2024-09-09T09:43:17Z","title":"DriveScape: Towards High-Resolution Controllable Multi-View Driving\n  Video Generation","summary":"  Recent advancements in generative models have provided promising solutions\nfor synthesizing realistic driving videos, which are crucial for training\nautonomous driving perception models. However, existing approaches often\nstruggle with multi-view video generation due to the challenges of integrating\n3D information while maintaining spatial-temporal consistency and effectively\nlearning from a unified model. In this paper, we propose an end-to-end\nframework named DriveScape for multi-view, 3D condition-guided video\ngeneration. DriveScape not only streamlines the process by integrating camera\ndata to ensure comprehensive spatial-temporal coverage, but also introduces a\nBi-Directional Modulated Transformer module to effectively align 3D road\nstructural information. As a result, our approach enables precise control over\nvideo generation, significantly enhancing realism and providing a robust\nsolution for generating multi-view driving videos. Our framework achieves\nstate-of-the-art results on the nuScenes dataset, demonstrating impressive\ngenerative quality metrics with an FID score of 8.34 and an FVD score of 76.39,\nas well as superior performance across various perception tasks. This paves the\nway for more accurate environmental simulations in autonomous driving. Code\nwill be available at\n\\href{https://metadrivescape.github.io/papers_project/drivescapev1/index.html}{our\nproject homepage}.\n","authors":["Wei Wu","Xi Guo","Weixuan Tang","Tingxuan Huang","Chiyu Wang","Dongyue Chen","Chenjing Ding"],"pdf_url":"https://arxiv.org/pdf/2409.05463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06189v1","updated":"2024-09-10T03:39:08Z","published":"2024-09-10T03:39:08Z","title":"MyGo: Consistent and Controllable Multi-View Driving Video Generation\n  with Camera Control","summary":"  High-quality driving video generation is crucial for providing training data\nfor autonomous driving models. However, current generative models rarely focus\non enhancing camera motion control under multi-view tasks, which is essential\nfor driving video generation. Therefore, we propose MyGo, an end-to-end\nframework for video generation, introducing motion of onboard cameras as\nconditions to make progress in camera controllability and multi-view\nconsistency. MyGo employs additional plug-in modules to inject camera\nparameters into the pre-trained video diffusion model, which retains the\nextensive knowledge of the pre-trained model as much as possible. Furthermore,\nwe use epipolar constraints and neighbor view information during the generation\nprocess of each view to enhance spatial-temporal consistency. Experimental\nresults show that MyGo has achieved state-of-the-art results in both general\ncamera-controlled video generation and multi-view driving video generation\ntasks, which lays the foundation for more accurate environment simulation in\nautonomous driving. Project page:\n\\href{https://metadrivescape.github.io/papers_project/MyGo/page.html}{metadrivescape.github.io/papers\\_project/MyGo/page.html}\n","authors":["Yining Yao","Xi Guo","Chenjing Ding","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.06189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06187v1","updated":"2024-09-10T03:31:18Z","published":"2024-09-10T03:31:18Z","title":"Bottleneck-based Encoder-decoder ARchitecture (BEAR) for Learning\n  Unbiased Consumer-to-Consumer Image Representations","summary":"  Unbiased representation learning is still an object of study under specific\napplications and contexts. Novel architectures are usually crafted to resolve\nparticular problems using mixtures of fundamental pieces. This paper presents\ndifferent image feature extraction mechanisms that work together with residual\nconnections to encode perceptual image information in an autoencoder\nconfiguration. We use image data that aims to support a larger research agenda\ndealing with issues regarding criminal activity in consumer-to-consumer online\nplatforms. Preliminary results suggest that the proposed architecture can learn\nrich spaces using ours and other image datasets resolving important challenges\nthat are identified.\n","authors":["Pablo Rivas","Gisela Bichler","Tomas Cerny","Laurie Giddens","Stacie Petter"],"pdf_url":"https://arxiv.org/pdf/2409.06187v1.pdf","comment":"2022 LXAI Workshop at the 39th International Conference on Machine\n  Learning (ICML), Baltimore, Maryland"},{"id":"http://arxiv.org/abs/2401.11067v2","updated":"2024-09-10T03:29:20Z","published":"2024-01-20T00:21:58Z","title":"Make-A-Shape: a Ten-Million-scale 3D Shape Model","summary":"  Significant progress has been made in training large generative models for\nnatural language and images. Yet, the advancement of 3D generative models is\nhindered by their substantial resource demands for training, along with\ninefficient, non-compact, and less expressive representations. This paper\nintroduces Make-A-Shape, a new 3D generative model designed for efficient\ntraining on a vast scale, capable of utilizing 10 millions publicly-available\nshapes. Technical-wise, we first innovate a wavelet-tree representation to\ncompactly encode shapes by formulating the subband coefficient filtering scheme\nto efficiently exploit coefficient relations. We then make the representation\ngeneratable by a diffusion model by devising the subband coefficients packing\nscheme to layout the representation in a low-resolution grid. Further, we\nderive the subband adaptive training strategy to train our model to effectively\nlearn to generate coarse and detail wavelet coefficients. Last, we extend our\nframework to be controlled by additional input conditions to enable it to\ngenerate shapes from assorted modalities, e.g., single/multi-view images, point\nclouds, and low-resolution voxels. In our extensive set of experiments, we\ndemonstrate various applications, such as unconditional generation, shape\ncompletion, and conditional generation on a wide range of modalities. Our\napproach not only surpasses the state of the art in delivering high-quality\nresults but also efficiently generates shapes within a few seconds, often\nachieving this in just 2 seconds for most conditions. Our source code is\navailable at https://github.com/AutodeskAILab/Make-a-Shape.\n","authors":["Ka-Hei Hui","Aditya Sanghi","Arianna Rampini","Kamal Rahimi Malekshan","Zhengzhe Liu","Hooman Shayani","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06183v1","updated":"2024-09-10T03:25:24Z","published":"2024-09-10T03:25:24Z","title":"EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation","summary":"  Due to their text-to-image synthesis feature, diffusion models have recently\nseen a rise in visual perception tasks, such as depth estimation. The lack of\ngood-quality datasets makes the extraction of a fine-grain semantic context\nchallenging for the diffusion models. The semantic context with fewer details\nfurther worsens the process of creating effective text embeddings that will be\nused as input for diffusion models. In this paper, we propose a novel EDADepth,\nan enhanced data augmentation method to estimate monocular depth without using\nadditional training data. We use Swin2SR, a super-resolution model, to enhance\nthe quality of input images. We employ the BEiT pre-trained semantic\nsegmentation model for better extraction of text embeddings. We introduce\nBLIP-2 tokenizer to generate tokens from these text embeddings. The novelty of\nour approach is the introduction of Swin2SR, the BEiT model, and the BLIP-2\ntokenizer in the diffusion-based pipeline for the monocular depth estimation.\nOur model achieves state-of-the-art results (SOTA) on the {\\delta}3 metric on\nNYUv2 and KITTI datasets. It also achieves results comparable to those of the\nSOTA models in the RMSE and REL metrics. Finally, we also show improvements in\nthe visualization of the estimated depth compared to the SOTA diffusion-based\nmonocular depth estimation models. Code:\nhttps://github.com/edadepthmde/EDADepth_ICMLA.\n","authors":["Nischal Khanal","Shivanand Venkanna Sheshappanavar"],"pdf_url":"https://arxiv.org/pdf/2409.06183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06241v2","updated":"2024-09-10T03:15:07Z","published":"2024-05-10T04:42:21Z","title":"MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth\n  Smooth Regularization","summary":"  This letter introduces a novel framework for dense Visual Simultaneous\nLocalization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM\nbased on Gaussian Splatting has shown promising results. However, in monocular\nscenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit\nweaker tracking capability. To address these limitations, we jointly optimize\nsparse visual odometry tracking and 3D Gaussian Splatting scene representation\nfor the first time. We obtain depth maps on visual odometry keyframe windows\nusing a fast Multi-View Stereo (MVS) network for the geometric supervision of\nGaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense\nAdjustment Ring (SDAR) to reduce the negative effect of estimated depth maps\nand preserve the consistency in scale between the visual odometry and Gaussian\nmaps. We have evaluated our system across various synthetic and real-world\ndatasets. The accuracy of our pose estimation surpasses existing methods and\nachieves state-of-the-art. Additionally, it outperforms previous monocular\nmethods in terms of novel view synthesis and geometric reconstruction\nfidelities.\n","authors":["Pengcheng Zhu","Yaoming Zhuang","Baoquan Chen","Li Li","Chengdong Wu","Zhanlin Liu"],"pdf_url":"https://arxiv.org/pdf/2405.06241v2.pdf","comment":"Accepted by IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2409.04952v2","updated":"2024-09-10T03:07:20Z","published":"2024-09-08T02:19:40Z","title":"Deep Bayesian Active Learning-to-Rank with Relative Annotation for\n  Estimation of Ulcerative Colitis Severity","summary":"  Automatic image-based severity estimation is an important task in\ncomputer-aided diagnosis. Severity estimation by deep learning requires a large\namount of training data to achieve a high performance. In general, severity\nestimation uses training data annotated with discrete (i.e., quantized)\nseverity labels. Annotating discrete labels is often difficult in images with\nambiguous severity, and the annotation cost is high. In contrast, relative\nannotation, in which the severity between a pair of images is compared, can\navoid quantizing severity and thus makes it easier. We can estimate relative\ndisease severity using a learning-to-rank framework with relative annotations,\nbut relative annotation has the problem of the enormous number of pairs that\ncan be annotated. Therefore, the selection of appropriate pairs is essential\nfor relative annotation. In this paper, we propose a deep Bayesian active\nlearning-to-rank that automatically selects appropriate pairs for relative\nannotation. Our method preferentially annotates unlabeled pairs with high\nlearning efficiency from the model uncertainty of the samples. We prove the\ntheoretical basis for adapting Bayesian neural networks to pairwise\nlearning-to-rank and demonstrate the efficiency of our method through\nexperiments on endoscopic images of ulcerative colitis on both private and\npublic datasets. We also show that our method achieves a high performance under\nconditions of significant class imbalance because it automatically selects\nsamples from the minority classes.\n","authors":["Takeaki Kadota","Hideaki Hayashi","Ryoma Bise","Kiyohito Tanaka","Seiichi Uchida"],"pdf_url":"https://arxiv.org/pdf/2409.04952v2.pdf","comment":"14 pages, 8 figures, accepted in Medical Image Analysis 2024"},{"id":"http://arxiv.org/abs/2409.06171v1","updated":"2024-09-10T03:02:39Z","published":"2024-09-10T03:02:39Z","title":"Loss Distillation via Gradient Matching for Point Cloud Completion with\n  Weighted Chamfer Distance","summary":"  3D point clouds enhanced the robot's ability to perceive the geometrical\ninformation of the environments, making it possible for many downstream tasks\nsuch as grasp pose detection and scene understanding. The performance of these\ntasks, though, heavily relies on the quality of data input, as incomplete can\nlead to poor results and failure cases. Recent training loss functions designed\nfor deep learning-based point cloud completion, such as Chamfer distance (CD)\nand its variants (\\eg HyperCD ), imply a good gradient weighting scheme can\nsignificantly boost performance. However, these CD-based loss functions usually\nrequire data-related parameter tuning, which can be time-consuming for\ndata-extensive tasks. To address this issue, we aim to find a family of\nweighted training losses ({\\em weighted CD}) that requires no parameter tuning.\nTo this end, we propose a search scheme, {\\em Loss Distillation via Gradient\nMatching}, to find good candidate loss functions by mimicking the learning\nbehavior in backpropagation between HyperCD and weighted CD. Once this is done,\nwe propose a novel bilevel optimization formula to train the backbone network\nbased on the weighted CD loss. We observe that: (1) with proper weighted\nfunctions, the weighted CD can always achieve similar performance to HyperCD,\nand (2) the Landau weighted CD, namely {\\em Landau CD}, can outperform HyperCD\nfor point cloud completion and lead to new state-of-the-art results on several\nbenchmark datasets. {\\it Our demo code is available at\n\\url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}\n","authors":["Fangzhou Lin","Haotian Liu","Haoying Zhou","Songlin Hou","Kazunori D Yamada","Gregory S. Fischer","Yanhua Li","Haichong K. Zhang","Ziming Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06171v1.pdf","comment":"10 pages, 7 figures, 7 tables, this paper was accepted to IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2409.05005v2","updated":"2024-09-10T02:50:54Z","published":"2024-09-08T07:26:13Z","title":"Towards Patronizing and Condescending Language in Chinese Videos: A\n  Multimodal Dataset and Detector","summary":"  Patronizing and Condescending Language (PCL) is a form of discriminatory\ntoxic speech targeting vulnerable groups, threatening both online and offline\nsafety. While toxic speech research has mainly focused on overt toxicity, such\nas hate speech, microaggressions in the form of PCL remain underexplored.\nAdditionally, dominant groups' discriminatory facial expressions and attitudes\ntoward vulnerable communities can be more impactful than verbal cues, yet these\nframe features are often overlooked. In this paper, we introduce the PCLMM\ndataset, the first Chinese multimodal dataset for PCL, consisting of 715\nannotated videos from Bilibili, with high-quality PCL facial frame spans. We\nalso propose the MultiPCL detector, featuring a facial expression detection\nmodule for PCL recognition, demonstrating the effectiveness of modality\ncomplementarity in this challenging task. Our work makes an important\ncontribution to advancing microaggression detection within the domain of toxic\nspeech.\n","authors":["Hongbo Wang","Junyu Lu","Yan Han","Kai Ma","Liang Yang","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2409.05005v2.pdf","comment":"Under review in ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.01633v2","updated":"2024-09-10T02:39:25Z","published":"2024-09-03T06:04:39Z","title":"Dreaming is All You Need","summary":"  In classification tasks, achieving a harmonious balance between exploration\nand precision is of paramount importance. To this end, this research introduces\ntwo novel deep learning models, SleepNet and DreamNet, to strike this balance.\nSleepNet seamlessly integrates supervised learning with unsupervised ``sleep\"\nstages using pre-trained encoder models. Dedicated neurons within SleepNet are\nembedded in these unsupervised features, forming intermittent ``sleep\" blocks\nthat facilitate exploratory learning. Building upon the foundation of SleepNet,\nDreamNet employs full encoder-decoder frameworks to reconstruct the hidden\nstates, mimicking the human \"dreaming\" process. This reconstruction process\nenables further exploration and refinement of the learned representations.\nMoreover, the principle ideas of our SleepNet and DreamNet are generic and can\nbe applied to both computer vision and natural language processing downstream\ntasks. Through extensive empirical evaluations on diverse image and text\ndatasets, SleepNet and DreanNet have demonstrated superior performance compared\nto state-of-the-art models, showcasing the strengths of unsupervised\nexploration and supervised precision afforded by our innovative approaches.\n","authors":["Mingze Ni","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06166v1","updated":"2024-09-10T02:36:13Z","published":"2024-09-10T02:36:13Z","title":"Revisiting Prompt Pretraining of Vision-Language Models","summary":"  Prompt learning is an effective method to customize Vision-Language Models\n(VLMs) for various downstream tasks, involving tuning very few parameters of\ninput prompt tokens. Recently, prompt pretraining in large-scale dataset (e.g.,\nImageNet-21K) has played a crucial role in prompt learning for universal visual\ndiscrimination. However, we revisit and observe that the limited learnable\nprompts could face underfitting risks given the extensive images during prompt\npretraining, simultaneously leading to poor generalization. To address the\nabove issues, in this paper, we propose a general framework termed Revisiting\nPrompt Pretraining (RPP), which targets at improving the fitting and\ngeneralization ability from two aspects: prompt structure and prompt\nsupervision. For prompt structure, we break the restriction in common practice\nwhere query, key, and value vectors are derived from the shared learnable\nprompt token. Instead, we introduce unshared individual query, key, and value\nlearnable prompts, thereby enhancing the model's fitting capacity through\nincreased parameter diversity. For prompt supervision, we additionally utilize\nsoft labels derived from zero-shot probability predictions provided by a\npretrained Contrastive Language Image Pretraining (CLIP) teacher model. These\nsoft labels yield more nuanced and general insights into the inter-class\nrelationships, thereby endowing the pretraining process with better\ngeneralization ability. RPP produces a more resilient prompt initialization,\nenhancing its robust transferability across diverse visual recognition tasks.\nExperiments across various benchmarks consistently confirm the state-of-the-art\n(SOTA) performance of our pretrained prompts. Codes and models will be made\navailable soon.\n","authors":["Zhenyuan Chen","Lingfeng Yang","Shuo Chen","Zhaowei Chen","Jiajun Liang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2409.06166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00739v2","updated":"2024-09-10T02:34:24Z","published":"2023-12-01T17:20:47Z","title":"Adversarial Score Distillation: When score distillation meets GAN","summary":"  Existing score distillation methods are sensitive to classifier-free guidance\n(CFG) scale: manifested as over-smoothness or instability at small CFG scales,\nwhile over-saturation at large ones. To explain and analyze these issues, we\nrevisit the derivation of Score Distillation Sampling (SDS) and decipher\nexisting score distillation with the Wasserstein Generative Adversarial Network\n(WGAN) paradigm. With the WGAN paradigm, we find that existing score\ndistillation either employs a fixed sub-optimal discriminator or conducts\nincomplete discriminator optimization, resulting in the scale-sensitive issue.\nWe propose the Adversarial Score Distillation (ASD), which maintains an\noptimizable discriminator and updates it using the complete optimization\nobjective. Experiments show that the proposed ASD performs favorably in 2D\ndistillation and text-to-3D tasks against existing methods. Furthermore, to\nexplore the generalization ability of our WGAN paradigm, we extend ASD to the\nimage editing task, which achieves competitive results. The project page and\ncode are at https://github.com/2y7c3/ASD.\n","authors":["Min Wei","Jingkai Zhou","Junyao Sun","Xuesong Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.00739v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2404.17503v2","updated":"2024-09-10T02:32:32Z","published":"2024-04-26T16:09:42Z","title":"Inhomogeneous illumination image enhancement under ex-tremely low\n  visibility condition","summary":"  Imaging through dense fog presents unique challenges, with essential visual\ninformation crucial for applications like object detection and recognition\nobscured, thereby hindering conventional image processing methods. Despite\nimprovements through neural network-based approaches, these techniques falter\nunder extremely low visibility conditions exacerbated by inhomogeneous\nillumination, which degrades deep learning performance due to inconsistent\nsignal intensities. We introduce in this paper a novel method that adaptively\nfilters background illumination based on Structural Differential and Integral\nFiltering (SDIF) to enhance only vital signal information. The grayscale\nbanding is eliminated by incorporating a visual optimization strategy based on\nimage gradients. Maximum Histogram Equalization (MHE) is used to achieve high\ncontrast while maintaining fidelity to the original content. We evaluated our\nalgorithm using data collected from both a fog chamber and outdoor\nenvironments, and performed comparative analyses with existing methods. Our\nfindings demonstrate that our proposed method significantly enhances signal\nclarity under extremely low visibility conditions and out-performs existing\ntechniques, offering substantial improvements for deep fog imaging\napplications.\n","authors":["Libang Chen","Jinyan Lin","Qihang Bian","Yikun Liu","Jianying Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.17503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05862v2","updated":"2024-09-10T02:28:40Z","published":"2024-09-09T17:59:13Z","title":"Evaluating Multiview Object Consistency in Humans and Image Models","summary":"  We introduce a benchmark to directly evaluate the alignment between human\nobservers and vision models on a 3D shape inference task. We leverage an\nexperimental design from the cognitive sciences which requires zero-shot visual\ninferences about object shape: given a set of images, participants identify\nwhich contain the same/different objects, despite considerable viewpoint\nvariation. We draw from a diverse range of images that include common objects\n(e.g., chairs) as well as abstract shapes (i.e., procedurally generated\n`nonsense' objects). After constructing over 2000 unique image sets, we\nadminister these tasks to human participants, collecting 35K trials of\nbehavioral data from over 500 participants. This includes explicit choice\nbehaviors as well as intermediate measures, such as reaction time and gaze\ndata. We then evaluate the performance of common vision models (e.g., DINOv2,\nMAE, CLIP). We find that humans outperform all models by a wide margin. Using a\nmulti-scale evaluation approach, we identify underlying similarities and\ndifferences between models and humans: while human-model performance is\ncorrelated, humans allocate more time/processing on challenging trials. All\nimages, data, and code can be accessed via our project page.\n","authors":["Tyler Bonnen","Stephanie Fu","Yutong Bai","Thomas O'Connell","Yoni Friedman","Nancy Kanwisher","Joshua B. Tenenbaum","Alexei A. Efros"],"pdf_url":"https://arxiv.org/pdf/2409.05862v2.pdf","comment":"Project page: https://tzler.github.io/MOCHI/ Code:\n  https://github.com/tzler/mochi_code Huggingface dataset:\n  https://huggingface.co/datasets/tzler/MOCHI"},{"id":"http://arxiv.org/abs/2409.06154v1","updated":"2024-09-10T01:57:57Z","published":"2024-09-10T01:57:57Z","title":"UniLearn: Enhancing Dynamic Facial Expression Recognition through\n  Unified Pre-Training and Fine-Tuning on Images and Videos","summary":"  Dynamic facial expression recognition (DFER) is essential for understanding\nhuman emotions and behavior. However, conventional DFER methods, which\nprimarily use dynamic facial data, often underutilize static expression images\nand their labels, limiting their performance and robustness. To overcome this,\nwe introduce UniLearn, a novel unified learning paradigm that integrates static\nfacial expression recognition (SFER) data to enhance DFER task. UniLearn\nemploys a dual-modal self-supervised pre-training method, leveraging both\nfacial expression images and videos to enhance a ViT model's spatiotemporal\nrepresentation capability. Then, the pre-trained model is fine-tuned on both\nstatic and dynamic expression datasets using a joint fine-tuning strategy. To\nprevent negative transfer during joint fine-tuning, we introduce an innovative\nMixture of Adapter Experts (MoAE) module that enables task-specific knowledge\nacquisition and effectively integrates information from both static and dynamic\nexpression data. Extensive experiments demonstrate UniLearn's effectiveness in\nleveraging complementary information from static and dynamic facial data,\nleading to more accurate and robust DFER. UniLearn consistently achieves\nstate-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with\nweighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively.\nThe source code and model weights will be publicly available at\n\\url{https://github.com/MSA-LMC/UniLearn}.\n","authors":["Yin Chen","Jia Li","Yu Zhang","Zhenzhen Hu","Shiguang Shan","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2409.06154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19605v2","updated":"2024-09-10T01:34:13Z","published":"2024-07-28T22:35:08Z","title":"Look Hear: Gaze Prediction for Speech-directed Human Attention","summary":"  For computer systems to effectively interact with humans using spoken\nlanguage, they need to understand how the words being generated affect the\nusers' moment-by-moment attention. Our study focuses on the incremental\nprediction of attention as a person is seeing an image and hearing a referring\nexpression defining the object in the scene that should be fixated by gaze. To\npredict the gaze scanpaths in this incremental object referral task, we\ndeveloped the Attention in Referral Transformer model or ART, which predicts\nthe human fixations spurred by each word in a referring expression. ART uses a\nmultimodal transformer encoder to jointly learn gaze behavior and its\nunderlying grounding tasks, and an autoregressive transformer decoder to\npredict, for each word, a variable number of fixations based on fixation\nhistory. To train ART, we created RefCOCO-Gaze, a large-scale dataset of 19,738\nhuman gaze scanpaths, corresponding to 2,094 unique image-expression pairs,\nfrom 220 participants performing our referral task. In our quantitative and\nqualitative analyses, ART not only outperforms existing methods in scanpath\nprediction, but also appears to capture several human attention patterns, such\nas waiting, scanning, and verification.\n","authors":["Sounak Mondal","Seoyoung Ahn","Zhibo Yang","Niranjan Balasubramanian","Dimitris Samaras","Gregory Zelinsky","Minh Hoai"],"pdf_url":"https://arxiv.org/pdf/2407.19605v2.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2405.17872v3","updated":"2024-09-10T01:27:28Z","published":"2024-05-28T06:48:02Z","title":"HFGS: 4D Gaussian Splatting with Emphasis on Spatial and Temporal\n  High-Frequency Components for Endoscopic Scene Reconstruction","summary":"  Robot-assisted minimally invasive surgery benefits from enhancing dynamic\nscene reconstruction, as it improves surgical outcomes. While Neural Radiance\nFields (NeRF) have been effective in scene reconstruction, their slow inference\nspeeds and lengthy training durations limit their applicability. To overcome\nthese limitations, 3D Gaussian Splatting (3D-GS) based methods have emerged as\na recent trend, offering rapid inference capabilities and superior 3D quality.\nHowever, these methods still struggle with under-reconstruction in both static\nand dynamic scenes. In this paper, we propose HFGS, a novel approach for\ndeformable endoscopic reconstruction that addresses these challenges from\nspatial and temporal frequency perspectives. Our approach incorporates\ndeformation fields to better handle dynamic scenes and introduces Spatial\nHigh-Frequency Emphasis Reconstruction (SHF) to minimize discrepancies in\nspatial frequency spectra between the rendered image and its ground truth.\nAdditionally, we introduce Temporal High-Frequency Emphasis Reconstruction\n(THF) to enhance dynamic awareness in neural rendering by leveraging flow\npriors, focusing optimization on motion-intensive parts. Extensive experiments\non two widely used benchmarks demonstrate that HFGS achieves superior rendering\nquality.\n","authors":["Haoyu Zhao","Xingyue Zhao","Lingting Zhu","Weixi Zheng","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2405.17872v3.pdf","comment":"BMVC2024"},{"id":"http://arxiv.org/abs/2409.06135v1","updated":"2024-09-10T01:07:20Z","published":"2024-09-10T01:07:20Z","title":"Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis","summary":"  Foley is a term commonly used in filmmaking, referring to the addition of\ndaily sound effects to silent films or videos to enhance the auditory\nexperience. Video-to-Audio (V2A), as a particular type of automatic foley task,\npresents inherent challenges related to audio-visual synchronization. These\nchallenges encompass maintaining the content consistency between the input\nvideo and the generated audio, as well as the alignment of temporal and\nloudness properties within the video. To address these issues, we construct a\ncontrollable video-to-audio synthesis model, termed Draw an Audio, which\nsupports multiple input instructions through drawn masks and loudness signals.\nTo ensure content consistency between the synthesized audio and target video,\nwe introduce the Mask-Attention Module (MAM), which employs masked video\ninstruction to enable the model to focus on regions of interest. Additionally,\nwe implement the Time-Loudness Module (TLM), which uses an auxiliary loudness\nsignal to ensure the synthesis of sound that aligns with the video in both\nloudness and temporal dimensions. Furthermore, we have extended a large-scale\nV2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive\nexperiments on challenging benchmarks across two large-scale V2A datasets\nverify Draw an Audio achieves the state-of-the-art. Project page:\nhttps://yannqi.github.io/Draw-an-Audio/.\n","authors":["Qi Yang","Binjie Mao","Zili Wang","Xing Nie","Pengfei Gao","Ying Guo","Cheng Zhen","Pengfei Yan","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2409.06135v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2409.06129v1","updated":"2024-09-10T00:51:49Z","published":"2024-09-10T00:51:49Z","title":"DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned\n  Geometry Enhancement","summary":"  We present a 3D modeling method which enables end-users to refine or\ndetailize 3D shapes using machine learning, expanding the capabilities of\nAI-assisted 3D content creation. Given a coarse voxel shape (e.g., one produced\nwith a simple box extrusion tool or via generative modeling), a user can\ndirectly \"paint\" desired target styles representing compelling geometric\ndetails, from input exemplar shapes, over different regions of the coarse\nshape. These regions are then up-sampled into high-resolution geometries which\nadhere with the painted styles. To achieve such controllable and localized 3D\ndetailization, we build on top of a Pyramid GAN by making it masking-aware. We\ndevise novel structural losses and priors to ensure that our method preserves\nboth desired coarse structures and fine-grained features even if the painted\nstyles are borrowed from diverse sources, e.g., different semantic parts and\neven different shape categories. Through extensive experiments, we show that\nour ability to localize details enables novel interactive creative workflows\nand applications. Our experiments further demonstrate that in comparison to\nprior techniques built on global detailization, our method generates\nstructure-preserving, high-resolution stylized geometries with more coherent\nshape details and style transitions.\n","authors":["Qimin Chen","Zhiqin Chen","Vladimir G. Kim","Noam Aigerman","Hao Zhang","Siddhartha Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2409.06129v1.pdf","comment":"ECCV 2024 (poster). Code: https://qiminchen.github.io/decollage/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.04667v2","updated":"2024-09-10T17:56:52Z","published":"2024-09-07T00:46:58Z","title":"QueryBuilder: Human-in-the-Loop Query Development for Information\n  Retrieval","summary":"  Frequently, users of an Information Retrieval (IR) system start with an\noverarching information need (a.k.a., an analytic task) and proceed to define\nfiner-grained queries covering various important aspects (i.e., sub-topics) of\nthat analytic task. We present a novel, interactive system called\n$\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create\nqueries with a small amount of effort, through efficient exploration of an\nEnglish development corpus in order to rapidly develop cross-lingual\ninformation retrieval queries corresponding to the user's information needs.\nQueryBuilder performs near real-time retrieval of documents based on\nuser-entered search terms; the user looks through the retrieved documents and\nmarks sentences as relevant to the information needed. The marked sentences are\nused by the system as additional information in query formation and refinement:\nquery terms (and, optionally, event features, which capture event $'triggers'$\n(indicator terms) and agent/patient roles) are appropriately weighted, and a\nneural-based system, which better captures textual meaning, retrieves other\nrelevant content. The process of retrieval and marking is repeated as many\ntimes as desired, giving rise to increasingly refined queries in each\niteration. The final product is a fine-grained query used in Cross-Lingual\nInformation Retrieval (CLIR). Our experiments using analytic tasks and requests\nfrom the IARPA BETTER IR datasets show that with a small amount of effort (at\nmost 10 minutes per sub-topic), novice users can form $\\textit{useful}$\nfine-grained queries including in languages they don't understand. QueryBuilder\nalso provides beneficial capabilities to the traditional corpus exploration and\nquery formation process. A demonstration video is released at\nhttps://vimeo.com/734795835\n","authors":["Hemanth Kandula","Damianos Karakos","Haoling Qiu","Benjamin Rozonoyer","Ian Soboroff","Lee Tarlin","Bonan Min"],"pdf_url":"https://arxiv.org/pdf/2409.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06638v1","updated":"2024-09-10T16:48:05Z","published":"2024-09-10T16:48:05Z","title":"Critical Features Tracking on Triangulated Irregular Networks by a\n  Scale-Space Method","summary":"  The scale-space method is a well-established framework that constructs a\nhierarchical representation of an input signal and facilitates coarse-to-fine\nvisual reasoning. Considering the terrain elevation function as the input\nsignal, the scale-space method can identify and track significant topographic\nfeatures across different scales. The number of scales a feature persists,\ncalled its life span, indicates the importance of that feature. In this way,\nimportant topographic features of a landscape can be selected, which are useful\nfor many applications, including cartography, nautical charting, and land-use\nplanning. The scale-space methods developed for terrain data use gridded\nDigital Elevation Models (DEMs) to represent the terrain. However, gridded DEMs\nlack the flexibility to adapt to the irregular distribution of input data and\nthe varied topological complexity of different regions. Instead, Triangulated\nIrregular Networks (TINs) can be directly generated from irregularly\ndistributed point clouds and accurately preserve important features. In this\nwork, we introduce a novel scale-space analysis pipeline for TINs, addressing\nthe multiple challenges in extending grid-based scale-space methods to TINs.\nOur pipeline can efficiently identify and track topologically important\nfeatures on TINs. Moreover, it is capable of analyzing terrains with irregular\nboundaries, which poses challenges for grid-based methods. Comprehensive\nexperiments show that, compared to grid-based methods, our TIN-based pipeline\nis more efficient, accurate, and has better resolution robustness.\n","authors":["Haoan Feng","Yunting Song","Leila De Floriani"],"pdf_url":"https://arxiv.org/pdf/2409.06638v1.pdf","comment":"13pages, ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2409.05526v2","updated":"2024-09-10T16:46:10Z","published":"2024-09-09T11:35:35Z","title":"RBoard: A Unified Platform for Reproducible and Reusable Recommender\n  System Benchmarks","summary":"  Recommender systems research lacks standardized benchmarks for\nreproducibility and algorithm comparisons. We introduce RBoard, a novel\nframework addressing these challenges by providing a comprehensive platform for\nbenchmarking diverse recommendation tasks, including CTR prediction, Top-N\nrecommendation, and others. RBoard's primary objective is to enable fully\nreproducible and reusable experiments across these scenarios. The framework\nevaluates algorithms across multiple datasets within each task, aggregating\nresults for a holistic performance assessment. It implements standardized\nevaluation protocols, ensuring consistency and comparability. To facilitate\nreproducibility, all user-provided code can be easily downloaded and executed,\nallowing researchers to reliably replicate studies and build upon previous\nwork. By offering a unified platform for rigorous, reproducible evaluation\nacross various recommendation scenarios, RBoard aims to accelerate progress in\nthe field and establish a new standard for recommender systems benchmarking in\nboth academia and industry. The platform is available at https://rboard.org and\nthe demo video can be found at https://bit.ly/rboard-demo.\n","authors":["Xinyang Shao","Edoardo D'Amico","Gabor Fodor","Tri Kurniawan Wijaya"],"pdf_url":"https://arxiv.org/pdf/2409.05526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08921v2","updated":"2024-09-10T15:38:56Z","published":"2024-08-15T12:20:24Z","title":"Graph Retrieval-Augmented Generation: A Survey","summary":"  Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.\n","authors":["Boci Peng","Yun Zhu","Yongchao Liu","Xiaohe Bo","Haizhou Shi","Chuntao Hong","Yan Zhang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2408.08921v2.pdf","comment":"Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided"},{"id":"http://arxiv.org/abs/2405.04614v3","updated":"2024-09-10T14:37:00Z","published":"2024-05-07T18:58:32Z","title":"Multi-Margin Cosine Loss: Proposal and Application in Recommender\n  Systems","summary":"  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nstraightforward nature, relying only on user-item interactions. Typically,\nthese systems consist of three main components: an interaction module, a loss\nfunction, and a negative sampling strategy. Initially, researchers focused on\nenhancing performance by developing complex interaction modules. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has led to an increased interest in contrastive\nlearning, which pulls similar pairs closer while pushing dissimilar ones apart.\nContrastive learning may bring challenges like high memory demands and\nunder-utilization of some negative samples. The proposed Multi-Margin Cosine\nLoss (MMCL) addresses these challenges by introducing multiple margins and\nvarying weights for negative samples. It efficiently utilizes not only the\nhardest negatives but also other non-trivial negatives, offers a simpler yet\neffective loss function that outperforms more complex methods, especially when\nresources are limited. Experiments on two well-known datasets demonstrated that\nMMCL achieved up to a 20\\% performance improvement compared to a baseline loss\nfunction when fewer number of negative samples are used.\n","authors":["Makbule Gulcin Ozsoy"],"pdf_url":"https://arxiv.org/pdf/2405.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07606v2","updated":"2024-09-10T13:39:28Z","published":"2023-09-14T11:13:36Z","title":"Zero-shot Audio Topic Reranking using Large Language Models","summary":"  Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.\n","authors":["Mengjie Qian","Rao Ma","Adian Liusie","Erfan Loweimi","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2309.07606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06464v1","updated":"2024-09-10T12:46:23Z","published":"2024-09-10T12:46:23Z","title":"Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or\n  Inverted Indexes?","summary":"  Practitioners working on dense retrieval today face a bewildering number of\nchoices. Beyond selecting the embedding model, another consequential choice is\nthe actual implementation of nearest-neighbor vector search. While best\npractices recommend HNSW indexes, flat vector indexes with brute-force search\nrepresent another viable option, particularly for smaller corpora and for rapid\nprototyping. In this paper, we provide experimental results on the BEIR dataset\nusing the open-source Lucene search library that explicate the tradeoffs\nbetween HNSW and flat indexes (including quantized variants) from the\nperspectives of indexing time, query evaluation performance, and retrieval\nquality. With additional comparisons between dense and sparse retrievers, our\nresults provide guidance for today's search practitioner in understanding the\ndesign space of dense and sparse retrievers. To our knowledge, we are the first\nto provide operational advice supported by empirical experiments in this\nregard.\n","authors":["Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2409.06464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00860v2","updated":"2024-09-10T10:52:30Z","published":"2024-09-01T22:33:29Z","title":"A Counterfactual Explanation Framework for Retrieval Models","summary":"  Explainability has become a crucial concern in today's world, aiming to\nenhance transparency in machine learning and deep learning models. Information\nretrieval is no exception to this trend. In existing literature on\nexplainability of information retrieval, the emphasis has predominantly been on\nillustrating the concept of relevance concerning a retrieval model. The\nquestions addressed include why a document is relevant to a query, why one\ndocument exhibits higher relevance than another, or why a specific set of\ndocuments is deemed relevant for a query.\n  However, limited attention has been given to understanding why a particular\ndocument is considered non-relevant to a query with respect to a retrieval\nmodel. In an effort to address this gap, our work focus on the question of what\nterms need to be added within a document to improve its ranking. This in turn\nanswers the question of which words played a role in not being favored by a\nretrieval model for a particular query. We use an optimization framework to\nsolve the above-mentioned research problem. % To the best of our knowledge, we\nmark the first attempt to tackle this specific counterfactual problem. Our\nexperiments show the effectiveness of our proposed approach in predicting\ncounterfactuals for both statistical (e.g. BM25) and deep-learning-based models\n(e.g. DRMM, DSSM, ColBERT).\n","authors":["Bhavik Chandna","Procheta Sen"],"pdf_url":"https://arxiv.org/pdf/2409.00860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06377v1","updated":"2024-09-10T09:58:55Z","published":"2024-09-10T09:58:55Z","title":"Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration","summary":"  Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Xiao Zhang","Ming He","Jianping Fan","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06377v1.pdf","comment":"First 3 authors contributes equally to this work"},{"id":"http://arxiv.org/abs/2409.06177v1","updated":"2024-09-10T03:12:39Z","published":"2024-09-10T03:12:39Z","title":"HierLLM: Hierarchical Large Language Model for Question Recommendation","summary":"  Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM.\n","authors":["Yuxuan Liu","Haipeng Liu","Ting Long"],"pdf_url":"https://arxiv.org/pdf/2409.06177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06150v1","updated":"2024-09-10T01:52:29Z","published":"2024-09-10T01:52:29Z","title":"What makes a good concept anyway ?","summary":"  A good medical ontology is expected to cover its domain completely and\ncorrectly. On the other hand, large ontologies are hard to build, hard to\nunderstand, and hard to maintain. Thus, adding new concepts (often multi-word\nconcepts) to an existing ontology must be done judiciously. Only \"good\"\nconcepts should be added; however, it is difficult to define what makes a\nconcept good. In this research, we propose a metric to measure the goodness of\na concept. We identified factors that appear to influence goodness judgments of\nmedical experts and combined them into a single metric. These factors include\nconcept name length (in words), concept occurrence frequency in the medical\nliterature, and syntactic categories of component words. As an added factor, we\nused the simplicity of a term after mapping it into a specific foreign\nlanguage. We performed Bayesian optimization of factor weights to achieve\nmaximum agreement between the metric and three medical experts. The results\nshowed that our metric had a 50.67% overall agreement with the experts, as\nmeasured by Krippendorff's alpha.\n","authors":["Naren Khatwani","James Geller"],"pdf_url":"https://arxiv.org/pdf/2409.06150v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.04667v2","updated":"2024-09-10T17:56:52Z","published":"2024-09-07T00:46:58Z","title":"QueryBuilder: Human-in-the-Loop Query Development for Information\n  Retrieval","summary":"  Frequently, users of an Information Retrieval (IR) system start with an\noverarching information need (a.k.a., an analytic task) and proceed to define\nfiner-grained queries covering various important aspects (i.e., sub-topics) of\nthat analytic task. We present a novel, interactive system called\n$\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create\nqueries with a small amount of effort, through efficient exploration of an\nEnglish development corpus in order to rapidly develop cross-lingual\ninformation retrieval queries corresponding to the user's information needs.\nQueryBuilder performs near real-time retrieval of documents based on\nuser-entered search terms; the user looks through the retrieved documents and\nmarks sentences as relevant to the information needed. The marked sentences are\nused by the system as additional information in query formation and refinement:\nquery terms (and, optionally, event features, which capture event $'triggers'$\n(indicator terms) and agent/patient roles) are appropriately weighted, and a\nneural-based system, which better captures textual meaning, retrieves other\nrelevant content. The process of retrieval and marking is repeated as many\ntimes as desired, giving rise to increasingly refined queries in each\niteration. The final product is a fine-grained query used in Cross-Lingual\nInformation Retrieval (CLIR). Our experiments using analytic tasks and requests\nfrom the IARPA BETTER IR datasets show that with a small amount of effort (at\nmost 10 minutes per sub-topic), novice users can form $\\textit{useful}$\nfine-grained queries including in languages they don't understand. QueryBuilder\nalso provides beneficial capabilities to the traditional corpus exploration and\nquery formation process. A demonstration video is released at\nhttps://vimeo.com/734795835\n","authors":["Hemanth Kandula","Damianos Karakos","Haoling Qiu","Benjamin Rozonoyer","Ian Soboroff","Lee Tarlin","Bonan Min"],"pdf_url":"https://arxiv.org/pdf/2409.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06694v1","updated":"2024-09-10T17:55:59Z","published":"2024-09-10T17:55:59Z","title":"DANCE: Deep Learning-Assisted Analysis of Protein Sequences Using Chaos\n  Enhanced Kaleidoscopic Images","summary":"  Cancer is a complex disease characterized by uncontrolled cell growth. T cell\nreceptors (TCRs), crucial proteins in the immune system, play a key role in\nrecognizing antigens, including those associated with cancer. Recent\nadvancements in sequencing technologies have facilitated comprehensive\nprofiling of TCR repertoires, uncovering TCRs with potent anti-cancer activity\nand enabling TCR-based immunotherapies. However, analyzing these intricate\nbiomolecules necessitates efficient representations that capture their\nstructural and functional information. T-cell protein sequences pose unique\nchallenges due to their relatively smaller lengths compared to other\nbiomolecules. An image-based representation approach becomes a preferred choice\nfor efficient embeddings, allowing for the preservation of essential details\nand enabling comprehensive analysis of T-cell protein sequences. In this paper,\nwe propose to generate images from the protein sequences using the idea of\nChaos Game Representation (CGR) using the Kaleidoscopic images approach. This\nDeep Learning Assisted Analysis of Protein Sequences Using Chaos Enhanced\nKaleidoscopic Images (called DANCE) provides a unique way to visualize protein\nsequences by recursively applying chaos game rules around a central seed point.\nwe perform the classification of the T cell receptors (TCRs) protein sequences\nin terms of their respective target cancer cells, as TCRs are known for their\nimmune response against cancer disease. The TCR sequences are converted into\nimages using the DANCE method. We employ deep-learning vision models to perform\nthe classification to obtain insights into the relationship between the visual\npatterns observed in the generated kaleidoscopic images and the underlying\nprotein properties. By combining CGR-based image generation with deep learning\nclassification, this study opens novel possibilities in the protein analysis\ndomain.\n","authors":["Taslim Murad","Prakash Chourasia","Sarwan Ali","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2409.06694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06692v1","updated":"2024-09-10T17:55:00Z","published":"2024-09-10T17:55:00Z","title":"HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs","summary":"  We consider fact-checking approaches that aim to predict the veracity of\nassertions in knowledge graphs. Five main categories of fact-checking\napproaches for knowledge graphs have been proposed in the recent literature, of\nwhich each is subject to partially overlapping limitations. In particular,\ncurrent text-based approaches are limited by manual feature engineering.\nPath-based and rule-based approaches are limited by their exclusive use of\nknowledge graphs as background knowledge, and embedding-based approaches suffer\nfrom low accuracy scores on current fact-checking tasks. We propose a hybrid\napproach -- dubbed HybridFC -- that exploits the diversity of existing\ncategories of fact-checking approaches within an ensemble learning setting to\nachieve a significantly better prediction performance. In particular, our\napproach outperforms the state of the art by 0.14 to 0.27 in terms of Area\nUnder the Receiver Operating Characteristic curve on the FactBench dataset. Our\ncode is open-source and can be found at https://github.com/dice-group/HybridFC.\n","authors":["Umair Qudus","Michael Roeder","Muhammad Saleem","Axel-Cyrille Ngonga Ngomo"],"pdf_url":"https://arxiv.org/pdf/2409.06692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06691v1","updated":"2024-09-10T17:54:28Z","published":"2024-09-10T17:54:28Z","title":"Geometric-Averaged Preference Optimization for Soft Preference Labels","summary":"  Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, it is reasonable to think\nthat they can vary with different individuals, and thus should be\ndistributional to reflect the fine-grained relationship between the responses.\nIn this work, we introduce the distributional soft preference labels and\nimprove Direct Preference Optimization (DPO) with a weighted geometric average\nof the LLM output likelihood in the loss function. In doing so, the scale of\nlearning loss is adjusted based on the soft labels, and the loss with equally\npreferred responses would be close to zero. This simple modification can be\neasily applied to any DPO family and helps the models escape from the\nover-optimization and objective mismatch prior works suffer from. In our\nexperiments, we simulate the soft preference labels with AI feedback from LLMs\nand demonstrate that geometric averaging consistently improves performance on\nstandard benchmarks for alignment research. In particular, we observe more\npreferable responses than binary labels and significant improvements with data\nwhere modestly-confident labels are in the majority.\n","authors":["Hiroki Furuta","Kuang-Huei Lee","Shixiang Shane Gu","Yutaka Matsuo","Aleksandra Faust","Heiga Zen","Izzeddin Gur"],"pdf_url":"https://arxiv.org/pdf/2409.06691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06673v1","updated":"2024-09-10T17:41:31Z","published":"2024-09-10T17:41:31Z","title":"Liability and Insurance for Catastrophic Losses: the Nuclear Power\n  Precedent and Lessons for AI","summary":"  As AI systems become more autonomous and capable, experts warn of them\npotentially causing catastrophic losses. Drawing on the successful precedent\nset by the nuclear power industry, this paper argues that developers of\nfrontier AI models should be assigned limited, strict, and exclusive third\nparty liability for harms resulting from Critical AI Occurrences (CAIOs) -\nevents that cause or easily could have caused catastrophic losses. Mandatory\ninsurance for CAIO liability is recommended to overcome developers'\njudgment-proofness, mitigate winner's curse dynamics, and leverage insurers'\nquasi-regulatory abilities. Based on theoretical arguments and observations\nfrom the analogous nuclear power context, insurers are expected to engage in a\nmix of causal risk-modeling, monitoring, lobbying for stricter regulation, and\nproviding loss prevention guidance in the context of insuring against\nheavy-tail risks from AI. While not a substitute for regulation, clear\nliability assignment and mandatory insurance can help efficiently allocate\nresources to risk-modeling and safe design, facilitating future regulatory\nefforts.\n","authors":["Cristian Trout"],"pdf_url":"https://arxiv.org/pdf/2409.06673v1.pdf","comment":"Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)"},{"id":"http://arxiv.org/abs/2409.06672v1","updated":"2024-09-10T17:41:24Z","published":"2024-09-10T17:41:24Z","title":"Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort","summary":"  Many experts believe that AI systems will sooner or later pose uninsurable\nrisks, including existential risks. This creates an extreme judgment-proof\nproblem: few if any parties can be held accountable ex post in the event of\nsuch a catastrophe. This paper proposes a novel solution: a\ngovernment-provided, mandatory indemnification program for AI developers. The\nprogram uses risk-priced indemnity fees to induce socially optimal levels of\ncare. Risk-estimates are determined by surveying experts, including indemnified\ndevelopers. The Bayesian Truth Serum mechanism is employed to incent honest and\neffortful responses. Compared to alternatives, this approach arguably better\nleverages all private information, and provides a clearer signal to indemnified\ndevelopers regarding what risks they must mitigate to lower their fees. It's\nrecommended that collected fees be used to help fund the safety research\ndevelopers need, employing a fund matching mechanism (Quadratic Financing) to\ninduce an optimal supply of this public good. Under Quadratic Financing, safety\nresearch projects would compete for private contributions from developers,\nsignaling how much each is to be supplemented with public funds.\n","authors":["Cristian Trout"],"pdf_url":"https://arxiv.org/pdf/2409.06672v1.pdf","comment":"Accepted to Generative AI and Law Workshop at the International\n  Conference on Machine Learning (ICML 2024)"},{"id":"http://arxiv.org/abs/2409.06669v1","updated":"2024-09-10T17:36:15Z","published":"2024-09-10T17:36:15Z","title":"DA-MoE: Towards Dynamic Expert Allocation for Mixture-of-Experts Models","summary":"  Transformer-based Mixture-of-Experts (MoE) models have been driving several\nrecent technological advancements in Natural Language Processing (NLP). These\nMoE models adopt a router mechanism to determine which experts to activate for\nrouting input tokens. However, existing router mechanisms allocate a fixed\nnumber of experts to each token, which neglects the varying importance of\ndifferent input tokens. In this study, we propose a novel dynamic router\nmechanism that Dynamically Allocates a variable number of experts for\nMixture-of-Experts (DA-MoE) models based on an effective token importance\nmeasure. First, we show that the Transformer attention mechanism provides a\nnatural and effective way of calculating token importance. Second, we propose a\ndynamic router mechanism that effectively decides the optimal number of experts\n(K) and allocates the top-K experts for each input token. Third, comprehensive\nexperiments on several benchmark datasets demonstrate that our DA-MoE approach\nconsistently outperforms the state-of-the-art Transformer based MoE model on\nthe popular GLUE benchmark.\n","authors":["Maryam Akhavan Aghdam","Hongpeng Jin","Yanzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2409.06669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00055v2","updated":"2024-09-10T17:26:29Z","published":"2024-08-21T04:47:26Z","title":"SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models","summary":"  The rapid advancement in large language models (LLMs) comes with a\nsignificant increase in their parameter size, presenting challenges for\nadaptation and fine-tuning. Parameter-efficient fine-tuning (PEFT) methods are\nwidely used to adapt LLMs for downstream tasks efficiently. In this paper, we\npropose Singular Values and Orthonormal Regularized Singular Vectors\nAdaptation, or SORSA, a novel PEFT method. We introduce a method to analyze the\nvariation of the parameters by performing singular value decomposition (SVD)\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. Each SORSA adapter consists of two main parts: trainable principal\nsingular weights $W_p = U_p \\Sigma_p V^\\top_p$, and frozen residual weights\n$W_r = U_r \\Sigma_r V^\\top_r$. These parts are initialized by performing SVD on\npre-trained weights. Moreover, we implement and analyze an orthonormal\nregularizer, which could effectively transfer the scaling information into\n$\\Sigma_p$ and ultimately allows the training process to be more efficient.\nSORSA adapters could be merged during inference, thus eliminating any inference\nlatency. After all, SORSA shows a faster convergence than PiSSA and LoRA in our\nexperiments. On the MATH benchmark, Llama 2 7B adapted using SORSA achieved\n10.36% accuracy, outperforming LoRA (5.50%), Full FT (7.22%), and PiSSA\n(7.44%). On the GSM-8K benchmark, SORSA achieved 56.03% accuracy, surpassing\nLoRA (42.30%), Full FT (49.05%), and PiSSA (53.07%). We conclude that SORSA\noffers a new perspective on parameter-efficient fine-tuning, demonstrating\nremarkable performance. The code is available at\nhttps://github.com/Gunale0926/SORSA.\n","authors":["Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2409.00055v2.pdf","comment":"12 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.06656v1","updated":"2024-09-10T17:20:11Z","published":"2024-09-10T17:20:11Z","title":"Sortformer: Seamless Integration of Speaker Diarization and ASR by\n  Bridging Timestamps and Tokens","summary":"  We propose Sortformer, a novel neural model for speaker diarization, trained\nwith unconventional objectives compared to existing end-to-end diarization\nmodels. The permutation problem in speaker diarization has long been regarded\nas a critical challenge. Most prior end-to-end diarization systems employ\npermutation invariant loss (PIL), which optimizes for the permutation that\nyields the lowest error. In contrast, we introduce Sort Loss, which enables a\ndiarization model to autonomously resolve permutation, with or without PIL. We\ndemonstrate that combining Sort Loss and PIL achieves performance competitive\nwith state-of-the-art end-to-end diarization models trained exclusively with\nPIL. Crucially, we present a streamlined multispeaker ASR architecture that\nleverages Sortformer as a speaker supervision model, embedding speaker label\nestimation within the ASR encoder state using a sinusoidal kernel function.\nThis approach resolves the speaker permutation problem through sorted\nobjectives, effectively bridging speaker-label timestamps and speaker tokens.\nIn our experiments, we show that the proposed multispeaker ASR architecture,\nenhanced with speaker supervision, improves performance via adapter techniques.\nCode and trained models will be made publicly available via the NVIDIA NeMo\nframework\n","authors":["Taejin Park","Ivan Medennikov","Kunal Dhawan","Weiqing Wang","He Huang","Nithin Rao Koluguri","Krishna C. Puvvada","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2409.06656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06649v1","updated":"2024-09-10T17:12:37Z","published":"2024-09-10T17:12:37Z","title":"KANtrol: A Physics-Informed Kolmogorov-Arnold Network Framework for\n  Solving Multi-Dimensional and Fractional Optimal Control Problems","summary":"  In this paper, we introduce the KANtrol framework, which utilizes\nKolmogorov-Arnold Networks (KANs) to solve optimal control problems involving\ncontinuous time variables. We explain how Gaussian quadrature can be employed\nto approximate the integral parts within the problem, particularly for\nintegro-differential state equations. We also demonstrate how automatic\ndifferentiation is utilized to compute exact derivatives for integer-order\ndynamics, while for fractional derivatives of non-integer order, we employ\nmatrix-vector product discretization within the KAN framework. We tackle\nmulti-dimensional problems, including the optimal control of a 2D heat partial\ndifferential equation. The results of our simulations, which cover both forward\nand parameter identification problems, show that the KANtrol framework\noutperforms classical MLPs in terms of accuracy and efficiency.\n","authors":["Alireza Afzal Aghaei"],"pdf_url":"https://arxiv.org/pdf/2409.06649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02128v2","updated":"2024-09-10T16:42:56Z","published":"2023-05-03T13:58:13Z","title":"System Neural Diversity: Measuring Behavioral Heterogeneity in\n  Multi-Agent Learning","summary":"  Evolutionary science provides evidence that diversity confers resilience in\nnatural systems. Yet, traditional multi-agent reinforcement learning techniques\ncommonly enforce homogeneity to increase training sample efficiency. When a\nsystem of learning agents is not constrained to homogeneous policies,\nindividuals may develop diverse behaviors, resulting in emergent\ncomplementarity that benefits the system. Despite this, there is a surprising\nlack of tools that quantify behavioral diversity. Such techniques would pave\nthe way towards understanding the impact of diversity in collective artificial\nintelligence and enabling its control. In this paper, we introduce System\nNeural Diversity (SND): a measure of behavioral heterogeneity in multi-agent\nsystems. We discuss and prove its theoretical properties, and compare it with\nalternate, state-of-the-art behavioral diversity metrics used in the robotics\ndomain. Through simulations of a variety of cooperative multi-robot tasks, we\nshow how our metric constitutes an important tool that enables measurement and\ncontrol of behavioral heterogeneity. In dynamic tasks, where the problem is\naffected by repeated disturbances during training, we show that SND allows us\nto measure latent resilience skills acquired by the agents, while other\nproxies, such as task performance (reward), fail to. Finally, we show how the\nmetric can be employed to control diversity, allowing us to enforce a desired\nheterogeneity set-point or range. We demonstrate how this paradigm can be used\nto bootstrap the exploration phase, finding optimal policies faster, thus\nenabling novel and more efficient MARL paradigms.\n","authors":["Matteo Bettini","Ajay Shankar","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2305.02128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13878v2","updated":"2024-09-10T16:28:24Z","published":"2024-08-25T16:00:44Z","title":"Generalization of Graph Neural Networks is Robust to Model Mismatch","summary":"  Graph neural networks (GNNs) have demonstrated their effectiveness in various\ntasks supported by their generalization capabilities. However, the current\nanalysis of GNN generalization relies on the assumption that training and\ntesting data are independent and identically distributed (i.i.d). This imposes\nlimitations on the cases where a model mismatch exists when generating testing\ndata. In this paper, we examine GNNs that operate on geometric graphs generated\nfrom manifold models, explicitly focusing on scenarios where there is a\nmismatch between manifold models generating training and testing data. Our\nanalysis reveals the robustness of the GNN generalization in the presence of\nsuch model mismatch. This indicates that GNNs trained on graphs generated from\na manifold can still generalize well to unseen nodes and graphs generated from\na mismatched manifold. We attribute this mismatch to both node feature\nperturbations and edge perturbations within the generated graph. Our findings\nindicate that the generalization gap decreases as the number of nodes grows in\nthe training graph while increasing with larger manifold dimension as well as\nlarger mismatch. Importantly, we observe a trade-off between the generalization\nof GNNs and the capability to discriminate high-frequency components when\nfacing a model mismatch. The most important practical consequence of this\nanalysis is to shed light on the filter design of generalizable GNNs robust to\nmodel mismatch. We verify our theoretical findings with experiments on multiple\nreal-world datasets.\n","authors":["Zhiyang Wang","Juan Cervino","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.13878v2.pdf","comment":"20 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:2406.05225"},{"id":"http://arxiv.org/abs/2409.06624v1","updated":"2024-09-10T16:26:43Z","published":"2024-09-10T16:26:43Z","title":"A Practice of Post-Training on Llama-3 70B with Optimal Selection of\n  Additional Language Mixture Ratio","summary":"  Large Language Models (LLM) often needs to be Continual Pre-Trained (CPT) to\nobtain the unfamiliar language skill or adapt into new domains. The huge\ntraining cost of CPT often asks for cautious choice of key hyper-parameters\nsuch as the mixture ratio of extra language or domain corpus. However, there is\nno systematic study which bridge the gap between the optimal mixture ratio and\nthe actual model performance, and the gap between experimental scaling law and\nthe actual deployment in the full model size. In this paper, we perform CPT on\nLlama-3 8B and 70B to enhance its Chinese ability. We study the optimal\ncorrelation between the Additional Language Mixture Ratio (ALMR) and the\nLearning Rate (LR) on the 8B size which directly indicate the optimal\nexperimental set up. By thorough choice of hyper-parameter, and subsequent\nfine-tuning, the model capability is improved not only on the Chinese-related\nbenchmark, but also some specific domains including math, coding and emotional\nintelligence. We deploy the final 70B version of LLM on an real-life chat\nsystem which obtain satisfying performance.\n","authors":["Ningyuan Xi","Yetao Wu","Kun Fan","Teng Chen","Qingqing Gu","Peng Yu","Jinxian Qu","Chenxi Liu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.06624v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.05225v3","updated":"2024-09-10T16:16:13Z","published":"2024-06-07T19:25:02Z","title":"A Manifold Perspective on the Statistical Generalization of Graph Neural\n  Networks","summary":"  Convolutional neural networks have been successfully extended to operate on\ngraphs, giving rise to Graph Neural Networks (GNNs). GNNs combine information\nfrom adjacent nodes by successive applications of graph convolutions. GNNs have\nbeen implemented successfully in various learning tasks while the theoretical\nunderstanding of their generalization capability is still in progress. In this\npaper, we leverage manifold theory to analyze the statistical generalization\ngap of GNNs operating on graphs constructed on sampled points from manifolds.\nWe study the generalization gaps of GNNs on both node-level and graph-level\ntasks. We show that the generalization gaps decrease with the number of nodes\nin the training graphs, which guarantees the generalization of GNNs to unseen\npoints over manifolds. We validate our theoretical results in multiple\nreal-world datasets.\n","authors":["Zhiyang Wang","Juan Cervino","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2406.05225v3.pdf","comment":"34 pages,22 figures"},{"id":"http://arxiv.org/abs/2409.06618v1","updated":"2024-09-10T16:15:01Z","published":"2024-09-10T16:15:01Z","title":"Hierarchical Multi-Label Classification with Missing Information for\n  Benthic Habitat Imagery","summary":"  In this work, we apply state-of-the-art self-supervised learning techniques\non a large dataset of seafloor imagery, \\textit{BenthicNet}, and study their\nperformance for a complex hierarchical multi-label (HML) classification\ndownstream task. In particular, we demonstrate the capacity to conduct HML\ntraining in scenarios where there exist multiple levels of missing annotation\ninformation, an important scenario for handling heterogeneous real-world data\ncollected by multiple research groups with differing data collection protocols.\nWe find that, when using smaller one-hot image label datasets typical of local\nor regional scale benthic science projects, models pre-trained with\nself-supervision on a larger collection of in-domain benthic data outperform\nmodels pre-trained on ImageNet. In the HML setting, we find the model can\nattain a deeper and more precise classification if it is pre-trained with\nself-supervision on in-domain data. We hope this work can establish a benchmark\nfor future models in the field of automated underwater image annotation tasks\nand can guide work in other domains with hierarchical annotations of mixed\nresolution.\n","authors":["Isaac Xu","Benjamin Misiuk","Scott C. Lowe","Martin Gillis","Craig J. Brown","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2409.06618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06615v1","updated":"2024-09-10T16:11:57Z","published":"2024-09-10T16:11:57Z","title":"One-Shot Imitation under Mismatched Execution","summary":"  Human demonstrations as prompts are a powerful way to program robots to do\nlong-horizon manipulation tasks. However, directly translating such\ndemonstrations into robot-executable actions poses significant challenges due\nto execution mismatches, such as different movement styles and physical\ncapabilities. Existing methods either rely on robot-demonstrator paired data,\nwhich is infeasible to scale, or overly rely on frame-level visual\nsimilarities, which fail to hold. To address these challenges, we propose\nRHyME, a novel framework that automatically establishes task execution\ncorrespondences between the robot and the demonstrator by using optimal\ntransport costs. Given long-horizon robot demonstrations, RHyME synthesizes\nsemantically equivalent human demonstrations by retrieving and composing\nsimilar short-horizon human clips, facilitating effective policy training\nwithout the need for paired data. We show that RHyME outperforms a range of\nbaselines across various cross-embodiment datasets on all degrees of\nmismatches. Through detailed analysis, we uncover insights for learning and\nleveraging cross-embodiment visual representations.\n","authors":["Kushal Kedia","Prithwish Dan","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2409.06615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06613v1","updated":"2024-09-10T16:05:25Z","published":"2024-09-10T16:05:25Z","title":"DemoStart: Demonstration-led auto-curriculum applied to sim-to-real with\n  multi-fingered robots","summary":"  We present DemoStart, a novel auto-curriculum reinforcement learning method\ncapable of learning complex manipulation behaviors on an arm equipped with a\nthree-fingered robotic hand, from only a sparse reward and a handful of\ndemonstrations in simulation. Learning from simulation drastically reduces the\ndevelopment cycle of behavior generation, and domain randomization techniques\nare leveraged to achieve successful zero-shot sim-to-real transfer. Transferred\npolicies are learned directly from raw pixels from multiple cameras and robot\nproprioception. Our approach outperforms policies learned from demonstrations\non the real robot and requires 100 times fewer demonstrations, collected in\nsimulation. More details and videos in https://sites.google.com/view/demostart.\n","authors":["Maria Bauza","Jose Enrique Chen","Valentin Dalibard","Nimrod Gileadi","Roland Hafner","Murilo F. Martins","Joss Moore","Rugile Pevceviciute","Antoine Laurens","Dushyant Rao","Martina Zambelli","Martin Riedmiller","Jon Scholz","Konstantinos Bousmalis","Francesco Nori","Nicolas Heess"],"pdf_url":"https://arxiv.org/pdf/2409.06613v1.pdf","comment":"15 pages total with 7 pages of appendix. 9 Figures, 4 in the main\n  text and 5 in the appendix"},{"id":"http://arxiv.org/abs/2409.06612v1","updated":"2024-09-10T16:04:10Z","published":"2024-09-10T16:04:10Z","title":"Label-free Monitoring of Self-Supervised Learning Progress","summary":"  Self-supervised learning (SSL) is an effective method for exploiting\nunlabelled data to learn a high-level embedding space that can be used for\nvarious downstream tasks. However, existing methods to monitor the quality of\nthe encoder -- either during training for one model or to compare several\ntrained models -- still rely on access to annotated data. When SSL\nmethodologies are applied to new data domains, a sufficiently large labelled\ndataset may not always be available. In this study, we propose several\nevaluation metrics which can be applied on the embeddings of unlabelled data\nand investigate their viability by comparing them to linear probe accuracy (a\ncommon metric which utilizes an annotated dataset). In particular, we apply\n$k$-means clustering and measure the clustering quality with the silhouette\nscore and clustering agreement. We also measure the entropy of the embedding\ndistribution. We find that while the clusters did correspond better to the\nground truth annotations as training of the network progressed, label-free\nclustering metrics correlated with the linear probe accuracy only when training\nwith SSL methods SimCLR and MoCo-v2, but not with SimSiam. Additionally,\nalthough entropy did not always have strong correlations with LP accuracy, this\nappears to be due to instability arising from early training, with the metric\nstabilizing and becoming more reliable at later stages of learning.\nFurthermore, while entropy generally decreases as learning progresses, this\ntrend reverses for SimSiam. More research is required to establish the cause\nfor this unexpected behaviour. Lastly, we find that while clustering based\napproaches are likely only viable for same-architecture comparisons, entropy\nmay be architecture-independent.\n","authors":["Isaac Xu","Scott Lowe","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2409.06612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07761v3","updated":"2024-09-10T16:03:23Z","published":"2023-08-15T13:29:14Z","title":"NeFL: Nested Model Scaling for Federated Learning with System\n  Heterogeneous Clients","summary":"  Federated learning (FL) enables distributed training while preserving data\nprivacy, but stragglers-slow or incapable clients-can significantly slow down\nthe total training time and degrade performance. To mitigate the impact of\nstragglers, system heterogeneity, including heterogeneous computing and network\nbandwidth, has been addressed. While previous studies have addressed system\nheterogeneity by splitting models into submodels, they offer limited\nflexibility in model architecture design, without considering potential\ninconsistencies arising from training multiple submodel architectures. We\npropose nested federated learning (NeFL), a generalized framework that\nefficiently divides deep neural networks into submodels using both depthwise\nand widthwise scaling. To address the inconsistency arising from training\nmultiple submodel architectures, NeFL decouples a subset of parameters from\nthose being trained for each submodel. An averaging method is proposed to\nhandle these decoupled parameters during aggregation. NeFL enables\nresource-constrained devices to effectively participate in the FL pipeline,\nfacilitating larger datasets for model training. Experiments demonstrate that\nNeFL achieves performance gain, especially for the worst-case submodel compared\nto baseline approaches (7.63% improvement on CIFAR-100). Furthermore, NeFL\naligns with recent advances in FL, such as leveraging pre-trained models and\naccounting for statistical heterogeneity. Our code is available online.\n","authors":["Honggu Kang","Seohyeon Cha","Jinwoo Shin","Jongmyeong Lee","Joonhyuk Kang"],"pdf_url":"https://arxiv.org/pdf/2308.07761v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06609v1","updated":"2024-09-10T16:02:12Z","published":"2024-09-10T16:02:12Z","title":"Improving the Precision of CNNs for Magnetic Resonance Spectral Modeling","summary":"  Magnetic resonance spectroscopic imaging is a widely available imaging\nmodality that can non-invasively provide a metabolic profile of the tissue of\ninterest, yet is challenging to integrate clinically. One major reason is the\nexpensive, expert data processing and analysis that is required. Using machine\nlearning to predict MRS-related quantities offers avenues around this problem,\nbut deep learning models bring their own challenges, especially model trust.\nCurrent research trends focus primarily on mean error metrics, but\ncomprehensive precision metrics are also needed, e.g. standard deviations,\nconfidence intervals, etc.. This work highlights why more comprehensive error\ncharacterization is important and how to improve the precision of CNNs for\nspectral modeling, a quantitative task. The results highlight advantages and\ntrade-offs of these techniques that should be considered when addressing such\nregression tasks with CNNs. Detailed insights into the underlying mechanisms of\neach technique, and how they interact with other techniques, are discussed in\ndepth.\n","authors":["John LaMaster","Dhritiman Das","Florian Kofler","Jason Crane","Yan Li","Tobias Lasser","Bjoern H Menze"],"pdf_url":"https://arxiv.org/pdf/2409.06609v1.pdf","comment":"11 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2409.06605v1","updated":"2024-09-10T15:58:21Z","published":"2024-09-10T15:58:21Z","title":"Interactive 3D Segmentation for Primary Gross Tumor Volume in\n  Oropharyngeal Cancer","summary":"  The main treatment modality for oropharyngeal cancer (OPC) is radiotherapy,\nwhere accurate segmentation of the primary gross tumor volume (GTVp) is\nessential. However, accurate GTVp segmentation is challenging due to\nsignificant interobserver variability and the time-consuming nature of manual\nannotation, while fully automated methods can occasionally fail. An interactive\ndeep learning (DL) model offers the advantage of automatic high-performance\nsegmentation with the flexibility for user correction when necessary. In this\nstudy, we examine interactive DL for GTVp segmentation in OPC. We implement\nstate-of-the-art algorithms and propose a novel two-stage Interactive Click\nRefinement (2S-ICR) framework. Using the 2021 HEad and neCK TumOR (HECKTOR)\ndataset for development and an external dataset from The University of Texas MD\nAnderson Cancer Center for evaluation, the 2S-ICR framework achieves a Dice\nsimilarity coefficient of 0.713 $\\pm$ 0.152 without user interaction and 0.824\n$\\pm$ 0.099 after five interactions, outperforming existing methods in both\ncases.\n","authors":["Mikko Saukkoriipi","Jaakko Sahlsten","Joel Jaskari","Lotta Orasmaa","Jari Kangas","Nastaran Rasouli","Roope Raisamo","Jussi Hirvonen","Helena Mehtonen","Jorma Järnstedt","Antti Mäkitie","Mohamed Naser","Clifton Fuller","Benjamin Kann","Kimmo Kaski"],"pdf_url":"https://arxiv.org/pdf/2409.06605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08131v4","updated":"2024-09-10T15:51:39Z","published":"2023-07-16T19:04:48Z","title":"INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks","summary":"  Leveraging network information for predictive modeling has become widespread\nin many domains. Within the realm of referral and targeted marketing,\ninfluencer detection stands out as an area that could greatly benefit from the\nincorporation of dynamic network representation due to the continuous evolution\nof customer-brand relationships. In this paper, we present INFLECT-DGNN, a new\nmethod for profit-driven INFLuencer prEdiCTion with Dynamic Graph Neural\nNetworks that innovatively combines Graph Neural Networks (GNNs) and Recurrent\nNeural Networks (RNNs) with weighted loss functions, synthetic minority\noversampling adapted to graph data, and a carefully crafted rolling-window\nstrategy. We introduce a novel profit-driven framework that supports\ndecision-making based on model predictions. To test the framework, we use a\nunique corporate dataset with diverse networks, capturing the customer\ninteractions across three cities with different socioeconomic and demographic\ncharacteristics. Our results show how using RNNs to encode temporal attributes\nalongside GNNs significantly improves predictive performance, while the\nprofit-driven framework determines the optimal classification threshold for\nprofit maximization. We compare the results of different models to demonstrate\nthe importance of capturing network representation, temporal dependencies, and\nusing a profit-driven evaluation. Our research has significant implications for\nthe fields of referral and targeted marketing, expanding the technical use of\ndeep graph learning within corporate environments.\n","authors":["Elena Tiukhova","Emiliano Penaloza","María Óskarsdóttir","Bart Baesens","Monique Snoeck","Cristián Bravo"],"pdf_url":"https://arxiv.org/pdf/2307.08131v4.pdf","comment":"27 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.06601v1","updated":"2024-09-10T15:51:15Z","published":"2024-09-10T15:51:15Z","title":"Alleviating Hallucinations in Large Language Models with Scepticism\n  Modeling","summary":"  Hallucinations is a major challenge for large language models (LLMs),\nprevents adoption in diverse fields. Uncertainty estimation could be used for\nalleviating the damages of hallucinations. The skeptical emotion of human could\nbe useful for enhancing the ability of self estimation. Inspirited by this\nobservation, we proposed a new approach called Skepticism Modeling (SM). This\napproach is formalized by combining the information of token and logits for\nself estimation. We construct the doubt emotion aware data, perform continual\npre-training, and then fine-tune the LLMs, improve their ability of self\nestimation. Experimental results demonstrate this new approach effectively\nenhances a model's ability to estimate their uncertainty, and validate its\ngeneralization ability of other tasks by out-of-domain experiments.\n","authors":["Yetao Wu","Yihong Wang","Teng Chen","Chenxi Liu","Ningyuan Xi","Qingqing Gu","Hongyang Lei","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.06601v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.06593v1","updated":"2024-09-10T15:34:48Z","published":"2024-09-10T15:34:48Z","title":"Advancing Causal Inference: A Nonparametric Approach to ATE and CATE\n  Estimation with Continuous Treatments","summary":"  This paper introduces a generalized ps-BART model for the estimation of\nAverage Treatment Effect (ATE) and Conditional Average Treatment Effect (CATE)\nin continuous treatments, addressing limitations of the Bayesian Causal Forest\n(BCF) model. The ps-BART model's nonparametric nature allows for flexibility in\ncapturing nonlinear relationships between treatment and outcome variables.\nAcross three distinct sets of Data Generating Processes (DGPs), the ps-BART\nmodel consistently outperforms the BCF model, particularly in highly nonlinear\nsettings. The ps-BART model's robustness in uncertainty estimation and accuracy\nin both point-wise and probabilistic estimation demonstrate its utility for\nreal-world applications. This research fills a crucial gap in causal inference\nliterature, providing a tool better suited for nonlinear treatment-outcome\nrelationships and opening avenues for further exploration in the domain of\ncontinuous treatment effect estimation.\n","authors":["Hugo Gobato Souto","Francisco Louzada Neto"],"pdf_url":"https://arxiv.org/pdf/2409.06593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00515v4","updated":"2024-09-10T15:34:36Z","published":"2024-02-01T11:31:26Z","title":"Developing A Multi-Agent and Self-Adaptive Framework with Deep\n  Reinforcement Learning for Dynamic Portfolio Risk Management","summary":"  Deep or reinforcement learning (RL) approaches have been adapted as reactive\nagents to quickly learn and respond with new investment strategies for\nportfolio management under the highly turbulent financial market environments\nin recent years. In many cases, due to the very complex correlations among\nvarious financial sectors, and the fluctuating trends in different financial\nmarkets, a deep or reinforcement learning based agent can be biased in\nmaximising the total returns of the newly formulated investment portfolio while\nneglecting its potential risks under the turmoil of various market conditions\nin the global or regional sectors. Accordingly, a multi-agent and self-adaptive\nframework namely the MASA is proposed in which a sophisticated multi-agent\nreinforcement learning (RL) approach is adopted through two cooperating and\nreactive agents to carefully and dynamically balance the trade-off between the\noverall portfolio returns and their potential risks. Besides, a very flexible\nand proactive agent as the market observer is integrated into the MASA\nframework to provide some additional information on the estimated market trends\nas valuable feedbacks for multi-agent RL approach to quickly adapt to the\never-changing market conditions. The obtained empirical results clearly reveal\nthe potential strengths of our proposed MASA framework based on the multi-agent\nRL approach against many well-known RL-based approaches on the challenging data\nsets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the\npast 10 years. More importantly, our proposed MASA framework shed lights on\nmany possible directions for future investigation.\n","authors":["Zhenglong Li","Vincent Tam","Kwan L. Yeung"],"pdf_url":"https://arxiv.org/pdf/2402.00515v4.pdf","comment":"In Proceedings of the 23rd International Conference on Autonomous\n  Agents and Multiagent Systems"},{"id":"http://arxiv.org/abs/2409.06585v1","updated":"2024-09-10T15:26:58Z","published":"2024-09-10T15:26:58Z","title":"Developing the Temporal Graph Convolutional Neural Network Model to\n  Predict Hip Replacement using Electronic Health Records","summary":"  Background: Hip replacement procedures improve patient lives by relieving\npain and restoring mobility. Predicting hip replacement in advance could reduce\npain by enabling timely interventions, prioritising individuals for surgery or\nrehabilitation, and utilising physiotherapy to potentially delay the need for\njoint replacement. This study predicts hip replacement a year in advance to\nenhance quality of life and health service efficiency. Methods: Adapting\nprevious work using Temporal Graph Convolutional Neural Network (TG-CNN)\nmodels, we construct temporal graphs from primary care medical event codes,\nsourced from ResearchOne EHRs of 40-75-year-old patients, to predict hip\nreplacement risk. We match hip replacement cases to controls by age, sex, and\nIndex of Multiple Deprivation. The model, trained on 9,187 cases and 9,187\ncontrols, predicts hip replacement one year in advance. We validate the model\non two unseen datasets, recalibrating for class imbalance. Additionally, we\nconduct an ablation study and compare against four baseline models. Results:\nOur best model predicts hip replacement risk one year in advance with an AUROC\nof 0.724 (95% CI: 0.715-0.733) and an AUPRC of 0.185 (95% CI: 0.160-0.209),\nachieving a calibration slope of 1.107 (95% CI: 1.074-1.139) after\nrecalibration. Conclusions: The TG-CNN model effectively predicts hip\nreplacement risk by identifying patterns in patient trajectories, potentially\nimproving understanding and management of hip-related conditions.\n","authors":["Zoe Hancox","Sarah R. Kingsbury","Andrew Clegg","Philip G. Conaghan","Samuel D. Relton"],"pdf_url":"https://arxiv.org/pdf/2409.06585v1.pdf","comment":"Accepted to the 2024 International Conference on Machine Learning and\n  Applications (ICMLA). 8 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.04170v3","updated":"2024-09-10T15:04:17Z","published":"2024-06-06T15:27:52Z","title":"Deeper-PINNs: Element-wise Multiplication Based Physics-informed Neural\n  Networks","summary":"  As a promising framework for resolving partial differential equations (PDEs),\nphysics-informed neural networks (PINNs) have received widespread attention\nfrom industrial and scientific fields. However, lack of expressive ability and\ninitialization pathology issues are found to prevent the application of PINNs\nin complex PDEs. In this work, we propose Deeper Physics-Informed Neural\nNetwork (Deeper-PINN) to resolve these issues. The element-wise multiplication\noperation is adopted to transform features into high-dimensional, non-linear\nspaces. Benefiting from element-wise multiplication operation, Deeper-PINNs can\nalleviate the initialization pathologies of PINNs and enhance the expressive\ncapability of PINNs. The proposed structure is verified on various benchmarks.\nThe results show that Deeper-PINNs can effectively resolve the initialization\npathology and exhibit strong expressive ability.\n","authors":["Feilong Jiang","Xiaonan Hou","Min Xia"],"pdf_url":"https://arxiv.org/pdf/2406.04170v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06560v1","updated":"2024-09-10T14:43:03Z","published":"2024-09-10T14:43:03Z","title":"A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling","summary":"  Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. Deriving the central learning objective for VI must often be tailored\nto new learning tasks where the nature of the problems dictates the conditional\ndependence between variables of interest, such as arising in physics problems.\nIn this paper, we provide an accessible and thorough technical introduction to\nVI for forward and inverse problems, guiding the reader through standard\nderivations of the VI framework and how it can best be realized through deep\nlearning. We then review and unify recent literature exemplifying the creative\nflexibility allowed by VI. This paper is designed for a general scientific\naudience looking to solve physics-based problems with an emphasis on\nuncertainty quantification.\n","authors":["Alex Glyn-Davies","Arnaud Vadeboncoeur","O. Deniz Akyildiz","Ieva Kazlauskaite","Mark Girolami"],"pdf_url":"https://arxiv.org/pdf/2409.06560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06559v1","updated":"2024-09-10T14:41:46Z","published":"2024-09-10T14:41:46Z","title":"Learn2Aggregate: Supervised Generation of Chvátal-Gomory Cuts Using\n  Graph Neural Networks","summary":"  We present $\\textit{Learn2Aggregate}$, a machine learning (ML) framework for\noptimizing the generation of Chv\\'atal-Gomory (CG) cuts in mixed integer linear\nprogramming (MILP). The framework trains a graph neural network to classify\nuseful constraints for aggregation in CG cut generation. The ML-driven CG\nseparator selectively focuses on a small set of impactful constraints,\nimproving runtimes without compromising the strength of the generated cuts. Key\nto our approach is the formulation of a constraint classification task which\nfavours sparse aggregation of constraints, consistent with empirical findings.\nThis, in conjunction with a careful constraint labeling scheme and a hybrid of\ndeep learning and feature engineering, results in enhanced CG cut generation\nacross five diverse MILP benchmarks. On the largest test sets, our method\ncloses roughly $\\textit{twice}$ as much of the integrality gap as the standard\nCG method while running 40$% faster. This performance improvement is due to our\nmethod eliminating 75% of the constraints prior to aggregation.\n","authors":["Arnaud Deza","Elias B. Khalil","Zhenan Fan","Zirui Zhou","Yong Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06559v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2405.12372v2","updated":"2024-09-10T14:38:30Z","published":"2024-05-20T20:56:01Z","title":"DispaRisk: Auditing Fairness Through Usable Information","summary":"  Machine Learning algorithms (ML) impact virtually every aspect of human lives\nand have found use across diverse sectors including healthcare, finance, and\neducation. Often, ML algorithms have been found to exacerbate societal biases\npresent in datasets leading to adversarial impacts on subsets/groups of\nindividuals and in many cases on minority groups. To effectively mitigate these\nuntoward effects, it is crucial that disparities/biases are identified early in\na ML pipeline. This proactive approach facilitates timely interventions to\nprevent bias amplification and reduce complexity at later stages of model\ndevelopment. In this paper, we leverage recent advancements in usable\ninformation theory to introduce DispaRisk, a novel framework designed to\nproactively assess the potential risks of disparities in datasets during the\ninitial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by\nbenchmarking it against commonly used datasets in fairness research. Our\nfindings demonstrate DispaRisk's capabilities to identify datasets with a high\nrisk of discrimination, detect model families prone to biases within an ML\npipeline, and enhance the explainability of these bias risks. This work\ncontributes to the development of fairer ML systems by providing a robust tool\nfor early bias detection and mitigation. The code for our experiments is\navailable in the following repository: https://github.com/jovasque156/disparisk\n","authors":["Jonathan Vasquez","Carlotta Domeniconi","Huzefa Rangwala"],"pdf_url":"https://arxiv.org/pdf/2405.12372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04614v3","updated":"2024-09-10T14:37:00Z","published":"2024-05-07T18:58:32Z","title":"Multi-Margin Cosine Loss: Proposal and Application in Recommender\n  Systems","summary":"  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nstraightforward nature, relying only on user-item interactions. Typically,\nthese systems consist of three main components: an interaction module, a loss\nfunction, and a negative sampling strategy. Initially, researchers focused on\nenhancing performance by developing complex interaction modules. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has led to an increased interest in contrastive\nlearning, which pulls similar pairs closer while pushing dissimilar ones apart.\nContrastive learning may bring challenges like high memory demands and\nunder-utilization of some negative samples. The proposed Multi-Margin Cosine\nLoss (MMCL) addresses these challenges by introducing multiple margins and\nvarying weights for negative samples. It efficiently utilizes not only the\nhardest negatives but also other non-trivial negatives, offers a simpler yet\neffective loss function that outperforms more complex methods, especially when\nresources are limited. Experiments on two well-known datasets demonstrated that\nMMCL achieved up to a 20\\% performance improvement compared to a baseline loss\nfunction when fewer number of negative samples are used.\n","authors":["Makbule Gulcin Ozsoy"],"pdf_url":"https://arxiv.org/pdf/2405.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05030v2","updated":"2024-09-10T14:32:23Z","published":"2024-09-08T08:48:42Z","title":"Some Results on Neural Network Stability, Consistency, and Convergence:\n  Insights into Non-IID Data, High-Dimensional Settings, and Physics-Informed\n  Neural Networks","summary":"  This paper addresses critical challenges in machine learning, particularly\nthe stability, consistency, and convergence of neural networks under non-IID\ndata, distribution shifts, and high-dimensional settings. We provide new\ntheoretical results on uniform stability for neural networks with dynamic\nlearning rates in non-convex settings. Further, we establish consistency bounds\nfor federated learning models in non-Euclidean spaces, accounting for\ndistribution shifts and curvature effects. For Physics-Informed Neural Networks\n(PINNs), we derive stability, consistency, and convergence guarantees for\nsolving Partial Differential Equations (PDEs) in noisy environments. These\nresults fill significant gaps in understanding model behavior in complex,\nnon-ideal conditions, paving the way for more robust and reliable machine\nlearning applications.\n","authors":["Ronald Katende","Henry Kasumba","Godwin Kakuba","John M. Mango"],"pdf_url":"https://arxiv.org/pdf/2409.05030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06555v1","updated":"2024-09-10T14:31:21Z","published":"2024-09-10T14:31:21Z","title":"Deep Neural Networks: Multi-Classification and Universal Approximation","summary":"  We demonstrate that a ReLU deep neural network with a width of $2$ and a\ndepth of $2N+4M-1$ layers can achieve finite sample memorization for any\ndataset comprising $N$ elements in $\\mathbb{R}^d$, where $d\\ge1,$ and $M$\nclasses, thereby ensuring accurate classification.\n  By modeling the neural network as a time-discrete nonlinear dynamical system,\nwe interpret the memorization property as a problem of simultaneous or ensemble\ncontrollability. This problem is addressed by constructing the network\nparameters inductively and explicitly, bypassing the need for training or\nsolving any optimization problem.\n  Additionally, we establish that such a network can achieve universal\napproximation in $L^p(\\Omega;\\mathbb{R}_+)$, where $\\Omega$ is a bounded subset\nof $\\mathbb{R}^d$ and $p\\in[1,\\infty)$, using a ReLU deep neural network with a\nwidth of $d+1$. We also provide depth estimates for approximating $W^{1,p}$\nfunctions and width estimates for approximating $L^p(\\Omega;\\mathbb{R}^m)$ for\n$m\\geq1$. Our proofs are constructive, offering explicit values for the biases\nand weights involved.\n","authors":["Martín Hernández","Enrique Zuazua"],"pdf_url":"https://arxiv.org/pdf/2409.06555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06554v1","updated":"2024-09-10T14:31:03Z","published":"2024-09-10T14:31:03Z","title":"Modelling Global Trade with Optimal Transport","summary":"  Global trade is shaped by a complex mix of factors beyond supply and demand,\nincluding tangible variables like transport costs and tariffs, as well as less\nquantifiable influences such as political and economic relations.\nTraditionally, economists model trade using gravity models, which rely on\nexplicit covariates but often struggle to capture these subtler drivers of\ntrade. In this work, we employ optimal transport and a deep neural network to\nlearn a time-dependent cost function from data, without imposing a specific\nfunctional form. This approach consistently outperforms traditional gravity\nmodels in accuracy while providing natural uncertainty quantification. Applying\nour framework to global food and agricultural trade, we show that the global\nSouth suffered disproportionately from the war in Ukraine's impact on wheat\nmarkets. We also analyze the effects of free-trade agreements and trade\ndisputes with China, as well as Brexit's impact on British trade with Europe,\nuncovering hidden patterns that trade volumes alone cannot reveal.\n","authors":["Thomas Gaskin","Marie-Therese Wolfram","Andrew Duncan","Guven Demirel"],"pdf_url":"https://arxiv.org/pdf/2409.06554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06542v1","updated":"2024-09-10T14:15:56Z","published":"2024-09-10T14:15:56Z","title":"Dynamic Decoupling of Placid Terminal Attractor-based Gradient Descent\n  Algorithm","summary":"  Gradient descent (GD) and stochastic gradient descent (SGD) have been widely\nused in a large number of application domains. Therefore, understanding the\ndynamics of GD and improving its convergence speed is still of great\nimportance. This paper carefully analyzes the dynamics of GD based on the\nterminal attractor at different stages of its gradient flow. On the basis of\nthe terminal sliding mode theory and the terminal attractor theory, four\nadaptive learning rates are designed. Their performances are investigated in\nlight of a detailed theoretical investigation, and the running times of the\nlearning procedures are evaluated and compared. The total times of their\nlearning processes are also studied in detail. To evaluate their effectiveness,\nvarious simulation results are investigated on a function approximation problem\nand an image classification problem.\n","authors":["Jinwei Zhao","Marco Gori","Alessandro Betti","Stefano Melacci","Hongtao Zhang","Jiedong Liu","Xinhong Hei"],"pdf_url":"https://arxiv.org/pdf/2409.06542v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.06530v1","updated":"2024-09-10T14:05:12Z","published":"2024-09-10T14:05:12Z","title":"Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems","summary":"  This paper studies simple bilevel problems, where a convex upper-level\nfunction is minimized over the optimal solutions of a convex lower-level\nproblem. We first show the fundamental difficulty of simple bilevel problems,\nthat the approximate optimal value of such problems is not obtainable by\nfirst-order zero-respecting algorithms. Then we follow recent works to pursue\nthe weak approximate solutions. For this goal, we propose novel near-optimal\nmethods for smooth and nonsmooth problems by reformulating them into\nfunctionally constrained problems.\n","authors":["Huaqing Zhang","Lesi Chen","Jing Xu","Jingzhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06525v1","updated":"2024-09-10T14:02:34Z","published":"2024-09-10T14:02:34Z","title":"MENSA: A Multi-Event Network for Survival Analysis under Informative\n  Censoring","summary":"  Given an instance, a multi-event survival model predicts the time until that\ninstance experiences each of several different events. These events are not\nmutually exclusive and there are often statistical dependencies between them.\nThere are relatively few multi-event survival results, most focusing on\nproducing a simple risk score, rather than the time-to-event itself. To\novercome these issues, we introduce MENSA, a novel, deep learning approach for\nmulti-event survival analysis that can jointly learn representations of the\ninput covariates and the dependence structure between events. As a practical\nmotivation for multi-event survival analysis, we consider the problem of\npredicting the time until a patient with amyotrophic lateral sclerosis (ALS)\nloses various physical functions, i.e., the ability to speak, swallow, write,\nor walk. When estimating when a patient is no longer able to swallow, our\napproach achieves an L1-Margin loss of 278.8 days, compared to 355.2 days when\nmodeling each event separately. In addition, we also evaluate our approach in\nsingle-event and competing risk scenarios by modeling the censoring and event\ndistributions as equal contributing factors in the optimization process, and\nshow that our approach performs well across multiple benchmark datasets. The\nsource code is available at: https://github.com/thecml/mensa\n","authors":["Christian Marius Lillelund","Ali Hossein Gharari Foomani","Weijie Sun","Shi-ang Qi","Russell Greiner"],"pdf_url":"https://arxiv.org/pdf/2409.06525v1.pdf","comment":"Submitted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.06522v1","updated":"2024-09-10T13:56:54Z","published":"2024-09-10T13:56:54Z","title":"Deep Learning for Koopman Operator Estimation in Idealized Atmospheric\n  Dynamics","summary":"  Deep learning is revolutionizing weather forecasting, with new data-driven\nmodels achieving accuracy on par with operational physical models for\nmedium-term predictions. However, these models often lack interpretability,\nmaking their underlying dynamics difficult to understand and explain. This\npaper proposes methodologies to estimate the Koopman operator, providing a\nlinear representation of complex nonlinear dynamics to enhance the transparency\nof data-driven models. Despite its potential, applying the Koopman operator to\nlarge-scale problems, such as atmospheric modeling, remains challenging. This\nstudy aims to identify the limitations of existing methods, refine these models\nto overcome various bottlenecks, and introduce novel convolutional neural\nnetwork architectures that capture simplified dynamics.\n","authors":["David Millard","Arielle Carr","Stéphane Gaudreault"],"pdf_url":"https://arxiv.org/pdf/2409.06522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06514v1","updated":"2024-09-10T13:50:53Z","published":"2024-09-10T13:50:53Z","title":"Limit Order Book Simulation and Trade Evaluation with\n  $K$-Nearest-Neighbor Resampling","summary":"  In this paper, we show how $K$-nearest neighbor ($K$-NN) resampling, an\noff-policy evaluation method proposed in \\cite{giegrich2023k}, can be applied\nto simulate limit order book (LOB) markets and how it can be used to evaluate\nand calibrate trading strategies. Using historical LOB data, we demonstrate\nthat our simulation method is capable of recreating realistic LOB dynamics and\nthat synthetic trading within the simulation leads to a market impact in line\nwith the corresponding literature. Compared to other statistical LOB simulation\nmethods, our algorithm has theoretical convergence guarantees under general\nconditions, does not require optimization, is easy to implement and\ncomputationally efficient. Furthermore, we show that in a benchmark comparison\nour method outperforms a deep learning-based algorithm for several key\nstatistics. In the context of a LOB with pro-rata type matching, we demonstrate\nhow our algorithm can calibrate the size of limit orders for a liquidation\nstrategy. Finally, we describe how $K$-NN resampling can be modified for\nchoices of higher dimensional state spaces.\n","authors":["Michael Giegrich","Roel Oomen","Christoph Reisinger"],"pdf_url":"https://arxiv.org/pdf/2409.06514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06509v1","updated":"2024-09-10T13:41:08Z","published":"2024-09-10T13:41:08Z","title":"Aligning Machine and Human Visual Representations across Abstraction\n  Levels","summary":"  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n","authors":["Lukas Muttenthaler","Klaus Greff","Frieda Born","Bernhard Spitzer","Simon Kornblith","Michael C. Mozer","Klaus-Robert Müller","Thomas Unterthiner","Andrew K. Lampinen"],"pdf_url":"https://arxiv.org/pdf/2409.06509v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2409.02363v2","updated":"2024-09-10T13:40:15Z","published":"2024-09-04T01:18:55Z","title":"Optimal Neural Network Approximation for High-Dimensional Continuous\n  Functions","summary":"  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural\nnetwork with width $36d(2d + 1)$ and depth $11$, which utilizes a special\nactivation function called the elementary universal activation function, to\nachieve the super approximation property for functions in $C([a,b]^d)$. That\nis, the constructed network only requires a fixed number of neurons to\napproximate a $d$-variate continuous function on a $d$-dimensional hypercube\nwith arbitrary accuracy. Their network uses $\\mathcal{O}(d^2)$ fixed neurons.\nOne natural question to address is whether we can reduce the number of these\nneurons in such a network. By leveraging a variant of the Kolmogorov\nSuperposition Theorem, our analysis shows that there is a neural network\ngenerated by the elementary universal activation function with only $366d +365$\nfixed, intrinsic (non-repeated) neurons that attains this super approximation\nproperty. Furthermore, we present a family of continuous functions that\nrequires at least width $d$, and therefore at least $d$ intrinsic neurons, to\nachieve arbitrary accuracy in its approximation. This shows that the\nrequirement of $\\mathcal{O}(d)$ intrinsic neurons is optimal in the sense that\nit grows linearly with the input dimension $d$, unlike some approximation\nmethods where parameters may grow exponentially with $d$.\n","authors":["Ayan Maiti","Michelle Michelle","Haizhao Yang"],"pdf_url":"https://arxiv.org/pdf/2409.02363v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06498v1","updated":"2024-09-10T13:26:37Z","published":"2024-09-10T13:26:37Z","title":"Learning local and semi-local density functionals from exact\n  exchange-correlation potentials and energies","summary":"  Finding accurate exchange-correlation (XC) functionals remains the defining\nchallenge in density functional theory (DFT). Despite 40 years of active\ndevelopment, the desired chemical accuracy is still elusive with existing\nfunctionals. We present a data-driven pathway to learn the XC functionals by\nutilizing the exact density, XC energy, and XC potential. While the exact\ndensities are obtained from accurate configuration interaction (CI), the exact\nXC energies and XC potentials are obtained via inverse DFT calculations on the\nCI densities. We demonstrate how simple neural network (NN) based local density\napproximation (LDA) and generalized gradient approximation (GGA), trained on\njust five atoms and two molecules, provide remarkable improvement in total\nenergies, densities, atomization energies, and barrier heights for hundreds of\nmolecules outside the training set. Particularly, the NN-based GGA functional\nattains similar accuracy as the higher rung SCAN meta-GGA, highlighting the\npromise of using the XC potential in modeling XC functionals. We expect this\napproach to pave the way for systematic learning of increasingly accurate and\nsophisticated XC functionals.\n","authors":["Bikash Kanungo","Jeffrey Hatch","Paul M. Zimmerman","Vikram Gavini"],"pdf_url":"https://arxiv.org/pdf/2409.06498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03767v2","updated":"2024-09-10T13:23:57Z","published":"2024-08-21T02:15:26Z","title":"EMCNet : Graph-Nets for Electron Micrographs Classification","summary":"  Characterization of materials via electron micrographs is an important and\nchallenging task in several materials processing industries. Classification of\nelectron micrographs is complex due to the high intra-class dissimilarity, high\ninter-class similarity, and multi-spatial scales of patterns. However, existing\nmethods are ineffective in learning complex image patterns. We propose an\neffective end-to-end electron micrograph representation learning-based\nframework for nanomaterial identification to overcome the challenges. We\ndemonstrate that our framework outperforms the popular baselines on the\nopen-source datasets in nanomaterials-based identification tasks. The ablation\nstudies are reported in great detail to support the efficacy of our approach.\n","authors":["Sakhinana Sagar Srinivas","Rajat Kumar Sarkar","Venkataramana Runkana"],"pdf_url":"https://arxiv.org/pdf/2409.03767v2.pdf","comment":"12 pages, 10 figures, Accepted in a ACM SIGKDD 2022 Workshop on\n  Machine Learning for Materials"},{"id":"http://arxiv.org/abs/2409.06477v1","updated":"2024-09-10T13:05:45Z","published":"2024-09-10T13:05:45Z","title":"Superior Computer Chess with Model Predictive Control, Reinforcement\n  Learning, and Rollout","summary":"  In this paper we apply model predictive control (MPC), rollout, and\nreinforcement learning (RL) methodologies to computer chess. We introduce a new\narchitecture for move selection, within which available chess engines are used\nas components. One engine is used to provide position evaluations in an\napproximation in value space MPC/RL scheme, while a second engine is used as\nnominal opponent, to emulate or approximate the moves of the true opponent\nplayer.\n  We show that our architecture improves substantially the performance of the\nposition evaluation engine. In other words our architecture provides an\nadditional layer of intelligence, on top of the intelligence of the engines on\nwhich it is based. This is true for any engine, regardless of its strength: top\nengines such as Stockfish and Komodo Dragon (of varying strengths), as well as\nweaker engines.\n  Structurally, our basic architecture selects moves by a one-move lookahead\nsearch, with an intermediate move generated by a nominal opponent engine, and\nfollowed by a position evaluation by another chess engine. Simpler schemes that\nforego the use of the nominal opponent, also perform better than the position\nevaluator, but not quite by as much. More complex schemes, involving multistep\nlookahead, may also be used and generally tend to perform better as the length\nof the lookahead increases.\n  Theoretically, our methodology relies on generic cost improvement properties\nand the superlinear convergence framework of Newton's method, which\nfundamentally underlies approximation in value space, and related MPC/RL and\nrollout/policy iteration schemes. A critical requirement of this framework is\nthat the first lookahead step should be executed exactly. This fact has guided\nour architectural choices, and is apparently an important factor in improving\nthe performance of even the best available chess engines.\n","authors":["Atharva Gundawar","Yuchao Li","Dimitri Bertsekas"],"pdf_url":"https://arxiv.org/pdf/2409.06477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06315v3","updated":"2024-09-10T12:59:47Z","published":"2024-07-08T18:31:19Z","title":"Shedding More Light on Robust Classifiers under the lens of Energy-based\n  Models","summary":"  By reinterpreting a robust discriminative classifier as Energy-based Model\n(EBM), we offer a new take on the dynamics of adversarial training (AT). Our\nanalysis of the energy landscape during AT reveals that untargeted attacks\ngenerate adversarial images much more in-distribution (lower energy) than the\noriginal data from the point of view of the model. Conversely, we observe the\nopposite for targeted attacks. On the ground of our thorough analysis, we\npresent new theoretical and practical results that show how interpreting AT\nenergy dynamics unlocks a better understanding: (1) AT dynamic is governed by\nthree phases and robust overfitting occurs in the third phase with a drastic\ndivergence between natural and adversarial energies (2) by rewriting the loss\nof TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization\n(TRADES) in terms of energies, we show that TRADES implicitly alleviates\noverfitting by means of aligning the natural energy with the adversarial one\n(3) we empirically show that all recent state-of-the-art robust classifiers are\nsmoothing the energy landscape and we reconcile a variety of studies about\nunderstanding AT and weighting the loss function under the umbrella of EBMs.\nMotivated by rigorous evidence, we propose Weighted Energy Adversarial Training\n(WEAT), a novel sample weighting scheme that yields robust accuracy matching\nthe state-of-the-art on multiple benchmarks such as CIFAR-10 and SVHN and going\nbeyond in CIFAR-100 and Tiny-ImageNet. We further show that robust classifiers\nvary in the intensity and quality of their generative capabilities, and offer a\nsimple method to push this capability, reaching a remarkable Inception Score\n(IS) and FID using a robust classifier without training for generative\nmodeling. The code to reproduce our results is available at\nhttp://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ .\n","authors":["Mujtaba Hussain Mirza","Maria Rosaria Briglia","Senad Beadini","Iacopo Masi"],"pdf_url":"https://arxiv.org/pdf/2407.06315v3.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2409.06466v1","updated":"2024-09-10T12:50:46Z","published":"2024-09-10T12:50:46Z","title":"A Machine Learning Based Approach for Statistical Analysis of Detonation\n  Cells from Soot Foils","summary":"  This study presents a novel algorithm based on machine learning (ML) for the\nprecise segmentation and measurement of detonation cells from soot foil images,\naddressing the limitations of manual and primitive edge detection methods\nprevalent in the field. Using advances in cellular biology segmentation models,\nthe proposed algorithm is designed to accurately extract cellular patterns\nwithout a training procedure or dataset, which is a significant challenge in\ndetonation research. The algorithm's performance was validated using a series\nof test cases that mimic experimental and numerical detonation studies. The\nresults demonstrated consistent accuracy, with errors remaining within 10%,\neven in complex cases. The algorithm effectively captured key cell metrics such\nas cell area and span, revealing trends across different soot foil samples with\nuniform to highly irregular cellular structures. Although the model proved\nrobust, challenges remain in segmenting and analyzing highly complex or\nirregular cellular patterns. This work highlights the broad applicability and\npotential of the algorithm to advance the understanding of detonation wave\ndynamics.\n","authors":["Vansh Sharma","Michael Ullman","Venkat Raman"],"pdf_url":"https://arxiv.org/pdf/2409.06466v1.pdf","comment":"23 pages, 12 figures, submitted to Comb. and Flame"},{"id":"http://arxiv.org/abs/2409.06452v1","updated":"2024-09-10T12:17:23Z","published":"2024-09-10T12:17:23Z","title":"Ransomware Detection Using Machine Learning in the Linux Kernel","summary":"  Linux-based cloud environments have become lucrative targets for ransomware\nattacks, employing various encryption schemes at unprecedented speeds.\nAddressing the urgency for real-time ransomware protection, we propose\nleveraging the extended Berkeley Packet Filter (eBPF) to collect system call\ninformation regarding active processes and infer about the data directly at the\nkernel level. In this study, we implement two Machine Learning (ML) models in\neBPF - a decision tree and a multilayer perceptron. Benchmarking latency and\naccuracy against their user space counterparts, our findings underscore the\nefficacy of this approach.\n","authors":["Adrian Brodzik","Tomasz Malec-Kruszyński","Wojciech Niewolski","Mikołaj Tkaczyk","Krzysztof Bocianiak","Sok-Yen Loui"],"pdf_url":"https://arxiv.org/pdf/2409.06452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06446v1","updated":"2024-09-10T12:01:43Z","published":"2024-09-10T12:01:43Z","title":"HexaCoder: Secure Code Generation via Oracle-Guided Synthetic Training\n  Data","summary":"  Large language models (LLMs) have shown great potential for automatic code\ngeneration and form the basis for various tools such as GitHub Copilot.\nHowever, recent studies highlight that many LLM-generated code contains serious\nsecurity vulnerabilities. While previous work tries to address this by training\nmodels that generate secure code, these attempts remain constrained by limited\naccess to training data and labor-intensive data preparation.\n  In this paper, we introduce HexaCoder, a novel approach to enhance the\nability of LLMs to generate secure codes by automatically synthesizing secure\ncodes, which reduces the effort of finding suitable training data. HexaCoder\ncomprises two key components: an oracle-guided data synthesis pipeline and a\ntwo-step process for secure code generation. The data synthesis pipeline\ngenerates pairs of vulnerable and fixed codes for specific Common Weakness\nEnumeration (CWE) types by utilizing a state-of-the-art LLM for repairing\nvulnerable code. A security oracle identifies vulnerabilities, and a\nstate-of-the-art LLM repairs them by extending and/or editing the codes,\ncreating data pairs for fine-tuning using the Low-Rank Adaptation (LoRA)\nmethod. Each example of our fine-tuning dataset includes the necessary\nsecurity-related libraries and code that form the basis of our novel two-step\ngeneration approach. This allows the model to integrate security-relevant\nlibraries before generating the main code, significantly reducing the number of\ngenerated vulnerable codes by up to 85% compared to the baseline methods. We\nperform extensive evaluations on three different benchmarks for four LLMs,\ndemonstrating that HexaCoder not only improves the security of the generated\ncode but also maintains a high level of functional correctness.\n","authors":["Hossein Hajipour","Lea Schönherr","Thorsten Holz","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2409.06446v1.pdf","comment":"24 pages, 16 tables, 8 figures"},{"id":"http://arxiv.org/abs/2409.06439v1","updated":"2024-09-10T11:42:55Z","published":"2024-09-10T11:42:55Z","title":"Extending Explainable Ensemble Trees (E2Tree) to regression contexts","summary":"  Ensemble methods such as random forests have transformed the landscape of\nsupervised learning, offering highly accurate prediction through the\naggregation of multiple weak learners. However, despite their effectiveness,\nthese methods often lack transparency, impeding users' comprehension of how RF\nmodels arrive at their predictions. Explainable ensemble trees (E2Tree) is a\nnovel methodology for explaining random forests, that provides a graphical\nrepresentation of the relationship between response variables and predictors. A\nstriking characteristic of E2Tree is that it not only accounts for the effects\nof predictor variables on the response but also accounts for associations\nbetween the predictor variables through the computation and use of\ndissimilarity measures. The E2Tree methodology was initially proposed for use\nin classification tasks. In this paper, we extend the methodology to encompass\nregression contexts. To demonstrate the explanatory power of the proposed\nalgorithm, we illustrate its use on real-world datasets.\n","authors":["Massimo Aria","Agostino Gnasso","Carmela Iorio","Marjolein Fokkema"],"pdf_url":"https://arxiv.org/pdf/2409.06439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06437v1","updated":"2024-09-10T11:42:22Z","published":"2024-09-10T11:42:22Z","title":"A Short Information-Theoretic Analysis of Linear Auto-Regressive\n  Learning","summary":"  In this note, we give a short information-theoretic proof of the consistency\nof the Gaussian maximum likelihood estimator in linear auto-regressive models.\nOur proof yields nearly optimal non-asymptotic rates for parameter recovery and\nworks without any invocation of stability in the case of finite hypothesis\nclasses.\n","authors":["Ingvar Ziemann"],"pdf_url":"https://arxiv.org/pdf/2409.06437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06433v1","updated":"2024-09-10T11:31:02Z","published":"2024-09-10T11:31:02Z","title":"Fine-tuning and Prompt Engineering with Cognitive Knowledge Graphs for\n  Scholarly Knowledge Organization","summary":"  The increasing amount of published scholarly articles, exceeding 2.5 million\nyearly, raises the challenge for researchers in following scientific progress.\nIntegrating the contributions from scholarly articles into a novel type of\ncognitive knowledge graph (CKG) will be a crucial element for accessing and\norganizing scholarly knowledge, surpassing the insights provided by titles and\nabstracts. This research focuses on effectively conveying structured scholarly\nknowledge by utilizing large language models (LLMs) to categorize scholarly\narticles and describe their contributions in a structured and comparable\nmanner. While previous studies explored language models within specific\nresearch domains, the extensive domain-independent knowledge captured by LLMs\noffers a substantial opportunity for generating structured contribution\ndescriptions as CKGs. Additionally, LLMs offer customizable pathways through\nprompt engineering or fine-tuning, thus facilitating to leveraging of smaller\nLLMs known for their efficiency, cost-effectiveness, and environmental\nconsiderations. Our methodology involves harnessing LLM knowledge, and\ncomplementing it with domain expert-verified scholarly data sourced from a CKG.\nThis strategic fusion significantly enhances LLM performance, especially in\ntasks like scholarly article categorization and predicate recommendation. Our\nmethod involves fine-tuning LLMs with CKG knowledge and additionally injecting\nknowledge from a CKG with a novel prompting technique significantly increasing\nthe accuracy of scholarly knowledge extraction. We integrated our approach in\nthe Open Research Knowledge Graph (ORKG), thus enabling precise access to\norganized scholarly knowledge, crucially benefiting domain-independent\nscholarly knowledge exchange and dissemination among policymakers, industrial\npractitioners, and the general public.\n","authors":["Gollam Rabby","Sören Auer","Jennifer D'Souza","Allard Oelen"],"pdf_url":"https://arxiv.org/pdf/2409.06433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06428v1","updated":"2024-09-10T11:20:36Z","published":"2024-09-10T11:20:36Z","title":"Spectral Map for Slow Collective Variables, Markovian Dynamics, and\n  Transition State Ensembles","summary":"  Understanding the behavior of complex molecular systems is a fundamental\nproblem in physical chemistry. To describe the long-time dynamics of such\nsystems, which is responsible for their most informative characteristics, we\ncan identify a few slow collective variables (CVs) while treating the remaining\nfast variables as thermal noise. This enables us to simplify the dynamics and\ntreat it as diffusion in a free-energy landscape spanned by slow CVs,\neffectively rendering the dynamics Markovian. Our recent statistical learning\ntechnique, spectral map [Rydzewski, J. Phys. Chem. Lett. 2023, 14, 22,\n5216-5220], explores this strategy to learn slow CVs by maximizing a spectral\ngap of a transition matrix. In this work, we introduce several advancements\ninto our framework, using a high-dimensional reversible folding process of a\nprotein as an example. We implement an algorithm for coarse-graining Markov\ntransition matrices to partition the reduced space of slow CVs kinetically and\nuse it to define a transition state ensemble. We show that slow CVs learned by\nspectral map closely approach the Markovian limit for an overdamped diffusion.\nWe demonstrate that coordinate-dependent diffusion coefficients only slightly\naffect the constructed free-energy landscapes. Finally, we present how spectral\nmap can be used to quantify the importance of features and compare slow CVs\nwith structural descriptors commonly used in protein folding. Overall, we\ndemonstrate that a single slow CV learned by spectral map can be used as a\nphysical reaction coordinate to capture essential characteristics of protein\nfolding.\n","authors":["Jakub Rydzewski"],"pdf_url":"https://arxiv.org/pdf/2409.06428v1.pdf","comment":"Accepted as part of J. Chem. Theory Comput. special issue \"Machine\n  Learning and Statistical Mechanics: Shared Synergies for Next Generation of\n  Chemical Theory and Computation.\""},{"id":"http://arxiv.org/abs/2409.06427v1","updated":"2024-09-10T11:19:13Z","published":"2024-09-10T11:19:13Z","title":"GeMuCo: Generalized Multisensory Correlational Model for Body Schema\n  Learning","summary":"  Humans can autonomously learn the relationship between sensation and motion\nin their own bodies, estimate and control their own body states, and move while\ncontinuously adapting to the current environment. On the other hand, current\nrobots control their bodies by learning the network structure described by\nhumans from their experiences, making certain assumptions on the relationship\nbetween sensors and actuators. In addition, the network model does not adapt to\nchanges in the robot's body, the tools that are grasped, or the environment,\nand there is no unified theory, not only for control but also for state\nestimation, anomaly detection, simulation, and so on. In this study, we propose\na Generalized Multisensory Correlational Model (GeMuCo), in which the robot\nitself acquires a body schema describing the correlation between sensors and\nactuators from its own experience, including model structures such as network\ninput/output. The robot adapts to the current environment by updating this body\nschema model online, estimates and controls its body state, and even performs\nanomaly detection and simulation. We demonstrate the effectiveness of this\nmethod by applying it to tool-use considering changes in grasping state for an\naxis-driven robot, to joint-muscle mapping learning for a musculoskeletal\nrobot, and to full-body tool manipulation for a low-rigidity plastic-made\nhumanoid.\n","authors":["Kento Kawaharazuka","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2409.06427v1.pdf","comment":"Accepted at IEEE Robotics and Automation Magazine"},{"id":"http://arxiv.org/abs/2409.06416v1","updated":"2024-09-10T10:55:48Z","published":"2024-09-10T10:55:48Z","title":"Exploring the Integration of Large Language Models in Industrial Test\n  Maintenance Processes","summary":"  Much of the cost and effort required during the software testing process is\ninvested in performing test maintenance - the addition, removal, or\nmodification of test cases to keep the test suite in sync with the\nsystem-under-test or to otherwise improve its quality. Tool support could\nreduce the cost - and improve the quality - of test maintenance by automating\naspects of the process or by providing guidance and support to developers.\n  In this study, we explore the capabilities and applications of large language\nmodels (LLMs) - complex machine learning models adapted to textual analysis -\nto support test maintenance. We conducted a case study at Ericsson AB where we\nexplored the triggers that indicate the need for test maintenance, the actions\nthat LLMs can take, and the considerations that must be made when deploying\nLLMs in an industrial setting. We also proposed and demonstrated\nimplementations of two multi-agent architectures that can predict which test\ncases require maintenance following a change to the source code. Collectively,\nthese contributions advance our theoretical and practical understanding of how\nLLMs can be deployed to benefit industrial test maintenance processes.\n","authors":["Ludvig Lemner","Linnea Wahlgren","Gregory Gay","Nasser Mohammadiha","Jingxiong Liu","Joakim Wennerberg"],"pdf_url":"https://arxiv.org/pdf/2409.06416v1.pdf","comment":"Under submission to ACM TOSEM"},{"id":"http://arxiv.org/abs/2409.06411v1","updated":"2024-09-10T10:49:38Z","published":"2024-09-10T10:49:38Z","title":"Length Desensitization in Directed Preference Optimization","summary":"  Direct Preference Optimization (DPO) is widely utilized in the Reinforcement\nLearning from Human Feedback (RLHF) phase to align Large Language Models (LLMs)\nwith human preferences, thereby enhancing both their harmlessness and efficacy.\nHowever, it has been observed that DPO tends to over-optimize for verbosity,\nwhich can detrimentally affect both performance and user experience. In this\npaper, we conduct an in-depth theoretical analysis of DPO's optimization\nobjective and reveal a strong correlation between its implicit reward and data\nlength. This correlation misguides the optimization direction, resulting in\nlength sensitivity during the DPO training and leading to verbosity. To address\nthis issue, we propose a length-desensitization improvement method for DPO,\ntermed LD-DPO. The proposed method aims to desensitize DPO to data length by\ndecoupling explicit length preference, which is relatively insignificant, from\nthe other implicit preferences, thereby enabling more effective learning of the\nintrinsic preferences. We utilized two settings (Base and Instruct) of\nLlama2-13B, Llama3-8B, and Qwen2-7B for experimental validation on various\nbenchmarks including MT-Bench and AlpacaEval 2. The experimental results\nindicate that LD-DPO consistently outperforms DPO and other baseline methods,\nachieving more concise responses with a 10-40\\% reduction in length compared to\nDPO. We conducted in-depth experimental analyses to demonstrate that LD-DPO can\nindeed achieve length desensitization and align the model more closely with\nhuman-real preferences.\n","authors":["Wei Liu","Yang Bai","Chengcheng Han","Rongxiang Weng","Jun Xu","Xuezhi Cao","Jingang Wang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2409.06411v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.06407v1","updated":"2024-09-10T10:43:53Z","published":"2024-09-10T10:43:53Z","title":"Sources of Uncertainty in 3D Scene Reconstruction","summary":"  The process of 3D scene reconstruction can be affected by numerous\nuncertainty sources in real-world scenes. While Neural Radiance Fields (NeRFs)\nand 3D Gaussian Splatting (GS) achieve high-fidelity rendering, they lack\nbuilt-in mechanisms to directly address or quantify uncertainties arising from\nthe presence of noise, occlusions, confounding outliers, and imprecise camera\npose inputs. In this paper, we introduce a taxonomy that categorizes different\nsources of uncertainty inherent in these methods. Moreover, we extend NeRF- and\nGS-based methods with uncertainty estimation techniques, including learning\nuncertainty outputs and ensembles, and perform an empirical study to assess\ntheir ability to capture the sensitivity of the reconstruction. Our study\nhighlights the need for addressing various uncertainty aspects when designing\nNeRF/GS-based methods for uncertainty-aware 3D reconstruction.\n","authors":["Marcus Klasson","Riccardo Mereu","Juho Kannala","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2409.06407v1.pdf","comment":"To appear in ECCV 2024 Workshop Proceedings. Project page at\n  https://aaltoml.github.io/uncertainty-nerf-gs/"},{"id":"http://arxiv.org/abs/2409.06402v1","updated":"2024-09-10T10:36:40Z","published":"2024-09-10T10:36:40Z","title":"Symmetry Breaking in Neural Network Optimization: Insights from Input\n  Dimension Expansion","summary":"  Understanding the mechanisms behind neural network optimization is crucial\nfor improving network design and performance. While various optimization\ntechniques have been developed, a comprehensive understanding of the underlying\nprinciples that govern these techniques remains elusive. Specifically, the role\nof symmetry breaking, a fundamental concept in physics, has not been fully\nexplored in neural network optimization. This gap in knowledge limits our\nability to design networks that are both efficient and effective. Here, we\npropose the symmetry breaking hypothesis to elucidate the significance of\nsymmetry breaking in enhancing neural network optimization. We demonstrate that\na simple input expansion can significantly improve network performance across\nvarious tasks, and we show that this improvement can be attributed to the\nunderlying symmetry breaking mechanism. We further develop a metric to quantify\nthe degree of symmetry breaking in neural networks, providing a practical\napproach to evaluate and guide network design. Our findings confirm that\nsymmetry breaking is a fundamental principle that underpins various\noptimization techniques, including dropout, batch normalization, and\nequivariance. By quantifying the degree of symmetry breaking, our work offers a\npractical technique for performance enhancement and a metric to guide network\ndesign without the need for complete datasets and extensive training processes.\n","authors":["Jun-Jie Zhang","Nan Cheng","Fu-Peng Li","Xiu-Cheng Wang","Jian-Nan Chen","Long-Gang Pang","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2409.06402v1.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.13870v4","updated":"2024-09-10T10:12:56Z","published":"2023-11-23T09:27:08Z","title":"Multi-intention Inverse Q-learning for Interpretable Behavior\n  Representation","summary":"  In advancing the understanding of natural decision-making processes, inverse\nreinforcement learning (IRL) methods have proven instrumental in reconstructing\nanimal's intentions underlying complex behaviors. Given the recent development\nof a continuous-time multi-intention IRL framework, there has been persistent\ninquiry into inferring discrete time-varying rewards with IRL. To address this\nchallenge, we introduce the class of hierarchical inverse Q-learning (HIQL)\nalgorithms. Through an unsupervised learning process, HIQL divides expert\ntrajectories into multiple intention segments, and solves the IRL problem\nindependently for each. Applying HIQL to simulated experiments and several real\nanimal behavior datasets, our approach outperforms current benchmarks in\nbehavior prediction and produces interpretable reward functions. Our results\nsuggest that the intention transition dynamics underlying complex\ndecision-making behavior is better modeled by a step function instead of a\nsmoothly varying function. This advancement holds promise for neuroscience and\ncognitive science, contributing to a deeper understanding of decision-making\nand uncovering underlying brain mechanisms.\n","authors":["Hao Zhu","Brice De La Crompe","Gabriel Kalweit","Artur Schneider","Maria Kalweit","Ilka Diester","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2311.13870v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02834v2","updated":"2024-09-10T09:48:02Z","published":"2023-02-06T14:52:56Z","title":"Surrogate uncertainty estimation for your time series forecasting\n  black-box: learn when to trust","summary":"  Machine learning models play a vital role in time series forecasting. These\nmodels, however, often overlook an important element: point uncertainty\nestimates. Incorporating these estimates is crucial for effective risk\nmanagement, informed model selection, and decision-making.To address this\nissue, our research introduces a method for uncertainty estimation. We employ a\nsurrogate Gaussian process regression model. It enhances any base regression\nmodel with reasonable uncertainty estimates. This approach stands out for its\ncomputational efficiency. It only necessitates training one supplementary\nsurrogate and avoids any data-specific assumptions. Furthermore, this method\nfor work requires only the presence of the base model as a black box and its\nrespective training data. The effectiveness of our approach is supported by\nexperimental results. Using various time-series forecasting data, we found that\nour surrogate model-based technique delivers significantly more accurate\nconfidence intervals. These techniques outperform both bootstrap-based and\nbuilt-in methods in a medium-data regime. This superiority holds across a range\nof base model types, including a linear regression, ARIMA, gradient boosting\nand a neural network.\n","authors":["Leonid Erlygin","Vladimir Zholobov","Valeriia Baklanova","Evgeny Sokolovskiy","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2302.02834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06366v1","updated":"2024-09-10T09:44:15Z","published":"2024-09-10T09:44:15Z","title":"One Policy to Run Them All: an End-to-end Learning Approach to\n  Multi-Embodiment Locomotion","summary":"  Deep Reinforcement Learning techniques are achieving state-of-the-art results\nin robust legged locomotion. While there exists a wide variety of legged\nplatforms such as quadruped, humanoids, and hexapods, the field is still\nmissing a single learning framework that can control all these different\nembodiments easily and effectively and possibly transfer, zero or few-shot, to\nunseen robot embodiments. We introduce URMA, the Unified Robot Morphology\nArchitecture, to close this gap. Our framework brings the end-to-end Multi-Task\nReinforcement Learning approach to the realm of legged robots, enabling the\nlearned policy to control any type of robot morphology. The key idea of our\nmethod is to allow the network to learn an abstract locomotion controller that\ncan be seamlessly shared between embodiments thanks to our morphology-agnostic\nencoders and decoders. This flexible architecture can be seen as a potential\nfirst step in building a foundation model for legged robot locomotion. Our\nexperiments show that URMA can learn a locomotion policy on multiple\nembodiments that can be easily transferred to unseen robot platforms in\nsimulation and the real world.\n","authors":["Nico Bohlinger","Grzegorz Czechmanowski","Maciej Krupka","Piotr Kicki","Krzysztof Walas","Jan Peters","Davide Tateo"],"pdf_url":"https://arxiv.org/pdf/2409.06366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06364v1","updated":"2024-09-10T09:42:58Z","published":"2024-09-10T09:42:58Z","title":"What happens to diffusion model likelihood when your model is\n  conditional?","summary":"  Diffusion Models (DMs) iteratively denoise random samples to produce\nhigh-quality data. The iterative sampling process is derived from Stochastic\nDifferential Equations (SDEs), allowing a speed-quality trade-off chosen at\ninference. Another advantage of sampling with differential equations is exact\nlikelihood computation. These likelihoods have been used to rank unconditional\nDMs and for out-of-domain classification. Despite the many existing and\npossible uses of DM likelihoods, the distinct properties captured are unknown,\nespecially in conditional contexts such as Text-To-Image (TTI) or\nText-To-Speech synthesis (TTS). Surprisingly, we find that TTS DM likelihoods\nare agnostic to the text input. TTI likelihood is more expressive but cannot\ndiscern confounding prompts. Our results show that applying DMs to conditional\ntasks reveals inconsistencies and strengthens claims that the properties of DM\nlikelihood are unknown. This impact sheds light on the previously unknown\nnature of DM likelihoods. Although conditional DMs maximise likelihood, the\nlikelihood in question is not as sensitive to the conditioning input as one\nexpects. This investigation provides a new point-of-view on diffusion\nlikelihoods.\n","authors":["Mattias Cross","Anton Ragni"],"pdf_url":"https://arxiv.org/pdf/2409.06364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04405v2","updated":"2024-09-10T09:36:07Z","published":"2024-08-08T12:14:17Z","title":"Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces","summary":"  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_url":"https://arxiv.org/pdf/2408.04405v2.pdf","comment":"12 pages, {Owner/Author | ACM} {2024}. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will published in https://energy.acm.org/eir"},{"id":"http://arxiv.org/abs/2409.06362v1","updated":"2024-09-10T09:32:16Z","published":"2024-09-10T09:32:16Z","title":"Connecting Concept Convexity and Human-Machine Alignment in Deep Neural\n  Networks","summary":"  Understanding how neural networks align with human cognitive processes is a\ncrucial step toward developing more interpretable and reliable AI systems.\nMotivated by theories of human cognition, this study examines the relationship\nbetween \\emph{convexity} in neural network representations and\n\\emph{human-machine alignment} based on behavioral data. We identify a\ncorrelation between these two dimensions in pretrained and fine-tuned vision\ntransformer models. Our findings suggest that the convex regions formed in\nlatent spaces of neural networks to some extent align with human-defined\ncategories and reflect the similarity relations humans use in cognitive tasks.\nWhile optimizing for alignment generally enhances convexity, increasing\nconvexity through fine-tuning yields inconsistent effects on alignment, which\nsuggests a complex relationship between the two. This study presents a first\nstep toward understanding the relationship between the convexity of latent\nrepresentations and human-machine alignment.\n","authors":["Teresa Dorszewski","Lenka Tětková","Lorenz Linhardt","Lars Kai Hansen"],"pdf_url":"https://arxiv.org/pdf/2409.06362v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2409.06356v1","updated":"2024-09-10T09:23:03Z","published":"2024-09-10T09:23:03Z","title":"Double Successive Over-Relaxation Q-Learning with an Extension to Deep\n  Reinforcement Learning","summary":"  Q-learning is a widely used algorithm in reinforcement learning (RL), but its\nconvergence can be slow, especially when the discount factor is close to one.\nSuccessive Over-Relaxation (SOR) Q-learning, which introduces a relaxation\nfactor to speed up convergence, addresses this issue but has two major\nlimitations: In the tabular setting, the relaxation parameter depends on\ntransition probability, making it not entirely model-free, and it suffers from\noverestimation bias. To overcome these limitations, we propose a sample-based,\nmodel-free double SOR Q-learning algorithm. Theoretically and empirically, this\nalgorithm is shown to be less biased than SOR Q-learning. Further, in the\ntabular setting, the convergence analysis under boundedness assumptions on\niterates is discussed. The proposed algorithm is extended to large-scale\nproblems using deep RL. Finally, the tabular version of the proposed algorithm\nis compared using roulette and grid world environments, while the deep RL\nversion is tested on a maximization bias example and OpenAI Gym environments.\n","authors":["Shreyas S R"],"pdf_url":"https://arxiv.org/pdf/2409.06356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03953v2","updated":"2024-09-10T09:16:56Z","published":"2024-09-06T00:34:44Z","title":"Epistemic Uncertainty and Observation Noise with the Neural Tangent\n  Kernel","summary":"  Recent work has shown that training wide neural networks with gradient\ndescent is formally equivalent to computing the mean of the posterior\ndistribution in a Gaussian Process (GP) with the Neural Tangent Kernel (NTK) as\nthe prior covariance and zero aleatoric noise \\parencite{jacot2018neural}. In\nthis paper, we extend this framework in two ways. First, we show how to deal\nwith non-zero aleatoric noise. Second, we derive an estimator for the posterior\ncovariance, giving us a handle on epistemic uncertainty. Our proposed approach\nintegrates seamlessly with standard training pipelines, as it involves training\na small number of additional predictors using gradient descent on a mean\nsquared error loss. We demonstrate the proof-of-concept of our method through\nempirical evaluation on synthetic regression.\n","authors":["Sergio Calvo-Ordoñez","Konstantina Palla","Kamil Ciosek"],"pdf_url":"https://arxiv.org/pdf/2409.03953v2.pdf","comment":"11 pages including appendix. Fix incorrect author affiliations in the\n  initial revision due to typos"},{"id":"http://arxiv.org/abs/2404.08263v2","updated":"2024-09-10T09:14:49Z","published":"2024-04-12T06:23:07Z","title":"Relational Prompt-based Pre-trained Language Models for Social Event\n  Detection","summary":"  Social Event Detection (SED) aims to identify significant events from social\nstreams, and has a wide application ranging from public opinion analysis to\nrisk management. In recent years, Graph Neural Network (GNN) based solutions\nhave achieved state-of-the-art performance. However, GNN-based methods often\nstruggle with missing and noisy edges between messages, affecting the quality\nof learned message embedding. Moreover, these methods statically initialize\nnode embedding before training, which, in turn, limits the ability to learn\nfrom message texts and relations simultaneously. In this paper, we approach\nsocial event detection from a new perspective based on Pre-trained Language\nModels (PLMs), and present RPLM_SED (Relational prompt-based Pre-trained\nLanguage Models for Social Event Detection). We first propose a new pairwise\nmessage modeling strategy to construct social messages into message pairs with\nmulti-relational sequences. Secondly, a new multi-relational prompt-based\npairwise message learning mechanism is proposed to learn more comprehensive\nmessage representation from message pairs with multi-relational prompts using\nPLMs. Thirdly, we design a new clustering constraint to optimize the encoding\nprocess by enhancing intra-cluster compactness and inter-cluster dispersion,\nmaking the message representation more distinguishable. We evaluate the\nRPLM_SED on three real-world datasets, demonstrating that the RPLM_SED model\nachieves state-of-the-art performance in offline, online, low-resource, and\nlong-tail distribution scenarios for social event detection tasks.\n","authors":["Pu Li","Xiaoyan Yu","Hao Peng","Yantuan Xian","Linqin Wang","Li Sun","Jingyun Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.08263v2.pdf","comment":"ACM TOIS"},{"id":"http://arxiv.org/abs/2409.06349v1","updated":"2024-09-10T09:07:47Z","published":"2024-09-10T09:07:47Z","title":"Improving Conditional Level Generation using Automated Validation in\n  Match-3 Games","summary":"  Generative models for level generation have shown great potential in game\nproduction. However, they often provide limited control over the generation,\nand the validity of the generated levels is unreliable. Despite this fact, only\na few approaches that learn from existing data provide the users with ways of\ncontrolling the generation, simultaneously addressing the generation of\nunsolvable levels. %One of the main challenges it faces is that levels\ngenerated through automation may not be solvable thus requiring validation. are\nnot always engaging, challenging, or even solvable. This paper proposes Avalon,\na novel method to improve models that learn from existing level designs using\ndifficulty statistics extracted from gameplay. In particular, we use a\nconditional variational autoencoder to generate layouts for match-3 levels,\nconditioning the model on pre-collected statistics such as game mechanics like\ndifficulty and relevant visual features like size and symmetry. Our method is\ngeneral enough that multiple approaches could potentially be used to generate\nthese statistics. We quantitatively evaluate our approach by comparing it to an\nablated model without difficulty conditioning. Additionally, we analyze both\nquantitatively and qualitatively whether the style of the dataset is preserved\nin the generated levels. Our approach generates more valid levels than the same\nmethod without difficulty conditioning.\n","authors":["Monica Villanueva Aylagas","Joakim Bergdahl","Jonas Gillberg","Alessandro Sestini","Theodor Tolstoy","Linus Gisslén"],"pdf_url":"https://arxiv.org/pdf/2409.06349v1.pdf","comment":"10 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.06343v1","updated":"2024-09-10T08:52:24Z","published":"2024-09-10T08:52:24Z","title":"Compute-Update Federated Learning: A Lattice Coding Approach","summary":"  This paper introduces a federated learning framework that enables\nover-the-air computation via digital communications, using a new joint\nsource-channel coding scheme. Without relying on channel state information at\ndevices, this scheme employs lattice codes to both quantize model parameters\nand exploit interference from the devices. We propose a novel receiver\nstructure at the server, designed to reliably decode an integer combination of\nthe quantized model parameters as a lattice point for the purpose of\naggregation. We present a mathematical approach to derive a convergence bound\nfor the proposed scheme and offer design remarks. In this context, we suggest\nan aggregation metric and a corresponding algorithm to determine effective\ninteger coefficients for the aggregation in each communication round. Our\nresults illustrate that, regardless of channel dynamics and data heterogeneity,\nour scheme consistently delivers superior learning accuracy across various\nparameters and markedly surpasses other over-the-air methodologies.\n","authors":["Seyed Mohammad Azimi-Abarghouyi","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2409.06343v1.pdf","comment":"Extended version of the preprint available at arXiv:2403.01023"},{"id":"http://arxiv.org/abs/2307.07975v4","updated":"2024-09-10T08:51:44Z","published":"2023-07-16T07:55:35Z","title":"Pseudo-rigid body networks: learning interpretable deformable object\n  dynamics from partial observations","summary":"  Accurately predicting deformable linear object (DLO) dynamics is challenging,\nespecially when the task requires a model that is both human-interpretable and\ncomputationally efficient. In this work, we draw inspiration from the\npseudo-rigid body method (PRB) and model a DLO as a serial chain of rigid\nbodies whose internal state is unrolled through time by a dynamics network.\nThis dynamics network is trained jointly with a physics-informed encoder that\nmaps observed motion variables to the DLO's hidden state. To encourage the\nstate to acquire a physically meaningful representation, we leverage the\nforward kinematics of the PRB model as a decoder. We demonstrate in robot\nexperiments that the proposed DLO dynamics model provides physically\ninterpretable predictions from partial observations while being on par with\nblack-box models regarding prediction accuracy. The project code is available\nat: http://tinyurl.com/prb-networks\n","authors":["Shamil Mamedov","A. René Geist","Jan Swevers","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2307.07975v4.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2409.06329v1","updated":"2024-09-10T08:34:55Z","published":"2024-09-10T08:34:55Z","title":"Modified Meta-Thompson Sampling for Linear Bandits and Its Bayes Regret\n  Analysis","summary":"  Meta-learning is characterized by its ability to learn how to learn, enabling\nthe adaptation of learning strategies across different tasks. Recent research\nintroduced the Meta-Thompson Sampling (Meta-TS), which meta-learns an unknown\nprior distribution sampled from a meta-prior by interacting with bandit\ninstances drawn from it. However, its analysis was limited to Gaussian bandit.\nThe contextual multi-armed bandit framework is an extension of the Gaussian\nBandit, which challenges agent to utilize context vectors to predict the most\nvaluable arms, optimally balancing exploration and exploitation to minimize\nregret over time. This paper introduces Meta-TSLB algorithm, a modified Meta-TS\nfor linear contextual bandits. We theoretically analyze Meta-TSLB and derive an\n$ O\\left( \\left( m+\\log \\left( m \\right) \\right) \\sqrt{n\\log \\left( n \\right)}\n\\right)$ bound on its Bayes regret, in which $m$ represents the number of\nbandit instances, and $n$ the number of rounds of Thompson Sampling.\nAdditionally, our work complements the analysis of Meta-TS for linear\ncontextual bandits. The performance of Meta-TSLB is evaluated experimentally\nunder different settings, and we experimente and analyze the generalization\ncapability of Meta-TSLB, showcasing its potential to adapt to unseen instances.\n","authors":["Hao Li","Dong Liang","Zheng Xie"],"pdf_url":"https://arxiv.org/pdf/2409.06329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06323v1","updated":"2024-09-10T08:27:39Z","published":"2024-09-10T08:27:39Z","title":"LAMP: Learnable Meta-Path Guided Adversarial Contrastive Learning for\n  Heterogeneous Graphs","summary":"  Heterogeneous graph neural networks (HGNNs) have significantly propelled the\ninformation retrieval (IR) field. Still, the effectiveness of HGNNs heavily\nrelies on high-quality labels, which are often expensive to acquire. This\nchallenge has shifted attention towards Heterogeneous Graph Contrastive\nLearning (HGCL), which usually requires pre-defined meta-paths. However, our\nfindings reveal that meta-path combinations significantly affect performance in\nunsupervised settings, an aspect often overlooked in current literature.\nExisting HGCL methods have considerable variability in outcomes across\ndifferent meta-path combinations, thereby challenging the optimization process\nto achieve consistent and high performance. In response, we introduce\n\\textsf{LAMP} (\\underline{\\textbf{L}}earn\\underline{\\textbf{A}}ble\n\\underline{\\textbf{M}}eta-\\underline{\\textbf{P}}ath), a novel adversarial\ncontrastive learning approach that integrates various meta-path sub-graphs into\na unified and stable structure, leveraging the overlap among these sub-graphs.\nTo address the denseness of this integrated sub-graph, we propose an\nadversarial training strategy for edge pruning, maintaining sparsity to enhance\nmodel performance and robustness. \\textsf{LAMP} aims to maximize the difference\nbetween meta-path and network schema views for guiding contrastive learning to\ncapture the most meaningful information. Our extensive experimental study\nconducted on four diverse datasets from the Heterogeneous Graph Benchmark (HGB)\ndemonstrates that \\textsf{LAMP} significantly outperforms existing\nstate-of-the-art unsupervised models in terms of accuracy and robustness.\n","authors":["Siqing Li","Jin-Duk Park","Wei Huang","Xin Cao","Won-Yong Shin","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06323v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.06319v1","updated":"2024-09-10T08:22:01Z","published":"2024-09-10T08:22:01Z","title":"Rate-Constrained Quantization for Communication-Efficient Federated\n  Learning","summary":"  Quantization is a common approach to mitigate the communication cost of\nfederated learning (FL). In practice, the quantized local parameters are\nfurther encoded via an entropy coding technique, such as Huffman coding, for\nefficient data compression. In this case, the exact communication overhead is\ndetermined by the bit rate of the encoded gradients. Recognizing this fact,\nthis work deviates from the existing approaches in the literature and develops\na novel quantized FL framework, called \\textbf{r}ate-\\textbf{c}onstrained\n\\textbf{fed}erated learning (RC-FED), in which the gradients are quantized\nsubject to both fidelity and data rate constraints. We formulate this scheme,\nas a joint optimization in which the quantization distortion is minimized while\nthe rate of encoded gradients is kept below a target threshold. This enables\nfor a tunable trade-off between quantization distortion and communication cost.\nWe analyze the convergence behavior of RC-FED, and show its superior\nperformance against baseline quantized FL schemes on several datasets.\n","authors":["Shayan Mohajer Hamidi","Ali Bereyhi"],"pdf_url":"https://arxiv.org/pdf/2409.06319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06316v1","updated":"2024-09-10T08:17:06Z","published":"2024-09-10T08:17:06Z","title":"PharmacoMatch: Efficient 3D Pharmacophore Screening through Neural\n  Subgraph Matching","summary":"  The increasing size of screening libraries poses a significant challenge for\nthe development of virtual screening methods for drug discovery, necessitating\na re-evaluation of traditional approaches in the era of big data. Although 3D\npharmacophore screening remains a prevalent technique, its application to very\nlarge datasets is limited by the computational cost associated with matching\nquery pharmacophores to database ligands. In this study, we introduce\nPharmacoMatch, a novel contrastive learning approach based on neural subgraph\nmatching. Our method reinterprets pharmacophore screening as an approximate\nsubgraph matching problem and enables efficient querying of conformational\ndatabases by encoding query-target relationships in the embedding space. We\nconduct comprehensive evaluations of the learned representations and benchmark\nour method on virtual screening datasets in a zero-shot setting. Our findings\ndemonstrate significantly shorter runtimes for pharmacophore matching, offering\na promising speed-up for screening very large datasets.\n","authors":["Daniel Rose","Oliver Wieder","Thomas Seidel","Thierry Langer"],"pdf_url":"https://arxiv.org/pdf/2409.06316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05100v2","updated":"2024-09-10T08:00:19Z","published":"2024-09-08T14:06:25Z","title":"MaxCutPool: differentiable feature-aware Maxcut for pooling in graph\n  neural networks","summary":"  We propose a novel approach to compute the MAXCUT in attributed graphs, i.e.,\ngraphs with features associated with nodes and edges. Our approach is robust to\nthe underlying graph topology and is fully differentiable, making it possible\nto find solutions that jointly optimize the MAXCUT along with other objectives.\nBased on the obtained MAXCUT partition, we implement a hierarchical graph\npooling layer for Graph Neural Networks, which is sparse, differentiable, and\nparticularly suitable for downstream tasks on heterophilic graphs.\n","authors":["Carlo Abate","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2409.05100v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.01457v2","updated":"2024-09-10T07:58:25Z","published":"2023-05-02T14:37:52Z","title":"Memory of recurrent networks: Do we compute it right?","summary":"  Numerical evaluations of the memory capacity (MC) of recurrent neural\nnetworks reported in the literature often contradict well-established\ntheoretical bounds. In this paper, we study the case of linear echo state\nnetworks, for which the total memory capacity has been proven to be equal to\nthe rank of the corresponding Kalman controllability matrix. We shed light on\nvarious reasons for the inaccurate numerical estimations of the memory, and we\nshow that these issues, often overlooked in the recent literature, are of an\nexclusively numerical nature. More explicitly, we prove that when the Krylov\nstructure of the linear MC is ignored, a gap between the theoretical MC and its\nempirical counterpart is introduced. As a solution, we develop robust numerical\napproaches by exploiting a result of MC neutrality with respect to the input\nmask matrix. Simulations show that the memory curves that are recovered using\nthe proposed methods fully agree with the theory.\n","authors":["Giovanni Ballarin","Lyudmila Grigoryeva","Juan-Pablo Ortega"],"pdf_url":"https://arxiv.org/pdf/2305.01457v2.pdf","comment":"33 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.05477v2","updated":"2024-09-10T07:54:18Z","published":"2024-09-09T10:11:25Z","title":"Retrofitting Temporal Graph Neural Networks with Transformer","summary":"  Temporal graph neural networks (TGNNs) outperform regular GNNs by\nincorporating time information into graph-based operations. However, TGNNs\nadopt specialized models (e.g., TGN, TGAT, and APAN ) and require tailored\ntraining frameworks (e.g., TGL and ETC). In this paper, we propose TF-TGN,\nwhich uses Transformer decoder as the backbone model for TGNN to enjoy\nTransformer's codebase for efficient training. In particular, Transformer\nachieves tremendous success for language modeling, and thus the community\ndeveloped high-performance kernels (e.g., flash-attention and memory-efficient\nattention) and efficient distributed training schemes (e.g., PyTorch FSDP,\nDeepSpeed, and Megatron-LM). We observe that TGNN resembles language modeling,\ni.e., the message aggregation operation between chronologically occurring nodes\nand their temporal neighbors in TGNNs can be structured as sequence modeling.\nBeside this similarity, we also incorporate a series of algorithm designs\nincluding suffix infilling, temporal graph attention with self-loop, and causal\nmasking self-attention to make TF-TGN work. During training, existing systems\nare slow in transforming the graph topology and conducting graph sampling. As\nsuch, we propose methods to parallelize the CSR format conversion and graph\nsampling. We also adapt Transformer codebase to train TF-TGN efficiently with\nmultiple GPUs. We experiment with 9 graphs and compare with 2 state-of-the-art\nTGNN training frameworks. The results show that TF-TGN can accelerate training\nby over 2.20 while providing comparable or even superior accuracy to existing\nSOTA TGNNs. TF-TGN is available at https://github.com/qianghuangwhu/TF-TGN.\n","authors":["Qiang Huang","Xiao Yan","Xin Wang","Susie Xi Rao","Zhichao Han","Fangcheng Fu","Wentao Zhang","Jiawei Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.05477v2.pdf","comment":"conference Under review"},{"id":"http://arxiv.org/abs/2409.06297v1","updated":"2024-09-10T07:51:53Z","published":"2024-09-10T07:51:53Z","title":"User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study","summary":"  Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems.\n","authors":["Julien Albert","Martin Balfroid","Miriam Doh","Jeremie Bogaert","Luca La Fisca","Liesbet De Vos","Bryan Renard","Vincent Stragier","Emmanuel Jean"],"pdf_url":"https://arxiv.org/pdf/2409.06297v1.pdf","comment":"Presented to the Dutch-Belgian Workshop on Recommender Systems 2023\n  (14-15 December, 2023 - Antwerp, Belgium)"},{"id":"http://arxiv.org/abs/2406.02765v4","updated":"2024-09-10T07:49:17Z","published":"2024-06-04T20:33:29Z","title":"Discovering Dynamic Symbolic Policies with Genetic Programming","summary":"  Artificial intelligence techniques are increasingly being applied to solve\ncontrol problems, but often rely on black-box methods without transparent\noutput generation. To improve the interpretability and transparency in control\nsystems, models can be defined as white-box symbolic policies described by\nmathematical expressions. While current approaches to learn symbolic policies\nfocus on static policies that directly map observations to control signals,\nthese may fail in partially observable and volatile environments. We instead\nconsider dynamic symbolic policies with memory, optimised with genetic\nprogramming. The resulting policies are robust, and consist of easy to\ninterpret coupled differential equations. Our results show that dynamic\nsymbolic policies compare with black-box policies on a variety of control\ntasks. Furthermore, the benefit of the memory in dynamic policies is\ndemonstrated on experiments where static policies fall short. Overall, we\npresent a method for evolving high-performing symbolic policies that offer\ninterpretability and transparency, which lacks in black-box models.\n","authors":["Sigur de Vries","Sander Keemink","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2406.02765v4.pdf","comment":"19 pages including references and appendix, 5 figures, 1 algorithm, 5\n  tables"},{"id":"http://arxiv.org/abs/2011.06356v2","updated":"2024-09-10T07:42:29Z","published":"2020-11-12T12:59:10Z","title":"Cross Layer Optimization and Distributed Reinforcement Learning for\n  Wireless 360° Video Streaming","summary":"  Wirelessly streaming high quality 360 degree videos is still a challenging\nproblem. When there are many users watching different 360 degree videos and\ncompeting for the computing and communication resources, the streaming\nalgorithm at hand should maximize the average quality of experience (QoE) while\nguaranteeing a minimum rate for each user. In this paper, we propose a cross\nlayer optimization approach that maximizes the available rate to each user and\nefficiently uses it to maximize users' QoE. Particularly, we consider a tile\nbased 360 degree video streaming, and we optimize a QoE metric that balances\nthe tradeoff between maximizing each user's QoE and ensuring fairness among\nusers. We show that the problem can be decoupled into two interrelated\nsubproblems: (i) a physical layer subproblem whose objective is to find the\ndownload rate for each user, and (ii) an application layer subproblem whose\nobjective is to use that rate to find a quality decision per tile such that the\nuser's QoE is maximized. We prove that the physical layer subproblem can be\nsolved optimally with low complexity and an actor-critic deep reinforcement\nlearning (DRL) is proposed to leverage the parallel training of multiple\nindependent agents and solve the application layer subproblem. Extensive\nexperiments reveal the robustness of our scheme and demonstrate its significant\nperformance improvement compared to several baseline algorithms.\n","authors":["Anis Elgabli","Mohammed S. Elbamby","Cristina Perfecto","Mounssif Krouka","Mehdi Bennis","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2011.06356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06289v1","updated":"2024-09-10T07:42:28Z","published":"2024-09-10T07:42:28Z","title":"Automate Strategy Finding with LLM in Quant investment","summary":"  Despite significant progress in deep learning for financial trading, existing\nmodels often face instability and high uncertainty, hindering their practical\napplication. Leveraging advancements in Large Language Models (LLMs) and\nmulti-agent architectures, we propose a novel framework for quantitative stock\ninvestment in portfolio management and alpha mining. Our framework addresses\nthese issues by integrating LLMs to generate diversified alphas and employing a\nmulti-agent approach to dynamically evaluate market conditions. This paper\nproposes a framework where large language models (LLMs) mine alpha factors from\nmultimodal financial data, ensuring a comprehensive understanding of market\ndynamics. The first module extracts predictive signals by integrating numerical\ndata, research papers, and visual charts. The second module uses ensemble\nlearning to construct a diverse pool of trading agents with varying risk\npreferences, enhancing strategy performance through a broader market analysis.\nIn the third module, a dynamic weight-gating mechanism selects and assigns\nweights to the most relevant agents based on real-time market conditions,\nenabling the creation of an adaptive and context-aware composite alpha formula.\nExtensive experiments on the Chinese stock markets demonstrate that this\nframework significantly outperforms state-of-the-art baselines across multiple\nfinancial metrics. The results underscore the efficacy of combining\nLLM-generated alphas with a multi-agent architecture to achieve superior\ntrading performance and stability. This work highlights the potential of\nAI-driven approaches in enhancing quantitative investment strategies and sets a\nnew benchmark for integrating advanced machine learning techniques in financial\ntrading can also be applied on diverse markets.\n","authors":["Zhizhuo Kou","Holam Yu","Jingshu Peng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2409.06289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11569v2","updated":"2024-09-10T07:34:50Z","published":"2024-06-17T14:06:13Z","title":"Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs","summary":"  For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.\n","authors":["Haifeng Wen","Hong Xing","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2406.11569v2.pdf","comment":"38 pages, 7 figures, submitted for possible journal publication"},{"id":"http://arxiv.org/abs/2409.06282v1","updated":"2024-09-10T07:34:19Z","published":"2024-09-10T07:34:19Z","title":"Learning Augmentation Policies from A Model Zoo for Time Series\n  Forecasting","summary":"  Time series forecasting models typically rely on a fixed-size training set\nand treat all data uniformly, which may not effectively capture the specific\npatterns present in more challenging training samples. To address this issue,\nwe introduce AutoTSAug, a learnable data augmentation method based on\nreinforcement learning. Our approach begins with an empirical analysis to\ndetermine which parts of the training data should be augmented. Specifically,\nwe identify the so-called marginal samples by considering the prediction\ndiversity across a set of pretrained forecasting models. Next, we propose using\nvariational masked autoencoders as the augmentation model and applying the\nREINFORCE algorithm to transform the marginal samples into new data. The goal\nof this generative model is not only to mimic the distribution of real data but\nalso to reduce the variance of prediction errors across the model zoo. By\naugmenting the marginal samples with a learnable policy, AutoTSAug\nsubstantially improves forecasting performance, advancing the prior art in this\nfield with minimal additional computational cost.\n","authors":["Haochen Yuan","Xuelin Li","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2409.06282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06277v1","updated":"2024-09-10T07:28:13Z","published":"2024-09-10T07:28:13Z","title":"Ferret: Federated Full-Parameter Tuning at Scale for Large Language\n  Models","summary":"  Large Language Models (LLMs) have become indispensable in numerous real-world\napplications. Unfortunately, fine-tuning these models at scale, especially in\nfederated settings where data privacy and communication efficiency are\ncritical, presents significant challenges. Existing methods often resort to\nparameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but\nthis typically comes at the cost of model accuracy. To address these\nlimitations, we propose federated full-parameter tuning at scale for LLMs\n(Ferret), the first first-order method with shared randomness to enable\nscalable full-parameter tuning of LLMs across decentralized data sources while\nmaintaining competitive model accuracy. Ferret accomplishes this through three\naspects: (1) it employs widely applied first-order methods for efficient local\nupdates; (2) it projects these updates into a low-dimensional space to\nconsiderably reduce communication overhead; and (3) it reconstructs local\nupdates from this low-dimensional space with shared randomness to facilitate\neffective full-parameter global aggregation, ensuring fast convergence and\ncompetitive final performance. Our rigorous theoretical analyses and insights\nalong with extensive experiments, show that Ferret significantly enhances the\nscalability of existing federated full-parameter tuning approaches by achieving\nhigh computational efficiency, reduced communication overhead, and fast\nconvergence, all while maintaining competitive model accuracy. Our\nimplementation is available at https://github.com/allen4747/Ferret.\n","authors":["Yao Shu","Wenyang Hu","See-Kiong Ng","Bryan Kian Hsiang Low","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2409.06277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10274v2","updated":"2024-09-10T07:21:27Z","published":"2024-04-16T04:17:17Z","title":"Sparse Attention Regression Network Based Soil Fertility Prediction With\n  Ummaso","summary":"  The challenge of imbalanced soil nutrient datasets significantly hampers\naccurate predictions of soil fertility. To tackle this, a new method is\nsuggested in this research, combining Uniform Manifold Approximation and\nProjection (UMAP) with Least Absolute Shrinkage and Selection Operator (LASSO).\nThe main aim is to counter the impact of uneven data distribution and improve\nsoil fertility models' predictive precision. The model introduced uses Sparse\nAttention Regression, effectively incorporating pertinent features from the\nimbalanced dataset. UMAP is utilized initially to reduce data complexity,\nunveiling hidden structures and important patterns. Following this, LASSO is\napplied to refine features and enhance the model's interpretability. The\nexperimental outcomes highlight the effectiveness of the UMAP and LASSO hybrid\napproach. The proposed model achieves outstanding performance metrics, reaching\na predictive accuracy of 98%, demonstrating its capability in accurate soil\nfertility predictions. Additionally, it showcases a Precision of 91.25%,\nindicating its adeptness in identifying fertile soil instances accurately. The\nRecall metric stands at 90.90%, emphasizing the model's ability to capture true\npositive cases effectively.\n","authors":["R V Raghavendra Rao","U Srinivasulu Reddy"],"pdf_url":"https://arxiv.org/pdf/2404.10274v2.pdf","comment":"There is an error in the result section"},{"id":"http://arxiv.org/abs/2409.06271v1","updated":"2024-09-10T07:20:51Z","published":"2024-09-10T07:20:51Z","title":"A new paradigm for global sensitivity analysis","summary":"  <div><p>Current theory of global sensitivity analysis, based on a nonlinear\nfunctional ANOVA decomposition of the random output, is limited in scope-for\ninstance, the analysis is limited to the output's variance and the inputs have\nto be mutually independent-and leads to sensitivity indices the interpretation\nof which is not fully clear, especially interaction effects. Alternatively,\nsensitivity indices built for arbitrary user-defined importance measures have\nbeen proposed but a theory to define interactions in a systematic fashion\nand/or establish a decomposition of the total importance measure is still\nmissing. It is shown that these important problems are solved all at once by\nadopting a new paradigm. By partitioning the inputs into those causing the\nchange in the output and those which do not, arbitrary user-defined variability\nmeasures are identified with the outcomes of a factorial experiment at two\nlevels, leading to all factorial effects without assuming any functional\ndecomposition. To link various well-known sensitivity indices of the literature\n(Sobol indices and Shapley effects), weighted factorial effects are studied and\nutilized.</p></div>\n","authors":["Gildas Mazo"],"pdf_url":"https://arxiv.org/pdf/2409.06271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06270v1","updated":"2024-09-10T07:18:57Z","published":"2024-09-10T07:18:57Z","title":"Towards Robust Uncertainty-Aware Incomplete Multi-View Classification","summary":"  Handling incomplete data in multi-view classification is challenging,\nespecially when traditional imputation methods introduce biases that compromise\nuncertainty estimation. Existing Evidential Deep Learning (EDL) based\napproaches attempt to address these issues, but they often struggle with\nconflicting evidence due to the limitations of the Dempster-Shafer combination\nrule, leading to unreliable decisions. To address these challenges, we propose\nthe Alternating Progressive Learning Network (APLN), specifically designed to\nenhance EDL-based methods in incomplete MVC scenarios. Our approach mitigates\nbias from corrupted observed data by first applying coarse imputation, followed\nby mapping the data to a latent space. In this latent space, we progressively\nlearn an evidence distribution aligned with the target domain, incorporating\nuncertainty considerations through EDL. Additionally, we introduce a\nconflict-aware Dempster-Shafer combination rule (DSCR) to better handle\nconflicting evidence. By sampling from the learned distribution, we optimize\nthe latent representations of missing views, reducing bias and enhancing\ndecision-making robustness. Extensive experiments demonstrate that APLN,\ncombined with DSCR, significantly outperforms traditional methods, particularly\nin environments characterized by high uncertainty and conflicting evidence,\nestablishing it as a promising solution for incomplete multi-view\nclassification.\n","authors":["Mulin Chen","Haojian Huang","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2409.06270v1.pdf","comment":"Ongoing work: 9 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.10940v2","updated":"2024-09-10T07:04:02Z","published":"2024-03-16T14:52:26Z","title":"ViSaRL: Visual Reinforcement Learning Guided by Human Saliency","summary":"  Training robots to perform complex control tasks from high-dimensional pixel\ninput using reinforcement learning (RL) is sample-inefficient, because image\nobservations are comprised primarily of task-irrelevant information. By\ncontrast, humans are able to visually attend to task-relevant objects and\nareas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement\nLearning (ViSaRL). Using ViSaRL to learn visual representations significantly\nimproves the success rate, sample efficiency, and generalization of an RL agent\non diverse tasks including DeepMind Control benchmark, robot manipulation in\nsimulation and on a real robot. We present approaches for incorporating\nsaliency into both CNN and Transformer-based encoders. We show that visual\nrepresentations learned using ViSaRL are robust to various sources of visual\nperturbations including perceptual noise and scene variations. ViSaRL nearly\ndoubles success rate on the real-robot tasks compared to the baseline which\ndoes not use saliency.\n","authors":["Anthony Liang","Jesse Thomason","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2403.10940v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09498v2","updated":"2024-09-10T07:03:32Z","published":"2024-06-12T18:30:03Z","title":"OT-VP: Optimal Transport-guided Visual Prompting for Test-Time\n  Adaptation","summary":"  Vision Transformers (ViTs) have demonstrated remarkable capabilities in\nlearning representations, but their performance is compromised when applied to\nunseen domains. Previous methods either engage in prompt learning during the\ntraining phase or modify model parameters at test time through entropy\nminimization. The former often overlooks unlabeled target data, while the\nlatter doesn't fully address domain shifts. In this work, our approach, Optimal\nTransport-guided Test-Time Visual Prompting (OT-VP), handles these problems by\nleveraging prompt learning at test time to align the target and source domains\nwithout accessing the training process or altering pre-trained model\nparameters. This method involves learning a universal visual prompt for the\ntarget domain by optimizing the Optimal Transport distance.OT-VP, with only\nfour learned prompt tokens, exceeds state-of-the-art performance across three\nstylistic datasets-PACS, VLCS, OfficeHome, and one corrupted dataset\nImageNet-C. Additionally, OT-VP operates efficiently, both in terms of memory\nand computation, and is adaptable for extension to online settings.\n","authors":["Yunbei Zhang","Akshay Mehra","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2407.09498v2.pdf","comment":"WACV2025"},{"id":"http://arxiv.org/abs/2409.02969v2","updated":"2024-09-10T07:03:04Z","published":"2024-09-04T07:44:43Z","title":"LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch","summary":"  Multiobjective optimization problems (MOPs) are prevalent in machine\nlearning, with applications in multi-task learning, learning under fairness or\nrobustness constraints, etc. Instead of reducing multiple objective functions\ninto a scalar objective, MOPs aim to optimize for the so-called Pareto\noptimality or Pareto set learning, which involves optimizing more than one\nobjective function simultaneously, over models with millions of parameters.\nExisting benchmark libraries for MOPs mainly focus on evolutionary algorithms,\nmost of which are zeroth-order methods that do not effectively utilize\nhigher-order information from objectives and cannot scale to large-scale models\nwith millions of parameters. In light of the above gap, this paper introduces\nLibMOON, the first multiobjective optimization library that supports\nstate-of-the-art gradient-based methods, provides a fair benchmark, and is\nopen-sourced for the community.\n","authors":["Xiaoyuan Zhang","Liang Zhao","Yingying Yu","Xi Lin","Zhenkun Wang","Han Zhao","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.02969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01204v4","updated":"2024-09-10T07:02:47Z","published":"2024-02-02T08:17:41Z","title":"A Survey on Self-Supervised Learning for Non-Sequential Tabular Data","summary":"  Self-supervised learning (SSL) has been incorporated into many\nstate-of-the-art models in various domains, where SSL defines pretext tasks\nbased on unlabeled datasets to learn contextualized and robust representations.\nRecently, SSL has become a new trend in exploring the representation learning\ncapability in the realm of tabular data, which is more challenging due to not\nhaving explicit relations for learning descriptive representations. This survey\naims to systematically review and summarize the recent progress and challenges\nof SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal\ndefinition of NS-TD and clarify its correlation to related studies. Then, these\napproaches are categorized into three groups - predictive learning, contrastive\nlearning, and hybrid learning, with their motivations and strengths of\nrepresentative methods in each direction. Moreover, application issues of\nSSL4NS-TD are presented, including automatic data engineering, cross-table\ntransferability, and domain knowledge integration. In addition, we elaborate on\nexisting benchmarks and datasets for NS-TD applications to analyze the\nperformance of existing tabular models. Finally, we discuss the challenges of\nSSL4NS-TD and provide potential directions for future research. We expect our\nwork to be useful in terms of encouraging more research on lowering the barrier\nto entry SSL for the tabular domain, and of improving the foundations for\nimplicit tabular data.\n","authors":["Wei-Yao Wang","Wei-Wei Du","Derek Xu","Wei Wang","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2402.01204v4.pdf","comment":"ACML-24 Journal Track. The paper list can be found at\n  https://github.com/wwweiwei/awesome-self-supervised-learning-for-tabular-data"},{"id":"http://arxiv.org/abs/2408.14928v2","updated":"2024-09-10T07:02:27Z","published":"2024-08-27T10:05:37Z","title":"Targeting the partition function of chemically disordered materials with\n  a generative approach based on inverse variational autoencoders","summary":"  Computing atomic-scale properties of chemically disordered materials requires\nan efficient exploration of their vast configuration space. Traditional\napproaches such as Monte Carlo or Special Quasirandom Structures either entail\nsampling an excessive amount of configurations or do not ensure that the\nconfiguration space has been properly covered. In this work, we propose a novel\napproach where generative machine learning is used to yield a representative\nset of configurations for accurate property evaluation and provide accurate\nestimations of atomic-scale properties with minimal computational cost. Our\nmethod employs a specific type of variational autoencoder with inverse roles\nfor the encoder and decoder, enabling the application of an unsupervised active\nlearning scheme that does not require any initial training database. The model\niteratively generates configuration batches, whose properties are computed with\nconventional atomic-scale methods. These results are then fed back into the\nmodel to estimate the partition function, repeating the process until\nconvergence. We illustrate our approach by computing point-defect formation\nenergies and concentrations in (U, Pu)O2 mixed-oxide fuels. In addition, the ML\nmodel provides valuable insights into the physical factors influencing the\ntarget property. Our method is generally applicable to explore other\nproperties, such as atomic-scale diffusion coefficients, in ideally or\nnon-ideally disordered materials like high-entropy alloys.\n","authors":["Maciej J. Karcz","Luca Messina","Eiji Kawasaki","Emeric Bourasseau"],"pdf_url":"https://arxiv.org/pdf/2408.14928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06255v1","updated":"2024-09-10T06:55:17Z","published":"2024-09-10T06:55:17Z","title":"Market Reaction to News Flows in Supply Chain Networks","summary":"  This study examines whether positive news about firms increases their stock\nprices and, moreover, whether it increases stock prices of the firms' suppliers\nand customers, using a large sample of publicly listed firms across the world\nand another of Japanese listed firms. The level of positiveness of each news\narticle is determined by FinBERT, a natural language processing model\nfine-tuned specifically for financial information. Supply chains of firms\nacross the world are identified mostly by financial statements, while those of\nJapanese firms are taken from large-scale firm-level surveys. We find that\npositive news increases the change rate of stock prices of firms mentioned in\nthe news before its disclosure, most likely because of diffusion of information\nthrough informal channels. Positive news also raises stock prices of the firms'\nsuppliers and customers before its disclosure, confirming propagation of market\nvalues through supply chains. In addition, we generally find a larger post-news\neffect on stock prices of the mentioned firms and their suppliers and customers\nthan the pre-news effect. The positive difference between the post- and\npre-news effects can be considered as the net effect of the disclosure of\npositive news, controlling for informal information diffusion. However, the\npost-news effect on suppliers and customers in Japan is smaller than the\npre-news effect, a result opposite to those from firms across the world. This\nnotable result is possibly because supply chain links of Japanese firms are\nstronger than global supply chains while such knowledge is restricted to\nselected investors.\n","authors":["Hiroyasu Inoue","Yasuyuki Todo"],"pdf_url":"https://arxiv.org/pdf/2409.06255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05139v2","updated":"2024-09-10T06:45:41Z","published":"2024-09-08T15:44:00Z","title":"Revisiting Trace Norm Minimization for Tensor Tucker Completion: A\n  Direct Multilinear Rank Learning Approach","summary":"  To efficiently express tensor data using the Tucker format, a critical task\nis to minimize the multilinear rank such that the model would not be\nover-flexible and lead to overfitting. Due to the lack of rank minimization\ntools in tensor, existing works connect Tucker multilinear rank minimization to\ntrace norm minimization of matrices unfolded from the tensor data. While these\nformulations try to exploit the common aim of identifying the low-dimensional\nstructure of the tensor and matrix, this paper reveals that existing trace\nnorm-based formulations in Tucker completion are inefficient in multilinear\nrank minimization. We further propose a new interpretation of Tucker format\nsuch that trace norm minimization is applied to the factor matrices of the\nequivalent representation, rather than some matrices unfolded from tensor data.\nBased on the newly established problem formulation, a fixed point iteration\nalgorithm is proposed, and its convergence is proved. Numerical results are\npresented to show that the proposed algorithm exhibits significant improved\nperformance in terms of multilinear rank learning and consequently tensor\nsignal recovery accuracy, compared to existing trace norm based Tucker\ncompletion methods.\n","authors":["Xueke Tong","Hancheng Zhu","Lei Cheng","Yik-Chung Wu"],"pdf_url":"https://arxiv.org/pdf/2409.05139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v3","updated":"2024-09-10T06:36:25Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking Neural Networks (SNNs) have emerged as a promising alternative to\nconventional Artificial Neural Networks (ANNs), demonstrating comparable\nperformance in both visual and linguistic tasks while offering the advantage of\nimproved energy efficiency. Despite these advancements, the integration of\nlinguistic and visual features into a unified representation through spike\ntrains poses a significant challenge, and the application of SNNs to multimodal\nscenarios remains largely unexplored. This paper presents SpikeCLIP, a novel\nframework designed to bridge the modality gap in spike-based computation. Our\napproach employs a two-step recipe: an ``alignment pre-training'' to align\nfeatures across modalities, followed by a ``dual-loss fine-tuning'' to refine\nthe model's performance. Extensive experiments reveal that SNNs achieve results\non par with ANNs while substantially reducing energy consumption across various\ndatasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP\nmaintains robust image classification capabilities, even when dealing with\nclasses that fall outside predefined categories. This study marks a significant\nadvancement in the development of energy-efficient and biologically plausible\nmultimodal learning systems.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Yufei Gu","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07202v6","updated":"2024-09-10T06:19:52Z","published":"2023-11-13T09:41:32Z","title":"Real-Time Machine-Learning-Based Optimization Using Input Convex Long\n  Short-Term Memory Network","summary":"  Neural network-based optimization and control methods, often referred to as\nblack-box approaches, are increasingly gaining attention in energy and\nmanufacturing systems, particularly in situations where first-principles models\nare either unavailable or inaccurate. However, their non-convex nature\nsignificantly slows down the optimization and control processes, limiting their\napplication in real-time decision-making processes. To address this challenge,\nwe propose a novel Input Convex Long Short-Term Memory (IC-LSTM) network to\nenhance the computational efficiency of neural network-based optimization.\nThrough two case studies employing real-time neural network-based optimization\nfor optimizing energy and chemical systems, we demonstrate the superior\nperformance of IC-LSTM-based optimization in terms of runtime. Specifically, in\na real-time optimization problem of a real-world solar photovoltaic energy\nsystem at LHT Holdings in Singapore, IC-LSTM-based optimization achieved at\nleast 4-fold speedup compared to conventional LSTM-based optimization. These\nresults highlight the potential of IC-LSTM networks to significantly enhance\nthe efficiency of neural network-based optimization and control in practical\napplications. Source code is available at\nhttps://github.com/killingbear999/ICLSTM.\n","authors":["Zihao Wang","Donghan Yu","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2311.07202v6.pdf","comment":"Applied Energy"},{"id":"http://arxiv.org/abs/2409.06241v1","updated":"2024-09-10T06:17:27Z","published":"2024-09-10T06:17:27Z","title":"DiPT: Enhancing LLM reasoning through diversified perspective-taking","summary":"  Existing work on improving language model reasoning typically explores a\nsingle solution path, which can be prone to errors. Inspired by\nperspective-taking in social studies, this paper introduces DiPT, a novel\napproach that complements current reasoning methods by explicitly incorporating\ndiversified viewpoints. This approach allows the model to gain a deeper\nunderstanding of the problem's context and identify the most effective solution\npath during the inference stage. Additionally, it provides a general\ndata-centric AI recipe for augmenting existing data to improve their quality\nfor fine-tuning.\n  Our empirical results demonstrate that DiPT can be flexibly integrated into\nexisting methods that focus on a single reasoning approach, enhancing their\nreasoning performance and stability when presented with paraphrased problems.\nFurthermore, we illustrate improved context understanding by maintaining the\nmodel's safe outputs against \"jailbreaking\" prompts intentionally designed to\nbypass safeguards built into deployed models. Lastly, we show that fine-tuning\nwith data enriched with diverse perspectives can boost the reasoning\ncapabilities of the model compared to fine-tuning with raw data alone.\n","authors":["Hoang Anh Just","Mahavir Dabas","Lifu Huang","Ming Jin","Ruoxi Jia"],"pdf_url":"https://arxiv.org/pdf/2409.06241v1.pdf","comment":"LLM Reasoning with Perspectives, Preprint"},{"id":"http://arxiv.org/abs/2402.02672v3","updated":"2024-09-10T06:17:16Z","published":"2024-02-05T02:17:21Z","title":"Estimation of conditional average treatment effects on distributed\n  confidential data","summary":"  Estimation of conditional average treatment effects (CATEs) is an important\ntopic in sciences. CATEs can be estimated with high accuracy if distributed\ndata across multiple parties can be centralized. However, it is difficult to\naggregate such data owing to confidential or privacy concerns. To address this\nissue, we proposed data collaboration double machine learning, a method that\ncan estimate CATE models from privacy-preserving fusion data constructed from\ndistributed data, and evaluated our method through simulations. Our\ncontributions are summarized in the following three points. First, our method\nenables estimation and testing of semi-parametric CATE models without iterative\ncommunication on distributed data. Our semi-parametric CATE method enable\nestimation and testing that is more robust to model mis-specification than\nparametric methods. Second, our method enables collaborative estimation between\nmultiple time points and different parties through the accumulation of a\nknowledge base. Third, our method performed equally or better than other\nmethods in simulations using synthetic, semi-synthetic and real-world datasets.\n","authors":["Yuji Kawamata","Ryoki Motai","Yukihiko Okada","Akira Imakura","Tetsuya Sakurai"],"pdf_url":"https://arxiv.org/pdf/2402.02672v3.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2409.06235v1","updated":"2024-09-10T06:07:20Z","published":"2024-09-10T06:07:20Z","title":"Recurrent Neural Networks for Still Images","summary":"  In this paper, we explore the application of Recurrent Neural Network (RNN)\nfor still images. Typically, Convolutional Neural Networks (CNNs) are the\nprevalent method applied for this type of data, and more recently, transformers\nhave gained popularity, although they often require large models. Unlike these\nmethods, RNNs are generally associated with processing sequences over time\nrather than single images. We argue that RNNs can effectively handle still\nimages by interpreting the pixels as a sequence. This approach could be\nparticularly advantageous for compact models designed for embedded systems,\nwhere resources are limited. Additionally, we introduce a novel RNN design\ntailored for two-dimensional inputs, such as images, and a custom version of\nBiDirectional RNN (BiRNN) that is more memory-efficient than traditional\nimplementations. In our research, we have tested these layers in Convolutional\nRecurrent Neural Networks (CRNNs), predominantly composed of Conv2D layers,\nwith RNN layers at or close to the end. Experiments on the COCO and CIFAR100\ndatasets show better results, particularly for small networks.\n","authors":[" Dmitri"," Lvov","Yair Smadar","Ran Bezen"],"pdf_url":"https://arxiv.org/pdf/2409.06235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04374v2","updated":"2024-09-10T05:51:18Z","published":"2024-09-06T16:13:04Z","title":"Gaussian-Mixture-Model Q-Functions for Reinforcement Learning by\n  Riemannian Optimization","summary":"  This paper establishes a novel role for Gaussian-mixture models (GMMs) as\nfunctional approximators of Q-function losses in reinforcement learning (RL).\nUnlike the existing RL literature, where GMMs play their typical role as\nestimates of probability density functions, GMMs approximate here Q-function\nlosses. The new Q-function approximators, coined GMM-QFs, are incorporated in\nBellman residuals to promote a Riemannian-optimization task as a novel\npolicy-evaluation step in standard policy-iteration schemes. The paper\ndemonstrates how the hyperparameters (means and covariance matrices) of the\nGaussian kernels are learned from the data, opening thus the door of RL to the\npowerful toolbox of Riemannian optimization. Numerical tests show that with no\nuse of experienced data, the proposed design outperforms state-of-the-art\nmethods, even deep Q-networks which use experienced data, on benchmark RL\ntasks.\n","authors":["Minh Vu","Konstantinos Slavakis"],"pdf_url":"https://arxiv.org/pdf/2409.04374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00785v2","updated":"2024-09-10T05:43:34Z","published":"2024-03-31T20:08:23Z","title":"Disentangling Hippocampal Shape Variations: A Study of Neurological\n  Disorders Using Mesh Variational Autoencoder with Contrastive Learning","summary":"  This paper presents a comprehensive study focused on disentangling\nhippocampal shape variations from diffusion tensor imaging (DTI) datasets\nwithin the context of neurological disorders. Leveraging a Graph Variational\nAutoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach\naims to improve interpretability by disentangling two distinct latent variables\ncorresponding to age and the presence of diseases. In our ablation study, we\ninvestigate a range of VAE architectures and contrastive loss functions,\nshowcasing the enhanced disentanglement capabilities of our approach. This\nevaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh\ndatasets derived from the DTI hippocampal dataset. Our supervised\ndisentanglement model outperforms several state-of-the-art (SOTA) methods like\nattribute and guided VAEs in terms of disentanglement scores. Our model\ndistinguishes between age groups and disease status in patients with Multiple\nSclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised\nContrastive Learning shows the volume changes of the hippocampus of MS\npopulations at different ages, and the result is consistent with the current\nneuroimaging literature. This research provides valuable insights into the\nrelationship between neurological disorder and hippocampal shape changes in\ndifferent age groups of MS populations using a Graph VAE with Supervised\nContrastive loss.\n","authors":["Jakaria Rabbi","Johannes Kiechle","Christian Beaulieu","Nilanjan Ray","Dana Cobzas"],"pdf_url":"https://arxiv.org/pdf/2404.00785v2.pdf","comment":"Length: 25 pages and submitted to the journal: MELBA (Machine\n  Learning for Biomedical Imaging)"},{"id":"http://arxiv.org/abs/2409.06226v1","updated":"2024-09-10T05:41:40Z","published":"2024-09-10T05:41:40Z","title":"NLP-Powered Repository and Search Engine for Academic Papers: A Case\n  Study on Cyber Risk Literature with CyLit","summary":"  As the body of academic literature continues to grow, researchers face\nincreasing difficulties in effectively searching for relevant resources.\nExisting databases and search engines often fall short of providing a\ncomprehensive and contextually relevant collection of academic literature. To\naddress this issue, we propose a novel framework that leverages Natural\nLanguage Processing (NLP) techniques. This framework automates the retrieval,\nsummarization, and clustering of academic literature within a specific research\ndomain. To demonstrate the effectiveness of our approach, we introduce CyLit,\nan NLP-powered repository specifically designed for the cyber risk literature.\nCyLit empowers researchers by providing access to context-specific resources\nand enabling the tracking of trends in the dynamic and rapidly evolving field\nof cyber risk. Through the automatic processing of large volumes of data, our\nNLP-powered solution significantly enhances the efficiency and specificity of\nacademic literature searches. We compare the literature categorization results\nof CyLit to those presented in survey papers or generated by ChatGPT,\nhighlighting the distinctive insights this tool provides into cyber risk\nresearch literature. Using NLP techniques, we aim to revolutionize the way\nresearchers discover, analyze, and utilize academic resources, ultimately\nfostering advancements in various domains of knowledge.\n","authors":["Linfeng Zhang","Changyue Hu","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2409.06226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06224v1","updated":"2024-09-10T05:28:38Z","published":"2024-09-10T05:28:38Z","title":"MIP-GAF: A MLLM-annotated Benchmark for Most Important Person\n  Localization and Group Context Understanding","summary":"  Estimating the Most Important Person (MIP) in any social event setup is a\nchallenging problem mainly due to contextual complexity and scarcity of labeled\ndata. Moreover, the causality aspects of MIP estimation are quite subjective\nand diverse. To this end, we aim to address the problem by annotating a\nlarge-scale `in-the-wild' dataset for identifying human perceptions about the\n`Most Important Person (MIP)' in an image. The paper provides a thorough\ndescription of our proposed Multimodal Large Language Model (MLLM) based data\nannotation strategy, and a thorough data quality analysis. Further, we perform\na comprehensive benchmarking of the proposed dataset utilizing state-of-the-art\nMIP localization methods, indicating a significant drop in performance compared\nto existing datasets. The performance drop shows that the existing MIP\nlocalization algorithms must be more robust with respect to `in-the-wild'\nsituations. We believe the proposed dataset will play a vital role in building\nthe next-generation social situation understanding methods. The code and data\nis available at https://github.com/surbhimadan92/MIP-GAF.\n","authors":["Surbhi Madan","Shreya Ghosh","Lownish Rai Sookha","M. A. Ganaie","Ramanathan Subramanian","Abhinav Dhall","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2409.06224v1.pdf","comment":"Accepted for publication at WACV 2025"},{"id":"http://arxiv.org/abs/2407.09096v3","updated":"2024-09-10T05:10:43Z","published":"2024-07-12T08:48:16Z","title":"STD-PLM: Understanding Both Spatial and Temporal Properties of\n  Spatial-Temporal Data with PLM","summary":"  Spatial-temporal forecasting and imputation are important for real-world\nintelligent systems. Most existing methods are tailored for individual\nforecasting or imputation tasks but are not designed for both. Additionally,\nthey are less effective for zero-shot and few-shot learning. While pre-trained\nlanguage model (PLM) have exhibited strong pattern recognition and reasoning\nabilities across various tasks, including few-shot and zero-shot learning,\ntheir applications in spatial-temporal data understanding has been constrained\nby insufficient modeling of complex correlations such as the temporal\ncorrelations, spatial connectivity, non-pairwise and high-order\nspatial-temporal correlations within data. In this paper, we propose STD-PLM\nfor understanding both spatial and temporal properties of\n\\underline{S}patial-\\underline{T}emporal \\underline{D}ata with \\underline{PLM},\nwhich is capable of implementing both spatial-temporal forecasting and\nimputation tasks. STD-PLM understands spatial-temporal correlations via\nexplicitly designed spatial and temporal tokenizers. Topology-aware node\nembeddings are designed for PLM to comprehend and exploit the topology\nstructure of data in inductive manner. Furthermore, to mitigate the efficiency\nissues introduced by the PLM, we design a sandglass attention module (SGA)\ncombined with a specific constrained loss function, which significantly\nimproves the model's efficiency while ensuring performance. Extensive\nexperiments demonstrate that STD-PLM exhibits competitive performance and\ngeneralization capabilities across the forecasting and imputation tasks on\nvarious datasets. Moreover, STD-PLM achieves promising results on both few-shot\nand zero-shot tasks.The code is made available at\n\\href{https://anonymous.4open.science/r/STD-PLM-F3BA}{https://anonymous.4open.science/r/STD-PLM-F3BA}\n","authors":["YiHeng Huang","Xiaowei Mao","Shengnan Guo","Yubin Chen","Junfeng Shen","Tiankuo Li","Youfang Lin","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2407.09096v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06219v1","updated":"2024-09-10T05:05:34Z","published":"2024-09-10T05:05:34Z","title":"Denoising: A Powerful Building-Block for Imaging, Inverse Problems, and\n  Machine Learning","summary":"  Denoising, the process of reducing random fluctuations in a signal to\nemphasize essential patterns, has been a fundamental problem of interest since\nthe dawn of modern scientific inquiry. Recent denoising techniques,\nparticularly in imaging, have achieved remarkable success, nearing theoretical\nlimits by some measures. Yet, despite tens of thousands of research papers, the\nwide-ranging applications of denoising beyond noise removal have not been fully\nrecognized. This is partly due to the vast and diverse literature, making a\nclear overview challenging.\n  This paper aims to address this gap. We present a comprehensive perspective\non denoisers, their structure, and desired properties. We emphasize the\nincreasing importance of denoising and showcase its evolution into an essential\nbuilding block for complex tasks in imaging, inverse problems, and machine\nlearning. Despite its long history, the community continues to uncover\nunexpected and groundbreaking uses for denoising, further solidifying its place\nas a cornerstone of scientific and engineering practice.\n","authors":["Peyman Milanfar","Mauricio Delbracio"],"pdf_url":"https://arxiv.org/pdf/2409.06219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07511v2","updated":"2024-09-10T04:52:17Z","published":"2023-10-11T14:07:05Z","title":"Learning a Cross-modality Anomaly Detector for Remote Sensing Imagery","summary":"  Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets for Earth monitoring. Given the diversity in\nearth anomaly types, designing a transferring model with cross-modality\ndetection ability should be cost-effective and flexible to new earth\nobservation sources and anomaly types. However, the current anomaly detectors\naim to learn the certain background distribution, the trained model cannot be\ntransferred to unseen images. Inspired by the fact that the deviation metric\nfor score ranking is consistent and independent from the image distribution,\nthis study exploits the learning target conversion from the varying background\ndistribution to the consistent deviation metric. We theoretically prove that\nthe large-margin condition in labeled samples ensures the transferring ability\nof learned deviation metric. To satisfy this condition, two large margin losses\nfor pixel-level and feature-level deviation ranking are proposed respectively.\nSince the real anomalies are difficult to acquire, anomaly simulation\nstrategies are designed to compute the model loss. With the large-margin\nlearning for deviation metric, the trained model achieves cross-modality\ndetection ability in five modalities including hyperspectral, visible light,\nsynthetic aperture radar (SAR), infrared and low-light in zero-shot manner.\n","authors":["Jingtao Li","Xinyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.07511v2.pdf","comment":"Journal paper"},{"id":"http://arxiv.org/abs/2209.07437v2","updated":"2024-09-10T04:45:49Z","published":"2022-09-15T16:33:38Z","title":"Mean-Field Approximation of Cooperative Constrained Multi-Agent\n  Reinforcement Learning (CMARL)","summary":"  Mean-Field Control (MFC) has recently been proven to be a scalable tool to\napproximately solve large-scale multi-agent reinforcement learning (MARL)\nproblems. However, these studies are typically limited to unconstrained\ncumulative reward maximization framework. In this paper, we show that one can\nuse the MFC approach to approximate the MARL problem even in the presence of\nconstraints. Specifically, we prove that, an $N$-agent constrained MARL\nproblem, with state, and action spaces of each individual agents being of sizes\n$|\\mathcal{X}|$, and $|\\mathcal{U}|$ respectively, can be approximated by an\nassociated constrained MFC problem with an error, $e\\triangleq\n\\mathcal{O}\\left([\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}]/\\sqrt{N}\\right)$.\nIn a special case where the reward, cost, and state transition functions are\nindependent of the action distribution of the population, we prove that the\nerror can be improved to $e=\\mathcal{O}(\\sqrt{|\\mathcal{X}|}/\\sqrt{N})$. Also,\nwe provide a Natural Policy Gradient based algorithm and prove that it can\nsolve the constrained MARL problem within an error of $\\mathcal{O}(e)$ with a\nsample complexity of $\\mathcal{O}(e^{-6})$.\n","authors":["Washim Uddin Mondal","Vaneet Aggarwal","Satish V. Ukkusuri"],"pdf_url":"https://arxiv.org/pdf/2209.07437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06211v1","updated":"2024-09-10T04:34:42Z","published":"2024-09-10T04:34:42Z","title":"STUN: Structured-Then-Unstructured Pruning for Scalable MoE Pruning","summary":"  Mixture-of-experts (MoEs) have been adopted for reducing inference costs by\nsparsely activating experts in Large language models (LLMs). Despite this\nreduction, the massive number of experts in MoEs still makes them expensive to\nserve. In this paper, we study how to address this, by pruning MoEs. Among\npruning methodologies, unstructured pruning has been known to achieve the\nhighest performance for a given pruning ratio, compared to structured pruning,\nsince the latter imposes constraints on the sparsification structure. This is\nintuitive, as the solution space of unstructured pruning subsumes that of\nstructured pruning. However, our counterintuitive finding reveals that expert\npruning, a form of structured pruning, can actually precede unstructured\npruning to outperform unstructured-only pruning. As existing expert pruning,\nrequiring $O(\\frac{k^n}{\\sqrt{n}})$ forward passes for $n$ experts, cannot\nscale for recent MoEs, we propose a scalable alternative with $O(1)$\ncomplexity, yet outperforming the more expensive methods. The key idea is\nleveraging a latent structure between experts, based on behavior similarity,\nsuch that the greedy decision of whether to prune closely captures the joint\npruning effect. Ours is highly effective -- for Snowflake Arctic, a 480B-sized\nMoE with 128 experts, our method needs only one H100 and two hours to achieve\nnearly no loss in performance with 40% sparsity, even in generative tasks such\nas GSM8K, where state-of-the-art unstructured pruning fails to. The code will\nbe made publicly available.\n","authors":["Jaeseong Lee","seung-won hwang","Aurick Qiao","Daniel F Campos","Zhewei Yao","Yuxiong He"],"pdf_url":"https://arxiv.org/pdf/2409.06211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06209v1","updated":"2024-09-10T04:29:59Z","published":"2024-09-10T04:29:59Z","title":"Adaptive Transformer Modelling of Density Function for Nonparametric\n  Survival Analysis","summary":"  Survival analysis holds a crucial role across diverse disciplines, such as\neconomics, engineering and healthcare. It empowers researchers to analyze both\ntime-invariant and time-varying data, encompassing phenomena like customer\nchurn, material degradation and various medical outcomes. Given the complexity\nand heterogeneity of such data, recent endeavors have demonstrated successful\nintegration of deep learning methodologies to address limitations in\nconventional statistical approaches. However, current methods typically involve\ncluttered probability distribution function (PDF), have lower sensitivity in\ncensoring prediction, only model static datasets, or only rely on recurrent\nneural networks for dynamic modelling. In this paper, we propose a novel\nsurvival regression method capable of producing high-quality unimodal PDFs\nwithout any prior distribution assumption, by optimizing novel\nMargin-Mean-Variance loss and leveraging the flexibility of Transformer to\nhandle both temporal and non-temporal data, coined UniSurv. Extensive\nexperiments on several datasets demonstrate that UniSurv places a significantly\nhigher emphasis on censoring compared to other methods.\n","authors":["Xin Zhang","Deval Mehta","Yanan Hu","Chao Zhu","David Darby","Zhen Yu","Daniel Merlo","Melissa Gresle","Anneke Van Der Walt","Helmut Butzkueven","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2409.06209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12391v2","updated":"2024-09-10T04:16:28Z","published":"2023-11-27T13:35:15Z","title":"vTrain: A Simulation Framework for Evaluating Cost-effective and\n  Compute-optimal Large Language Model Training","summary":"  As large language models (LLMs) become widespread in various application\ndomains, a critical challenge the AI community is facing is how to train these\nlarge AI models in a cost-effective manner. Existing LLM training plans\ntypically employ a heuristic based parallel training strategy which is based on\nempirical observations rather than grounded upon a thorough examination of the\nsearch space of LLM parallelization. Such limitation renders existing systems\nto leave significant performance left on the table, wasting millions of dollars\nworth of training cost. This paper presents our profiling-driven simulator\ncalled vTrain, providing AI practitioners a fast yet accurate software\nframework to determine an efficient and cost-effective LLM training system\nconfiguration. We demonstrate vTrain's practicality through several case\nstudies, e.g., effectively evaluating optimal training parallelization\nstrategies that balances training time and its associated training cost,\nefficient multi-tenant GPU cluster schedulers targeting multiple LLM training\njobs, and determining a compute-optimal LLM model architecture given a fixed\ncompute budget.\n","authors":["Jehyeon Bang","Yujeong Choi","Myeongwoo Kim","Yongdeok Kim","Minsoo Rhu"],"pdf_url":"https://arxiv.org/pdf/2312.12391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13011v4","updated":"2024-09-10T04:11:27Z","published":"2022-06-27T01:39:15Z","title":"Efficient Private SCO for Heavy-Tailed Data via Averaged Clipping","summary":"  We consider stochastic convex optimization for heavy-tailed data with the\nguarantee of being differentially private (DP). Most prior works on\ndifferentially private stochastic convex optimization for heavy-tailed data are\neither restricted to gradient descent (GD) or performed multi-times clipping on\nstochastic gradient descent (SGD), which is inefficient for large-scale\nproblems. In this paper, we consider a one-time clipping strategy and provide\nprincipled analyses of its bias and private mean estimation. We establish new\nconvergence results and improved complexity bounds for the proposed algorithm\ncalled AClipped-dpSGD for constrained and unconstrained convex problems. We\nalso extend our convergent analysis to the strongly convex case and non-smooth\ncase (which works for generalized smooth objectives with\nH$\\ddot{\\text{o}}$lder-continuous gradients). All the above results are\nguaranteed with a high probability for heavy-tailed data. Numerical experiments\nare conducted to justify the theoretical improvement.\n","authors":["Chenhan Jin","Kaiwen Zhou","Bo Han","James Cheng","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2206.13011v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06196v1","updated":"2024-09-10T03:57:21Z","published":"2024-09-10T03:57:21Z","title":"MTDA-HSED: Mutual-Assistance Tuning and Dual-Branch Aggregating for\n  Heterogeneous Sound Event Detection","summary":"  Sound Event Detection (SED) plays a vital role in comprehending and\nperceiving acoustic scenes. Previous methods have demonstrated impressive\ncapabilities. However, they are deficient in learning features of complex\nscenes from heterogeneous dataset. In this paper, we introduce a novel\ndual-branch architecture named Mutual-Assistance Tuning and Dual-Branch\nAggregating for Heterogeneous Sound Event Detection (MTDA-HSED). The MTDA-HSED\narchitecture employs the Mutual-Assistance Audio Adapter (M3A) to effectively\ntackle the multi-scenario problem and uses the Dual-Branch Mid-Fusion (DBMF)\nmodule to tackle the multi-granularity problem. Specifically, M3A is integrated\ninto the BEATs block as an adapter to improve the BEATs' performance by\nfine-tuning it on the multi-scenario dataset. The DBMF module connects BEATs\nand CNN branches, which facilitates the deep fusion of information from the\nBEATs and the CNN branches. Experimental results show that the proposed methods\nexceed the baseline of mpAUC by \\textbf{$5\\%$} on the DESED and MAESTRO Real\ndatasets. Code is \\href{https://github.com/Visitor-W/MTDA}{here}.\n","authors":["Zehao Wang","Haobo Yue","Zhicheng Zhang","Da Mu","Jin Tang","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2409.06196v1.pdf","comment":"Submit to Icassp2025"},{"id":"http://arxiv.org/abs/2409.06190v1","updated":"2024-09-10T03:41:10Z","published":"2024-09-10T03:41:10Z","title":"Multi-Source Music Generation with Latent Diffusion","summary":"  Most music generation models directly generate a single music mixture. To\nallow for more flexible and controllable generation, the Multi-Source Diffusion\nModel (MSDM) has been proposed to model music as a mixture of multiple\ninstrumental sources (e.g., piano, drums, bass, and guitar). Its goal is to use\none single diffusion model to generate consistent music sources, which are\nfurther mixed to form the music. Despite its capabilities, MSDM is unable to\ngenerate songs with rich melodies and often generates empty sounds. Also, its\nwaveform diffusion introduces significant Gaussian noise artifacts, which\ncompromises audio quality. In response, we introduce a multi-source latent\ndiffusion model (MSLDM) that employs Variational Autoencoders (VAEs) to encode\neach instrumental source into a distinct latent representation. By training a\nVAE on all music sources, we efficiently capture each source's unique\ncharacteristics in a source latent that our diffusion model models jointly.\nThis approach significantly enhances the total and partial generation of music\nby leveraging the VAE's latent compression and noise-robustness. The compressed\nsource latent also facilitates more efficient generation. Subjective listening\ntests and Frechet Audio Distance (FAD) scores confirm that our model\noutperforms MSDM, showcasing its practical and enhanced applicability in music\ngeneration systems. We also emphasize that modeling sources is more effective\nthan direct music mixture modeling. Codes and models are available at\nhttps://github.com/XZWY/MSLDM. Demos are available at\nhttps://xzwy.github.io/MSLDMDemo.\n","authors":["Zhongweiyang Xu","Debottam Dutta","Yu-Lin Wei","Romit Roy Choudhury"],"pdf_url":"https://arxiv.org/pdf/2409.06190v1.pdf","comment":"ICASSP 2025 in Submission"},{"id":"http://arxiv.org/abs/2311.04653v2","updated":"2024-09-10T03:38:37Z","published":"2023-11-08T12:53:07Z","title":"Hybrid Focal and Full-Range Attention Based Graph Transformers","summary":"  The paradigm of Transformers using the self-attention mechanism has\nmanifested its advantage in learning graph-structured data. Yet, Graph\nTransformers are capable of modeling full range dependencies but are often\ndeficient in extracting information from locality. A common practice is to\nutilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture\nlocal information, which however are still inadequate for comprehending\nsubstructures. In this paper, we present a purely attention-based architecture,\nnamely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the\nloss of local information in learning global correlations. The core component\nof FFGT is a new mechanism of compound attention, which combines the\nconventional full-range attention with K-hop focal attention on ego-nets to\naggregate both global and local information. Beyond the scope of canonical\nTransformers, the FFGT has the merit of being more substructure-aware. Our\napproach enhances the performance of existing Graph Transformers on various\nopen datasets, while achieves compatible SOTA performance on several Long-Range\nGraph Benchmark (LRGB) datasets even with a vanilla transformer. We further\nexamine influential factors on the optimal focal length of attention via\nintroducing a novel synthetic dataset based on SBM-PATTERN.\n","authors":["Minhong Zhu","Zhenhao Zhao","Weiran Cai"],"pdf_url":"https://arxiv.org/pdf/2311.04653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05273v2","updated":"2024-09-10T03:35:36Z","published":"2023-10-08T20:12:34Z","title":"Learning force laws in many-body systems","summary":"  Scientific laws describing natural systems may be more complex than our\nintuition can handle, thus how we discover laws must change. Machine learning\n(ML) models can analyze large quantities of data, but their structure should\nmatch the underlying physical constraints to provide useful insight. While\nprogress has been made using simulated data where the underlying physics is\nknown, training and validating ML models on experimental data requires\nfundamentally new approaches. Here we demonstrate and experimentally validate\nan ML approach that incorporates physical intuition to infer force laws in\ndusty plasma, a complex, many-body system. Trained on 3D particle trajectories,\nthe model accounts for inherent symmetries, non-identical particles, and learns\nthe effective non-reciprocal forces between particles with exquisite accuracy\n(R^2>0.99). We validate the model by inferring particle masses in two\nindependent yet consistent ways. The model's accuracy enables precise\nmeasurements of particle charge and screening length, discovering violations of\ncommon theoretical assumptions. Our ability to identify new physics from\nexperimental data demonstrates how ML-powered approaches can guide new routes\nof scientific discovery in many-body systems. Furthermore, we anticipate our ML\napproach to be a starting point for inferring laws from dynamics in a wide\nrange of many-body systems, from colloids to living organisms.\n","authors":["Wentao Yu","Eslam Abdelaleem","Ilya Nemenman","Justin C. Burton"],"pdf_url":"https://arxiv.org/pdf/2310.05273v2.pdf","comment":"14 pages, 4 Figures, 2 Supplemental Figures, 8 Supplemental Videos"},{"id":"http://arxiv.org/abs/2409.06187v1","updated":"2024-09-10T03:31:18Z","published":"2024-09-10T03:31:18Z","title":"Bottleneck-based Encoder-decoder ARchitecture (BEAR) for Learning\n  Unbiased Consumer-to-Consumer Image Representations","summary":"  Unbiased representation learning is still an object of study under specific\napplications and contexts. Novel architectures are usually crafted to resolve\nparticular problems using mixtures of fundamental pieces. This paper presents\ndifferent image feature extraction mechanisms that work together with residual\nconnections to encode perceptual image information in an autoencoder\nconfiguration. We use image data that aims to support a larger research agenda\ndealing with issues regarding criminal activity in consumer-to-consumer online\nplatforms. Preliminary results suggest that the proposed architecture can learn\nrich spaces using ours and other image datasets resolving important challenges\nthat are identified.\n","authors":["Pablo Rivas","Gisela Bichler","Tomas Cerny","Laurie Giddens","Stacie Petter"],"pdf_url":"https://arxiv.org/pdf/2409.06187v1.pdf","comment":"2022 LXAI Workshop at the 39th International Conference on Machine\n  Learning (ICML), Baltimore, Maryland"},{"id":"http://arxiv.org/abs/2409.06185v1","updated":"2024-09-10T03:26:42Z","published":"2024-09-10T03:26:42Z","title":"Can Large Language Models Unlock Novel Scientific Research Ideas?","summary":"  \"An idea is nothing more nor less than a new combination of old elements\"\n(Young, J.W.). The widespread adoption of Large Language Models (LLMs) and\npublicly available ChatGPT have marked a significant turning point in the\nintegration of Artificial Intelligence (AI) into people's everyday lives. This\nstudy explores the capability of LLMs in generating novel research ideas based\non information from research papers. We conduct a thorough examination of 4\nLLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and\nPhysics). We found that the future research ideas generated by Claude-2 and\nGPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini.\nWe also found that Claude-2 generates more diverse future research ideas than\nGPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the\nnovelty, relevancy, and feasibility of the generated future research ideas.\nThis investigation offers insights into the evolving role of LLMs in idea\ngeneration, highlighting both its capability and limitations. Our work\ncontributes to the ongoing efforts in evaluating and utilizing language models\nfor generating future research ideas. We make our datasets and codes publicly\navailable.\n","authors":["Sandeep Kumar","Tirthankar Ghosal","Vinayak Goyal","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2409.06185v1.pdf","comment":"24 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2310.10483v5","updated":"2024-09-10T03:15:54Z","published":"2023-10-16T15:03:55Z","title":"Passive Inference Attacks on Split Learning via Adversarial\n  Regularization","summary":"  Split Learning (SL) has emerged as a practical and efficient alternative to\ntraditional federated learning. While previous attempts to attack SL have often\nrelied on overly strong assumptions or targeted easily exploitable models, we\nseek to develop more capable attacks. We introduce SDAR, a novel attack\nframework against SL with an honest-but-curious server. SDAR leverages\nauxiliary data and adversarial regularization to learn a decodable simulator of\nthe client's private model, which can effectively infer the client's private\nfeatures under the vanilla SL, and both features and labels under the U-shaped\nSL. We perform extensive experiments in both configurations to validate the\neffectiveness of our proposed attacks. Notably, in challenging scenarios where\nexisting passive attacks struggle to reconstruct the client's private data\neffectively, SDAR consistently achieves significantly superior attack\nperformance, even comparable to active attacks. On CIFAR-10, at the deep split\nlevel of 7, SDAR achieves private feature reconstruction with less than 0.025\nmean squared error in both the vanilla and the U-shaped SL, and attains a label\ninference accuracy of over 98% in the U-shaped setting, while existing attacks\nfail to produce non-trivial results.\n","authors":["Xiaochen Zhu","Xinjian Luo","Yuncheng Wu","Yangfan Jiang","Xiaokui Xiao","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2310.10483v5.pdf","comment":"To appear at NDSS 2025; 25 pages, 27 figures"},{"id":"http://arxiv.org/abs/2407.01258v3","updated":"2024-09-10T03:07:19Z","published":"2024-07-01T13:08:09Z","title":"Physics-Inspired Deep Learning and Transferable Models for Bridge Scour\n  Prediction","summary":"  This paper introduces scour physics-inspired neural networks (SPINNs), a\nhybrid physics-data-driven framework for bridge scour prediction using deep\nlearning. SPINNs integrate physics-based, empirical equations into deep neural\nnetworks and are trained using site-specific historical scour monitoring data.\nLong-short Term Memory Network (LSTM) and Convolutional Neural Network (CNN)\nare considered as the base deep learning (DL) models. We also explore\ntransferable/general models, trained by aggregating datasets from a cluster of\nbridges, versus the site/bridge-specific models. Despite variation in\nperformance, SPINNs outperformed pure data-driven models in the majority of\ncases. In some bridge cases, SPINN reduced forecasting errors by up to 50\npercent. The pure data-driven models showed better transferability compared to\nhybrid models. The transferable DL models particularly proved effective for\nbridges with limited data. In addition, the calibrated time-dependent empirical\nequations derived from SPINNs showed great potential for maximum scour depth\nestimation, providing more accurate predictions compared to commonly used\nHEC-18 model. Comparing SPINNs with traditional empirical models indicates\nsubstantial improvements in scour prediction accuracy. This study can pave the\nway for further exploration of physics-inspired machine learning methods for\nscour prediction.\n","authors":["Negin Yousefpour","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2407.01258v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06171v1","updated":"2024-09-10T03:02:39Z","published":"2024-09-10T03:02:39Z","title":"Loss Distillation via Gradient Matching for Point Cloud Completion with\n  Weighted Chamfer Distance","summary":"  3D point clouds enhanced the robot's ability to perceive the geometrical\ninformation of the environments, making it possible for many downstream tasks\nsuch as grasp pose detection and scene understanding. The performance of these\ntasks, though, heavily relies on the quality of data input, as incomplete can\nlead to poor results and failure cases. Recent training loss functions designed\nfor deep learning-based point cloud completion, such as Chamfer distance (CD)\nand its variants (\\eg HyperCD ), imply a good gradient weighting scheme can\nsignificantly boost performance. However, these CD-based loss functions usually\nrequire data-related parameter tuning, which can be time-consuming for\ndata-extensive tasks. To address this issue, we aim to find a family of\nweighted training losses ({\\em weighted CD}) that requires no parameter tuning.\nTo this end, we propose a search scheme, {\\em Loss Distillation via Gradient\nMatching}, to find good candidate loss functions by mimicking the learning\nbehavior in backpropagation between HyperCD and weighted CD. Once this is done,\nwe propose a novel bilevel optimization formula to train the backbone network\nbased on the weighted CD loss. We observe that: (1) with proper weighted\nfunctions, the weighted CD can always achieve similar performance to HyperCD,\nand (2) the Landau weighted CD, namely {\\em Landau CD}, can outperform HyperCD\nfor point cloud completion and lead to new state-of-the-art results on several\nbenchmark datasets. {\\it Our demo code is available at\n\\url{https://github.com/Zhang-VISLab/IROS2024-LossDistillationWeightedCD}.}\n","authors":["Fangzhou Lin","Haotian Liu","Haoying Zhou","Songlin Hou","Kazunori D Yamada","Gregory S. Fischer","Yanhua Li","Haichong K. Zhang","Ziming Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06171v1.pdf","comment":"10 pages, 7 figures, 7 tables, this paper was accepted to IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2401.07250v2","updated":"2024-09-10T02:58:54Z","published":"2024-01-14T10:53:36Z","title":"Stabilizing Sharpness-aware Minimization Through A Simple\n  Renormalization Strategy","summary":"  Recently, sharpness-aware minimization (SAM) has attracted much attention\nbecause of its surprising effectiveness in improving generalization\nperformance. However, compared to stochastic gradient descent (SGD), it is more\nprone to getting stuck at the saddle points, which as a result may lead to\nperformance degradation. To address this issue, we propose a simple\nrenormalization strategy, dubbed Stable SAM (SSAM), so that the gradient norm\nof the descent step maintains the same as that of the ascent step. Our strategy\nis easy to implement and flexible enough to integrate with SAM and its\nvariants, almost at no computational cost. With elementary tools from convex\noptimization and learning theory, we also conduct a theoretical analysis of\nsharpness-aware training, revealing that compared to SGD, the effectiveness of\nSAM is only assured in a limited regime of learning rate. In contrast, we show\nhow SSAM extends this regime of learning rate and then it can consistently\nperform better than SAM with the minor modification. Finally, we demonstrate\nthe improved performance of SSAM on several representative data sets and tasks.\n","authors":["Chengli Tan","Jiangshe Zhang","Junmin Liu","Yicheng Wang","Yunda Hao"],"pdf_url":"https://arxiv.org/pdf/2401.07250v2.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2409.05208v2","updated":"2024-09-10T02:58:54Z","published":"2024-09-08T19:52:00Z","title":"Influence-based Attributions can be Manipulated","summary":"  Influence Functions are a standard tool for attributing predictions to\ntraining data in a principled manner and are widely used in applications such\nas data valuation and fairness. In this work, we present realistic incentives\nto manipulate influencebased attributions and investigate whether these\nattributions can be systematically tampered by an adversary. We show that this\nis indeed possible and provide efficient attacks with backward-friendly\nimplementations. Our work raises questions on the reliability of\ninfluence-based attributions under adversarial circumstances.\n","authors":["Chhavi Yadav","Ruihan Wu","Kamalika Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2409.05208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06169v1","updated":"2024-09-10T02:49:30Z","published":"2024-09-10T02:49:30Z","title":"VE: Modeling Multivariate Time Series Correlation with Variate Embedding","summary":"  Multivariate time series forecasting relies on accurately capturing the\ncorrelations among variates. Current channel-independent (CI) models and models\nwith a CI final projection layer are unable to capture these dependencies. In\nthis paper, we present the variate embedding (VE) pipeline, which learns a\nunique and consistent embedding for each variate and combines it with Mixture\nof Experts (MoE) and Low-Rank Adaptation (LoRA) techniques to enhance\nforecasting performance while controlling parameter size. The VE pipeline can\nbe integrated into any model with a CI final projection layer to improve\nmultivariate forecasting. The learned VE effectively groups variates with\nsimilar temporal patterns and separates those with low correlations. The\neffectiveness of the VE pipeline is demonstrated through extensive experiments\non four widely-used datasets. The code is available at:\n\\url{https://github.com/swang-song/VE}.\n","authors":["Shangjiong Wang","Zhihong Man","Zhengwei Cao","Jinchuan Zheng","Zhikang Ge"],"pdf_url":"https://arxiv.org/pdf/2409.06169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05484v2","updated":"2024-09-10T02:47:49Z","published":"2024-09-09T10:29:28Z","title":"CRADLE-VAE: Enhancing Single-Cell Gene Perturbation Modeling with\n  Counterfactual Reasoning-based Artifact Disentanglement","summary":"  Predicting cellular responses to various perturbations is a critical focus in\ndrug discovery and personalized therapeutics, with deep learning models playing\na significant role in this endeavor. Single-cell datasets contain technical\nartifacts that may hinder the predictability of such models, which poses\nquality control issues highly regarded in this area. To address this, we\npropose CRADLE-VAE, a causal generative framework tailored for single-cell gene\nperturbation modeling, enhanced with counterfactual reasoning-based artifact\ndisentanglement. Throughout training, CRADLE-VAE models the underlying latent\ndistribution of technical artifacts and perturbation effects present in\nsingle-cell datasets. It employs counterfactual reasoning to effectively\ndisentangle such artifacts by modulating the latent basal spaces and learns\nrobust features for generating cellular response data with improved quality.\nExperimental results demonstrate that this approach improves not only treatment\neffect estimation performance but also generative quality as well. The\nCRADLE-VAE codebase is publicly available at\nhttps://github.com/dmis-lab/CRADLE-VAE.\n","authors":["Seungheun Baek","Soyon Park","Yan Ting Chok","Junhyun Lee","Jueon Park","Mogan Gim","Jaewoo Kang"],"pdf_url":"https://arxiv.org/pdf/2409.05484v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01633v2","updated":"2024-09-10T02:39:25Z","published":"2024-09-03T06:04:39Z","title":"Dreaming is All You Need","summary":"  In classification tasks, achieving a harmonious balance between exploration\nand precision is of paramount importance. To this end, this research introduces\ntwo novel deep learning models, SleepNet and DreamNet, to strike this balance.\nSleepNet seamlessly integrates supervised learning with unsupervised ``sleep\"\nstages using pre-trained encoder models. Dedicated neurons within SleepNet are\nembedded in these unsupervised features, forming intermittent ``sleep\" blocks\nthat facilitate exploratory learning. Building upon the foundation of SleepNet,\nDreamNet employs full encoder-decoder frameworks to reconstruct the hidden\nstates, mimicking the human \"dreaming\" process. This reconstruction process\nenables further exploration and refinement of the learned representations.\nMoreover, the principle ideas of our SleepNet and DreamNet are generic and can\nbe applied to both computer vision and natural language processing downstream\ntasks. Through extensive empirical evaluations on diverse image and text\ndatasets, SleepNet and DreanNet have demonstrated superior performance compared\nto state-of-the-art models, showcasing the strengths of unsupervised\nexploration and supervised precision afforded by our innovative approaches.\n","authors":["Mingze Ni","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01633v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06163v1","updated":"2024-09-10T02:21:29Z","published":"2024-09-10T02:21:29Z","title":"MCDGLN: Masked Connection-based Dynamic Graph Learning Network for\n  Autism Spectrum Disorder","summary":"  Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized\nby complex physiological processes. Previous research has predominantly focused\non static cerebral interactions, often neglecting the brain's dynamic nature\nand the challenges posed by network noise. To address these gaps, we introduce\nthe Masked Connection-based Dynamic Graph Learning Network (MCDGLN). Our\napproach first segments BOLD signals using sliding temporal windows to capture\ndynamic brain characteristics. We then employ a specialized weighted edge\naggregation (WEA) module, which uses the cross convolution with channel-wise\nelement-wise convolutional kernel, to integrate dynamic functional connectivity\nand to isolating task-relevant connections. This is followed by topological\nfeature extraction via a hierarchical graph convolutional network (HGCN), with\nkey attributes highlighted by a self-attention module. Crucially, we refine\nstatic functional connections using a customized task-specific mask, reducing\nnoise and pruning irrelevant links. The attention-based connection encoder\n(ACE) then enhances critical connections and compresses static features. The\ncombined features are subsequently used for classification. Applied to the\nAutism Brain Imaging Data Exchange I (ABIDE I) dataset, our framework achieves\na 73.3\\% classification accuracy between ASD and Typical Control (TC) groups\namong 1,035 subjects. The pivotal roles of WEA and ACE in refining connectivity\nand enhancing classification accuracy underscore their importance in capturing\nASD-specific features, offering new insights into the disorder.\n","authors":["Peng Wang","Xin Wen","Ruochen Cao","Chengxin Gao","Yanrong Hao","Rui Cao"],"pdf_url":"https://arxiv.org/pdf/2409.06163v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.09895v3","updated":"2024-09-10T02:12:29Z","published":"2024-08-19T11:09:12Z","title":"Performance Law of Large Language Models","summary":"  Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.\n","authors":["Chuhan Wu","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2408.09895v3.pdf","comment":"Personal opinions of the authors"},{"id":"http://arxiv.org/abs/2409.06157v1","updated":"2024-09-10T02:07:39Z","published":"2024-09-10T02:07:39Z","title":"Causal Analysis of Shapley Values: Conditional vs. Marginal","summary":"  Shapley values, a game theoretic concept, has been one of the most popular\ntools for explaining Machine Learning (ML) models in recent years.\nUnfortunately, the two most common approaches, conditional and marginal, to\ncalculating Shapley values can lead to different results along with some\nundesirable side effects when features are correlated. This in turn has led to\nthe situation in the literature where contradictory recommendations regarding\nchoice of an approach are provided by different authors. In this paper we aim\nto resolve this controversy through the use of causal arguments. We show that\nthe differences arise from the implicit assumptions that are made within each\nmethod to deal with missing causal information. We also demonstrate that the\nconditional approach is fundamentally unsound from a causal perspective. This,\ntogether with previous work in [1], leads to the conclusion that the marginal\napproach should be preferred over the conditional one.\n","authors":["Ilya Rozenfeld"],"pdf_url":"https://arxiv.org/pdf/2409.06157v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.04825v2","updated":"2024-09-10T01:52:47Z","published":"2024-05-08T05:49:46Z","title":"Explanation as a Watermark: Towards Harmless and Multi-bit Model\n  Ownership Verification via Watermarking Feature Attribution","summary":"  Ownership verification is currently the most critical and widely adopted\npost-hoc method to safeguard model copyright. In general, model owners exploit\nit to identify whether a given suspicious third-party model is stolen from them\nby examining whether it has particular properties `inherited' from their\nreleased models. Currently, backdoor-based model watermarks are the primary and\ncutting-edge methods to implant such properties in the released models.\nHowever, backdoor-based methods have two fatal drawbacks, including harmfulness\nand ambiguity. The former indicates that they introduce maliciously\ncontrollable misclassification behaviors ($i.e.$, backdoor) to the watermarked\nreleased models. The latter denotes that malicious users can easily pass the\nverification by finding other misclassified samples, leading to ownership\nambiguity.\n  In this paper, we argue that both limitations stem from the `zero-bit' nature\nof existing watermarking schemes, where they exploit the status ($i.e.$,\nmisclassified) of predictions for verification. Motivated by this\nunderstanding, we design a new watermarking paradigm, $i.e.$, Explanation as a\nWatermark (EaaW), that implants verification behaviors into the explanation of\nfeature attribution instead of model predictions. Specifically, EaaW embeds a\n`multi-bit' watermark into the feature attribution explanation of specific\ntrigger samples without changing the original prediction. We correspondingly\ndesign the watermark embedding and extraction algorithms inspired by\nexplainable artificial intelligence. In particular, our approach can be used\nfor different tasks ($e.g.$, image classification and text generation).\nExtensive experiments verify the effectiveness and harmlessness of our EaaW and\nits resistance to potential attacks.\n","authors":["Shuo Shao","Yiming Li","Hongwei Yao","Yiling He","Zhan Qin","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2405.04825v2.pdf","comment":"This paper is accepted by Network and Distributed System Security\n  Symposium (NDSS) 2025"},{"id":"http://arxiv.org/abs/2409.06146v1","updated":"2024-09-10T01:42:10Z","published":"2024-09-10T01:42:10Z","title":"Configuration Interaction Guided Sampling with Interpretable Restricted\n  Boltzmann Machine","summary":"  We propose a data-driven approach using a Restricted Boltzmann Machine (RBM)\nto solve the Schr\\\"odinger equation in configuration space. Traditional\nConfiguration Interaction (CI) methods, while powerful, are computationally\nexpensive due to the large number of determinants required. Our approach\nleverages RBMs to efficiently identify and sample the most significant\ndeterminants, accelerating convergence and reducing computational cost. This\nmethod achieves up to 99.99\\% of the correlation energy even by four orders of\nmagnitude less determinants compared to full CI calculations and up to two\norders of magnitude less than previous state of the art works. Additionally,\nour study demonstrate that the RBM can learn the underlying quantum properties,\nproviding more detail insights than other methods . This innovative data-driven\napproach offers a promising tool for quantum chemistry, enhancing both\nefficiency and understanding of complex systems.\n","authors":["Jorge I. Hernandez-Martinez","Gerardo Rodriguez-Hernandez","Andres Mendez-Vazquez"],"pdf_url":"https://arxiv.org/pdf/2409.06146v1.pdf","comment":"Preprint to be submitted to Computer Physics Communications"},{"id":"http://arxiv.org/abs/2409.06142v1","updated":"2024-09-10T01:33:31Z","published":"2024-09-10T01:33:31Z","title":"Variational Search Distributions","summary":"  We develop variational search distributions (VSD), a method for finding\ndiscrete, combinatorial designs of a rare desired class in a batch sequential\nmanner with a fixed experimental budget. We formalize the requirements and\ndesiderata for this problem and formulate a solution via variational inference\nthat fulfill these. In particular, VSD uses off-the-shelf gradient based\noptimization routines, and can take advantage of scalable predictive models. We\nshow that VSD can outperform existing baseline methods on a set of real\nsequence-design problems in various biological systems.\n","authors":["Daniel M. Steinberg","Rafael Oliveira","Cheng Soon Ong","Edwin V. Bonilla"],"pdf_url":"https://arxiv.org/pdf/2409.06142v1.pdf","comment":"16 pages, 5 figures, Appendix material included"},{"id":"http://arxiv.org/abs/2408.01215v2","updated":"2024-09-10T01:06:31Z","published":"2024-08-02T12:04:19Z","title":"ZNorm: Z-Score Gradient Normalization for Deep Neural Networks","summary":"  The rapid advancements in deep learning necessitate better training methods\nfor deep neural networks (DNNs). As models grow in complexity, vanishing and\nexploding gradients impede performance. We propose Z-Score Normalization for\nGradient Descent (ZNorm), an innovative technique that adjusts only the\ngradients to accelerate training and improve model performance. ZNorm\nnormalizes the overall gradients, providing consistent gradient scaling across\nlayers, thereby reducing the risks of vanishing and exploding gradients, having\nbetter performances. Our extensive experiments on CIFAR-10 and medical datasets\ndemonstrate that ZNorm enhances performance metrics. ZNorm consistently\noutperforms existing methods, achieving superior results using the same\nexperimental settings. In medical imaging applications, ZNorm improves tumor\nprediction and segmentation performances, underscoring its practical utility.\nThese findings highlight ZNorm's potential as a robust and versatile tool for\nenhancing the training speed and effectiveness of deep neural networks across a\nwide range of architectures and applications.\n","authors":["Juyoung Yun","Hoyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06129v1","updated":"2024-09-10T00:51:49Z","published":"2024-09-10T00:51:49Z","title":"DECOLLAGE: 3D Detailization by Controllable, Localized, and Learned\n  Geometry Enhancement","summary":"  We present a 3D modeling method which enables end-users to refine or\ndetailize 3D shapes using machine learning, expanding the capabilities of\nAI-assisted 3D content creation. Given a coarse voxel shape (e.g., one produced\nwith a simple box extrusion tool or via generative modeling), a user can\ndirectly \"paint\" desired target styles representing compelling geometric\ndetails, from input exemplar shapes, over different regions of the coarse\nshape. These regions are then up-sampled into high-resolution geometries which\nadhere with the painted styles. To achieve such controllable and localized 3D\ndetailization, we build on top of a Pyramid GAN by making it masking-aware. We\ndevise novel structural losses and priors to ensure that our method preserves\nboth desired coarse structures and fine-grained features even if the painted\nstyles are borrowed from diverse sources, e.g., different semantic parts and\neven different shape categories. Through extensive experiments, we show that\nour ability to localize details enables novel interactive creative workflows\nand applications. Our experiments further demonstrate that in comparison to\nprior techniques built on global detailization, our method generates\nstructure-preserving, high-resolution stylized geometries with more coherent\nshape details and style transitions.\n","authors":["Qimin Chen","Zhiqin Chen","Vladimir G. Kim","Noam Aigerman","Hao Zhang","Siddhartha Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2409.06129v1.pdf","comment":"ECCV 2024 (poster). Code: https://qiminchen.github.io/decollage/"},{"id":"http://arxiv.org/abs/2409.06123v1","updated":"2024-09-10T00:24:59Z","published":"2024-09-10T00:24:59Z","title":"Contrastive Federated Learning with Tabular Data Silos","summary":"  Learning from data silos is a difficult task for organizations that need to\nobtain knowledge of objects that appeared in multiple independent data silos.\nObjects in multi-organizations, such as government agents, are referred by\ndifferent identifiers, such as driver license, passport number, and tax file\nnumber. The data distributions in data silos are mostly non-IID (Independently\nand Identically Distributed), labelless, and vertically partitioned (i.e.,\nhaving different attributes). Privacy concerns harden the above issues.\nConditions inhibit enthusiasm for collaborative work. While Federated Learning\n(FL) has been proposed to address these issues, the difficulty of labeling,\nnamely, label costliness, often hinders optimal model performance. A potential\nsolution lies in contrastive learning, an unsupervised self-learning technique\nto represent semantic data by contrasting similar data pairs. However,\ncontrastive learning is currently not designed to handle tabular data silos\nthat existed within multiple organizations where data linkage by quasi\nidentifiers are needed. To address these challenges, we propose using\nsemi-supervised contrastive federated learning, which we refer to as\nContrastive Federated Learning with Data Silos (CFL). Our approach tackles the\naforementioned issues with an integrated solution. Our experimental results\ndemonstrate that CFL outperforms current methods in addressing these challenges\nand providing improvements in accuracy. Additionally, we present positive\nresults that showcase the advantages of our contrastive federated learning\napproach in complex client environments.\n","authors":["Achmad Ginanjar","Xue Li","Wen Hua"],"pdf_url":"https://arxiv.org/pdf/2409.06123v1.pdf","comment":"18 Pages. Was submitted on Artificial Intelligence Journal, Jan 29,\n  2024, ARTINT-D-24-00098"},{"id":"http://arxiv.org/abs/2402.02636v2","updated":"2024-09-10T00:18:02Z","published":"2024-02-04T23:04:02Z","title":"Can Large Language Models Learn Independent Causal Mechanisms?","summary":"  Despite impressive performance on language modelling and complex reasoning\ntasks, Large Language Models (LLMs) fall short on the same tasks in uncommon\nsettings or with distribution shifts, exhibiting a lack of generalisation\nability. By contrast, systems such as causal models, that learn abstract\nvariables and causal relationships, can demonstrate increased robustness\nagainst changes in the distribution. One reason for this success is the\nexistence and use of Independent Causal Mechanisms (ICMs) representing\nhigh-level concepts that only sparsely interact. In this work, we apply two\nconcepts from causality to learn ICMs within LLMs. We develop a new LLM\narchitecture composed of multiple sparsely interacting language modelling\nmodules. We show that such causal constraints can improve out-of-distribution\nperformance on abstract and causal reasoning tasks. We also investigate the\nlevel of independence and domain specialisation and show that LLMs rely on\npre-trained partially domain-invariant mechanisms resilient to fine-tuning.\n","authors":["Gaël Gendron","Bao Trung Nguyen","Alex Yuxuan Peng","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2402.02636v2.pdf","comment":"20 pages, 7 pages for the main paper and 13 pages for references and\n  appendices, 17 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.06690v1","updated":"2024-09-10T17:54:00Z","published":"2024-09-10T17:54:00Z","title":"Benchmarking Sub-Genre Classification For Mainstage Dance Music","summary":"  Music classification, with a wide range of applications, is one of the most\nprominent tasks in music information retrieval. To address the absence of\ncomprehensive datasets and high-performing methods in the classification of\nmainstage dance music, this work introduces a novel benchmark comprising a new\ndataset and a baseline. Our dataset extends the number of sub-genres to cover\nmost recent mainstage live sets by top DJs worldwide in music festivals. A\ncontinuous soft labeling approach is employed to account for tracks that span\nmultiple sub-genres, preserving the inherent sophistication. For the baseline,\nwe developed deep learning models that outperform current state-of-the-art\nmultimodel language models, which struggle to identify house music sub-genres,\nemphasizing the need for specialized models trained on fine-grained datasets.\nOur benchmark is applicable to serve for application scenarios such as music\nrecommendation, DJ set curation, and interactive multimedia, where we also\nprovide video demos. Our code is on\n\\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.\n","authors":["Hongzhi Shu","Xinglin Li","Hongyu Jiang","Minghao Fu","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2409.06690v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2404.09654v2","updated":"2024-09-10T11:58:23Z","published":"2024-04-15T10:42:22Z","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection","summary":"  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Fang Deng","Beng Chin Ooi","Junran Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09654v2.pdf","comment":"Accepted by MM'24 (Oral)"},{"id":"http://arxiv.org/abs/2409.06371v1","updated":"2024-09-10T09:53:06Z","published":"2024-09-10T09:53:06Z","title":"Distilling Generative-Discriminative Representations for Very\n  Low-Resolution Face Recognition","summary":"  Very low-resolution face recognition is challenging due to the serious loss\nof informative facial details in resolution degradation. In this paper, we\npropose a generative-discriminative representation distillation approach that\ncombines generative representation with cross-resolution aligned knowledge\ndistillation. This approach facilitates very low-resolution face recognition by\njointly distilling generative and discriminative models via two distillation\nmodules. Firstly, the generative representation distillation takes the encoder\nof a diffusion model pretrained for face super-resolution as the generative\nteacher to supervise the learning of the student backbone via feature\nregression, and then freezes the student backbone. After that, the\ndiscriminative representation distillation further considers a pretrained face\nrecognizer as the discriminative teacher to supervise the learning of the\nstudent head via cross-resolution relational contrastive distillation. In this\nway, the general backbone representation can be transformed into discriminative\nhead representation, leading to a robust and discriminative student model for\nvery low-resolution face recognition. Our approach improves the recovery of the\nmissing details in very low-resolution faces and achieves better knowledge\ntransfer. Extensive experiments on face datasets demonstrate that our approach\nenhances the recognition accuracy of very low-resolution faces, showcasing its\neffectiveness and adaptability.\n","authors":["Junzheng Zhang","Weijia Guo","Bochao Liu","Ruixin Shi","Yong Li","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2409.06371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04388v3","updated":"2024-09-10T09:46:58Z","published":"2024-09-06T16:27:52Z","title":"Question-Answering Dense Video Events","summary":"  Multimodal Large Language Models (MLLMs) have shown excellent performance in\nquestion-answering of single-event videos. In this paper, we present\nquestion-answering dense video events, a novel task that requires answering and\ngrounding the dense-event questions in long videos, thus challenging MLLMs to\nfaithfully comprehend and reason about multiple events occurring over extended\ntime periods. To facilitate the study, we construct DeVE-QA - a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. We then\nbenchmark and show that existing MLLMs excelling at single-event QA struggle to\nperform well in DeVE-QA. For improvement, we propose DeVi, a novel\ntraining-free MLLM approach that highlights a hierarchical captioning module, a\ntemporal event memory module, and a self-consistency checking module to\nrespectively detect, contextualize and memorize, and ground dense-events in\nlong videos for question answering. Extensive experiments show that DeVi is\nsuperior at answering dense-event questions and grounding relevant video\nmoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1\npercent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA\nrespectively.\n","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2409.04388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02453v2","updated":"2024-09-10T08:20:36Z","published":"2024-09-04T05:19:57Z","title":"FrameCorr: Adaptive, Autoencoder-based Neural Compression for Video\n  Reconstruction in Resource and Timing Constrained Network Settings","summary":"  Despite the growing adoption of video processing via Internet of Things (IoT)\ndevices due to their cost-effectiveness, transmitting captured data to nearby\nservers poses challenges due to varying timing constraints and scarcity of\nnetwork bandwidth. Existing video compression methods face difficulties in\nrecovering compressed data when incomplete data is provided. Here, we introduce\nFrameCorr, a deep-learning based solution that utilizes previously received\ndata to predict the missing segments of a frame, enabling the reconstruction of\na frame from partially received data.\n","authors":["John Li","Shehab Sarar Ahmed","Deepak Nair"],"pdf_url":"https://arxiv.org/pdf/2409.02453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08564v2","updated":"2024-09-10T07:30:02Z","published":"2024-06-12T18:07:06Z","title":"Machine Learning-Driven Open-Source Framework for Assessing QoE in\n  Multimedia Networks","summary":"  The Internet is integral to modern life, influencing communication, business,\nand lifestyles globally. As dependence on Internet services grows, the demand\nfor high-quality service delivery increases. Service providers must maintain\nhigh standards of quality of service and quality of experience (QoE) to ensure\nuser satisfaction. QoE, which reflects user satisfaction with service quality,\nis a key metric for multimedia services, yet it is challenging to measure due\nto its subjective nature and the complexities of real-time feedback. This paper\nintroduces a machine learning-based framework for objectively assessing QoE in\nmultimedia networks. The open-source framework complies with the ITU-T P.1203\nstandard. It automates data collection and user satisfaction prediction using\nkey network parameters such as delay, jitter, packet loss, bitrate, and\nthroughput. Using a dataset of over 20,000 records from various network\nconditions, the Random Forest model predicts the mean opinion score with 95.8%\naccuracy. Our framework addresses the limitations of existing QoE models by\nintegrating real-time data collection, machine learning predictions, and\nadherence to international standards. This approach enhances QoE evaluation\naccuracy and allows dynamic network resource management, optimizing performance\nand cost-efficiency. Its open-source nature encourages adaptation and extension\nfor various multimedia services. The findings significantly affect the\ntelecommunications industry in managing and optimizing multimedia services. The\nnetwork centric QoE prediction of the framework offers a scalable solution to\nimprove user satisfaction without the need for content-specific data. Future\nenhancements could include advanced machine learning models and broader\napplicability to digital services. This research contributes a practical,\nstandardized tool for QoE assessment across diverse networks and platforms.\n","authors":["Parsa Hassani Shariat Panahi","Amir Hossein Jalilvand","Abolfazl Diyanat"],"pdf_url":"https://arxiv.org/pdf/2406.08564v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.06224v1","updated":"2024-09-10T05:28:38Z","published":"2024-09-10T05:28:38Z","title":"MIP-GAF: A MLLM-annotated Benchmark for Most Important Person\n  Localization and Group Context Understanding","summary":"  Estimating the Most Important Person (MIP) in any social event setup is a\nchallenging problem mainly due to contextual complexity and scarcity of labeled\ndata. Moreover, the causality aspects of MIP estimation are quite subjective\nand diverse. To this end, we aim to address the problem by annotating a\nlarge-scale `in-the-wild' dataset for identifying human perceptions about the\n`Most Important Person (MIP)' in an image. The paper provides a thorough\ndescription of our proposed Multimodal Large Language Model (MLLM) based data\nannotation strategy, and a thorough data quality analysis. Further, we perform\na comprehensive benchmarking of the proposed dataset utilizing state-of-the-art\nMIP localization methods, indicating a significant drop in performance compared\nto existing datasets. The performance drop shows that the existing MIP\nlocalization algorithms must be more robust with respect to `in-the-wild'\nsituations. We believe the proposed dataset will play a vital role in building\nthe next-generation social situation understanding methods. The code and data\nis available at https://github.com/surbhimadan92/MIP-GAF.\n","authors":["Surbhi Madan","Shreya Ghosh","Lownish Rai Sookha","M. A. Ganaie","Ramanathan Subramanian","Abhinav Dhall","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2409.06224v1.pdf","comment":"Accepted for publication at WACV 2025"},{"id":"http://arxiv.org/abs/2409.06207v1","updated":"2024-09-10T04:24:22Z","published":"2024-09-10T04:24:22Z","title":"Design and Implementation of Online Live Streaming System Using A 3D\n  Engine","summary":"  With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.\n","authors":["Aizierjiang Aiersilan"],"pdf_url":"https://arxiv.org/pdf/2409.06207v1.pdf","comment":null}]},"2024-09-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.05618v2","updated":"2024-09-09T23:54:35Z","published":"2024-01-11T01:52:25Z","title":"The Benefits of a Concise Chain of Thought on Problem-Solving in Large\n  Language Models","summary":"  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We\ncompared standard CoT and CCoT prompts to see how conciseness impacts response\nlength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4\nwith a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced\naverage response length by 48.70% for both GPT-3.5 and GPT-4 while having a\nnegligible impact on problem-solving performance. However, on math problems,\nGPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads\nto an average per-token cost reduction of 22.67%.\n","authors":["Matthew Renze","Erhan Guven"],"pdf_url":"https://arxiv.org/pdf/2401.05618v2.pdf","comment":"All code, data, and supplemental materials are available on GitHub at\n  https://github.com/matthewrenze/jhu-concise-cot"},{"id":"http://arxiv.org/abs/2409.06109v1","updated":"2024-09-09T23:31:56Z","published":"2024-09-09T23:31:56Z","title":"Estimating the Completeness of Discrete Speech Units","summary":"  Representing speech with discrete units has been widely used in speech codec\nand speech generation. However, there are several unverified claims about\nself-supervised discrete units, such as disentangling phonetic and speaker\ninformation with k-means, or assuming information loss after k-means. In this\nwork, we take an information-theoretic perspective to answer how much\ninformation is present (information completeness) and how much information is\naccessible (information accessibility), before and after residual vector\nquantization. We show a lower bound for information completeness and estimate\ncompleteness on discretized HuBERT representations after residual vector\nquantization. We find that speaker information is sufficiently present in\nHuBERT discrete units, and that phonetic information is sufficiently present in\nthe residual, showing that vector quantization does not achieve\ndisentanglement. Our results offer a comprehensive assessment on the choice of\ndiscrete units, and suggest that a lot more information in the residual should\nbe mined rather than discarded.\n","authors":["Sung-Lin Yeh","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2409.06109v1.pdf","comment":"SLT2024"},{"id":"http://arxiv.org/abs/2409.06107v1","updated":"2024-09-09T23:22:27Z","published":"2024-09-09T23:22:27Z","title":"Doppelgänger's Watch: A Split Objective Approach to Large Language\n  Models","summary":"  In this paper, we investigate the problem of \"generation supervision\" in\nlarge language models, and present a novel bicameral architecture to separate\nsupervision signals from their core capability, helpfulness. Doppelg\\\"anger, a\nnew module parallel to the underlying language model, supervises the generation\nof each token, and learns to concurrently predict the supervision score(s) of\nthe sequences up to and including each token. In this work, we present the\ntheoretical findings, and leave the report on experimental results to a\nforthcoming publication.\n","authors":["Shervin Ghasemlou","Ashish Katiyar","Aparajita Saraf","Seungwhan Moon","Mangesh Pujari","Pinar Donmez","Babak Damavandi","Anuj Kumar"],"pdf_url":"https://arxiv.org/pdf/2409.06107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06097v1","updated":"2024-09-09T22:29:35Z","published":"2024-09-09T22:29:35Z","title":"ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information\n  in Task-Oriented Dialog","summary":"  We introduce ClarQ-LLM, an evaluation framework consisting of bilingual\nEnglish-Chinese conversation tasks, conversational agents and evaluation\nmetrics, designed to serve as a strong benchmark for assessing agents' ability\nto ask clarification questions in task-oriented dialogues. The benchmark\nincludes 31 different task types, each with 10 unique dialogue scenarios\nbetween information seeker and provider agents. The scenarios require the\nseeker to ask questions to resolve uncertainty and gather necessary information\nto complete tasks. Unlike traditional benchmarks that evaluate agents based on\nfixed dialogue content, ClarQ-LLM includes a provider conversational agent to\nreplicate the original human provider in the benchmark. This allows both\ncurrent and future seeker agents to test their ability to complete information\ngathering tasks through dialogue by directly interacting with our provider\nagent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of\nonly 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future\nresearch.\n","authors":["Yujian Gan","Changling Li","Jinxia Xie","Luou Wen","Matthew Purver","Massimo Poesio"],"pdf_url":"https://arxiv.org/pdf/2409.06097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06072v1","updated":"2024-09-09T21:12:03Z","published":"2024-09-09T21:12:03Z","title":"DetoxBench: Benchmarking Large Language Models for Multitask Fraud &\n  Abuse Detection","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks. However, their practical application in\nhigh-stake domains, such as fraud and abuse detection, remains an area that\nrequires further exploration. The existing applications often narrowly focus on\nspecific tasks like toxicity or hate speech detection. In this paper, we\npresent a comprehensive benchmark suite designed to assess the performance of\nLLMs in identifying and mitigating fraudulent and abusive language across\nvarious real-world scenarios. Our benchmark encompasses a diverse set of tasks,\nincluding detecting spam emails, hate speech, misogynistic language, and more.\nWe evaluated several state-of-the-art LLMs, including models from Anthropic,\nMistral AI, and the AI21 family, to provide a comprehensive assessment of their\ncapabilities in this critical domain. The results indicate that while LLMs\nexhibit proficient baseline performance in individual fraud and abuse detection\ntasks, their performance varies considerably across tasks, particularly\nstruggling with tasks that demand nuanced pragmatic reasoning, such as\nidentifying diverse forms of misogynistic language. These findings have\nimportant implications for the responsible development and deployment of LLMs\nin high-risk applications. Our benchmark suite can serve as a tool for\nresearchers and practitioners to systematically evaluate LLMs for multi-task\nfraud detection and drive the creation of more robust, trustworthy, and\nethically-aligned systems for fraud and abuse detection.\n","authors":["Joymallya Chakraborty","Wei Xia","Anirban Majumder","Dan Ma","Walid Chaabene","Naveed Janvekar"],"pdf_url":"https://arxiv.org/pdf/2409.06072v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2211.03730v2","updated":"2024-09-09T21:04:05Z","published":"2022-11-07T17:59:05Z","title":"A transformer-based spelling error correction framework for Bangla and\n  resource scarce Indic languages","summary":"  Spelling error correction is the task of identifying and rectifying\nmisspelled words in texts. It is a potential and active research topic in\nNatural Language Processing because of numerous applications in human language\nunderstanding. The phonetically or visually similar yet semantically distinct\ncharacters make it an arduous task in any language. Earlier efforts on spelling\nerror correction in Bangla and resource-scarce Indic languages focused on\nrule-based, statistical, and machine learning-based methods which we found\nrather inefficient. In particular, machine learning-based approaches, which\nexhibit superior performance to rule-based and statistical methods, are\nineffective as they correct each character regardless of its appropriateness.\nIn this paper, we propose a novel detector-purificator-corrector framework,\nDPCSpell based on denoising transformers by addressing previous issues. In\naddition to that, we present a method for large-scale corpus creation from\nscratch which in turn resolves the resource limitation problem of any\nleft-to-right scripted language. The empirical outcomes demonstrate the\neffectiveness of our approach, which outperforms previous state-of-the-art\nmethods by attaining an exact match (EM) score of 94.78%, a precision score of\n0.9487, a recall score of 0.9478, an f1 score of 0.948, an f0.5 score of\n0.9483, and a modified accuracy (MA) score of 95.16% for Bangla spelling error\ncorrection. The models and corpus are publicly available at\nhttps://tinyurl.com/DPCSpell.\n","authors":["Mehedi Hasan Bijoy","Nahid Hossain","Salekul Islam","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2211.03730v2.pdf","comment":"29 pages, 4 figures, and 9 tables"},{"id":"http://arxiv.org/abs/2409.06043v1","updated":"2024-09-09T20:11:08Z","published":"2024-09-09T20:11:08Z","title":"Identifying the sources of ideological bias in GPT models through\n  linguistic variation in output","summary":"  Extant work shows that generative AI models such as GPT-3.5 and 4 perpetuate\nsocial stereotypes and biases. One concerning but less explored source of bias\nis ideology. Do GPT models take ideological stances on politically sensitive\ntopics? In this article, we provide an original approach to identifying\nideological bias in generative models, showing that bias can stem from both the\ntraining data and the filtering algorithm. We leverage linguistic variation in\ncountries with contrasting political attitudes to evaluate bias in average GPT\nresponses to sensitive political topics in those languages. First, we find that\nGPT output is more conservative in languages that map well onto conservative\nsocieties (i.e., Polish), and more liberal in languages used uniquely in\nliberal societies (i.e., Swedish). This result provides strong evidence of\ntraining data bias in GPT models. Second, differences across languages observed\nin GPT-3.5 persist in GPT-4, even though GPT-4 is significantly more liberal\ndue to OpenAI's filtering policy. Our main takeaway is that generative model\ntraining must focus on high-quality, curated datasets to reduce bias, even if\nit entails a compromise in training data size. Filtering responses after\ntraining only introduces new biases and does not remove the underlying training\nbiases.\n","authors":["Christina Walker","Joan C. Timoneda"],"pdf_url":"https://arxiv.org/pdf/2409.06043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00297v3","updated":"2024-09-09T19:57:17Z","published":"2024-03-30T09:20:43Z","title":"RoBERTa and Attention-based BiLSTM for Interpretable Sentiment Analysis\n  of Tweets","summary":"  Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","M. F. Mridha","Md Rashedul Islam","Yutaka Watanobe"],"pdf_url":"https://arxiv.org/pdf/2404.00297v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13598v2","updated":"2024-09-09T19:51:57Z","published":"2024-02-21T08:03:27Z","title":"User-LLM: Efficient LLM Contextualization with User Embeddings","summary":"  Large language models (LLMs) have achieved remarkable success across various\ndomains, but effectively incorporating complex and potentially noisy user\ntimeline data into LLMs remains a challenge. Current approaches often involve\ntranslating user timelines into text descriptions before feeding them to LLMs,\nwhich can be inefficient and may not fully capture the nuances of user\nbehavior. Inspired by how LLMs are effectively integrated with images through\ndirect embeddings, we propose User-LLM, a novel framework that leverages user\nembeddings to directly contextualize LLMs with user history interactions. These\nembeddings, generated by a user encoder pretrained using self-supervised\nlearning on diverse user interactions, capture latent user behaviors and\ninterests as well as their evolution over time. We integrate these user\nembeddings with LLMs through cross-attention, enabling LLMs to dynamically\nadapt their responses based on the context of a user's past actions and\npreferences.\n  Our approach achieves significant efficiency gains by representing user\ntimelines directly as embeddings, leading to substantial inference speedups of\nup to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google\nLocal Review datasets demonstrate that User-LLM outperforms text-prompt-based\ncontextualization on tasks requiring deep user understanding, with improvements\nof up to 16.33%, particularly excelling on long sequences that capture subtle\nshifts in user behavior. Furthermore, the incorporation of Perceiver layers\nstreamlines the integration between user encoders and LLMs, yielding additional\ncomputational savings.\n","authors":["Lin Ning","Luyang Liu","Jiaxing Wu","Neo Wu","Devora Berlowitz","Sushant Prakash","Bradley Green","Shawn O'Banion","Jun Xie"],"pdf_url":"https://arxiv.org/pdf/2402.13598v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06033v1","updated":"2024-09-09T19:47:57Z","published":"2024-09-09T19:47:57Z","title":"Investigating Causal Cues: Strengthening Spoofed Audio Detection with\n  Human-Discernible Linguistic Features","summary":"  Several types of spoofed audio, such as mimicry, replay attacks, and\ndeepfakes, have created societal challenges to information integrity. Recently,\nresearchers have worked with sociolinguistics experts to label spoofed audio\nsamples with Expert Defined Linguistic Features (EDLFs) that can be discerned\nby the human ear: pitch, pause, word-initial and word-final release bursts of\nconsonant stops, audible intake or outtake of breath, and overall audio\nquality. It is established that there is an improvement in several deepfake\ndetection algorithms when they augmented the traditional and common features of\naudio data with these EDLFs. In this paper, using a hybrid dataset comprised of\nmultiple types of spoofed audio augmented with sociolinguistic annotations, we\ninvestigate causal discovery and inferences between the discernible linguistic\nfeatures and the label in the audio clips, comparing the findings of the causal\nmodels with the expert ground truth validation labeling process. Our findings\nsuggest that the causal models indicate the utility of incorporating linguistic\nfeatures to help discern spoofed audio, as well as the overall need and\nopportunity to incorporate human knowledge into models and techniques for\nstrengthening AI models. The causal discovery and inference can be used as a\nfoundation of training humans to discern spoofed audio as well as automating\nEDLFs labeling for the purpose of performance improvement of the common\nAI-based spoofed audio detectors.\n","authors":["Zahra Khanjani","Tolulope Ale","Jianwu Wang","Lavon Davis","Christine Mallinson","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2409.06033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06013v1","updated":"2024-09-09T19:12:03Z","published":"2024-09-09T19:12:03Z","title":"Improved Visually Prompted Keyword Localisation in Real Low-Resource\n  Settings","summary":"  Given an image query, visually prompted keyword localisation (VPKL) aims to\nfind occurrences of the depicted word in a speech collection. This can be\nuseful when transcriptions are not available for a low-resource language (e.g.\nif it is unwritten). Previous work showed that VPKL can be performed with a\nvisually grounded speech model trained on paired images and unlabelled speech.\nBut all experiments were done on English. Moreover, transcriptions were used to\nget positive and negative pairs for the contrastive loss. This paper introduces\na few-shot learning scheme to mine pairs automatically without transcriptions.\nOn English, this results in only a small drop in performance. We also - for the\nfirst time - consider VPKL on a real low-resource language, Yoruba. While\nscores are reasonable, here we see a bigger drop in performance compared to\nusing ground truth pairs because the mining is less accurate in Yoruba.\n","authors":["Leanne Nortje","Dan Oneata","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2409.06013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05997v1","updated":"2024-09-09T18:47:00Z","published":"2024-09-09T18:47:00Z","title":"TransformerRanker: A Tool for Efficiently Finding the Best-Suited\n  Language Models for Downstream Classification Tasks","summary":"  Classification tasks in NLP are typically addressed by selecting a\npre-trained language model (PLM) from a model hub, and fine-tuning it for the\ntask at hand. However, given the very large number of PLMs that are currently\navailable, a practical challenge is to determine which of them will perform\nbest for a specific downstream task. With this paper, we introduce\nTransformerRanker, a lightweight library that efficiently ranks PLMs for\nclassification tasks without the need for computationally costly fine-tuning.\nOur library implements current approaches for transferability estimation\n(LogME, H-Score, kNN), in combination with layer aggregation options, which we\nempirically showed to yield state-of-the-art rankings of PLMs (Garbas et al.,\n2024). We designed the interface to be lightweight and easy to use, allowing\nusers to directly connect to the HuggingFace Transformers and Dataset\nlibraries. Users need only select a downstream classification task and a list\nof PLMs to create a ranking of likely best-suited PLMs for their task. We make\nTransformerRanker available as a pip-installable open-source library\nhttps://github.com/flairNLP/transformer-ranker.\n","authors":["Lukas Garbas","Max Ploner","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2409.05997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05994v1","updated":"2024-09-09T18:45:04Z","published":"2024-09-09T18:45:04Z","title":"MessIRve: A Large-Scale Spanish Information Retrieval Dataset","summary":"  Information retrieval (IR) is the task of finding relevant documents in\nresponse to a user query. Although Spanish is the second most spoken native\nlanguage, current IR benchmarks lack Spanish data, hindering the development of\ninformation access tools for Spanish speakers. We introduce MessIRve, a\nlarge-scale Spanish IR dataset with around 730 thousand queries from Google's\nautocomplete API and relevant documents sourced from Wikipedia. MessIRve's\nqueries reflect diverse Spanish-speaking regions, unlike other datasets that\nare translated from English or do not consider dialectal variations. The large\nsize of the dataset allows it to cover a wide variety of topics, unlike smaller\ndatasets. We provide a comprehensive description of the dataset, comparisons\nwith existing datasets, and baseline evaluations of prominent IR models. Our\ncontributions aim to advance Spanish IR research and improve information access\nfor Spanish speakers.\n","authors":["Francisco Valentini","Viviana Cotik","Damián Furman","Ivan Bercovich","Edgar Altszyler","Juan Manuel Pérez"],"pdf_url":"https://arxiv.org/pdf/2409.05994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05977v1","updated":"2024-09-09T18:21:28Z","published":"2024-09-09T18:21:28Z","title":"AI for Mathematics Mathematical Formalized Problem Solving and Theorem\n  Proving in Different Fields in Lean4","summary":"  Using computerized verifiable formal languages like Lean 4 to prove\nmathematical theorems has a significant impact on mathematical formalization.\nLean 4 offers prominent potential for advancing mathematical reasoning.\nHowever, existing efforts are limited to mathematical formalization languages\nin substantial online corpora and are dedicated to keeping pace with rapidly\nevolving languages. To bridge the gap between the traditional and computerized\nproof, my approach to formalizing theorem proving involves generating formal\nsteps and complete proofs using Large Language Models (LLMs) based on Natural\nLanguage (NL) proofs. The method is to introduce the basic structure and\ntactics in general, determine how AI can assist the mathematical formalization\nprocess to improve its performance, and give examples of solving problems in\nLean 4 comparing to NL, mainly in IMO, and a sample theorem proving in abstract\nalgebra.\n","authors":["Xichen Tang"],"pdf_url":"https://arxiv.org/pdf/2409.05977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05972v1","updated":"2024-09-09T18:10:05Z","published":"2024-09-09T18:10:05Z","title":"A Small Claims Court for the NLP: Judging Legal Text Classification\n  Strategies With Small Datasets","summary":"  Recent advances in language modelling has significantly decreased the need of\nlabelled data in text classification tasks. Transformer-based models,\npre-trained on unlabeled data, can outmatch the performance of models trained\nfrom scratch for each task. However, the amount of labelled data need to\nfine-tune such type of model is still considerably high for domains requiring\nexpert-level annotators, like the legal domain. This paper investigates the\nbest strategies for optimizing the use of a small labeled dataset and large\namounts of unlabeled data and perform a classification task in the legal area\nwith 50 predefined topics. More specifically, we use the records of demands to\na Brazilian Public Prosecutor's Office aiming to assign the descriptions in one\nof the subjects, which currently demands deep legal knowledge for manual\nfilling. The task of optimizing the performance of classifiers in this scenario\nis especially challenging, given the low amount of resources available\nregarding the Portuguese language, especially in the legal domain. Our results\ndemonstrate that classic supervised models such as logistic regression and SVM\nand the ensembles random forest and gradient boosting achieve better\nperformance along with embeddings extracted with word2vec when compared to BERT\nlanguage model. The latter demonstrates superior performance in association\nwith the architecture of the model itself as a classifier, having surpassed all\nprevious models in that regard. The best result was obtained with Unsupervised\nData Augmentation (UDA), which jointly uses BERT, data augmentation, and\nstrategies of semi-supervised learning, with an accuracy of 80.7% in the\naforementioned task.\n","authors":["Mariana Yukari Noguti","Edduardo Vellasques","Luiz Eduardo Soares Oliveira"],"pdf_url":"https://arxiv.org/pdf/2409.05972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05840v1","updated":"2024-09-09T17:44:00Z","published":"2024-09-09T17:44:00Z","title":"MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct","summary":"  The development of Multimodal Large Language Models (MLLMs) has seen\nsignificant advancements. However, the quantity and quality of multimodal\ninstruction data have emerged as significant bottlenecks in their progress.\nManually creating multimodal instruction data is both time-consuming and\ninefficient, posing challenges in producing instructions of high complexity.\nMoreover, distilling instruction data from black-box commercial models (e.g.,\nGPT-4o, GPT-4V) often results in simplistic instruction data, which constrains\nperformance to that of these models. The challenge of curating diverse and\ncomplex instruction data remains substantial. We propose MMEvol, a novel\nmultimodal instruction data evolution framework that combines fine-grained\nperception evolution, cognitive reasoning evolution, and interaction evolution.\nThis iterative approach breaks through data quality bottlenecks to generate a\ncomplex and diverse image-text instruction dataset, thereby empowering MLLMs\nwith enhanced capabilities. Beginning with an initial set of instructions,\nSEED-163K, we utilize MMEvol to systematically broadens the diversity of\ninstruction types, integrates reasoning steps to enhance cognitive\ncapabilities, and extracts detailed information from images to improve visual\nunderstanding and robustness. To comprehensively evaluate the effectiveness of\nour data, we train LLaVA-NeXT using the evolved data and conduct experiments\nacross 13 vision-language tasks. Compared to the baseline trained with seed\ndata, our approach achieves an average accuracy improvement of 3.1 points and\nreaches state-of-the-art (SOTA) performance on 9 of these tasks.\n","authors":["Run Luo","Haonan Zhang","Longze Chen","Ting-En Lin","Xiong Liu","Yuchuan Wu","Min Yang","Minzheng Wang","Pengpeng Zeng","Lianli Gao","Heng Tao Shen","Yunshui Li","Xiaobo Xia","Fei Huang","Jingkuan Song","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2409.05840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14770v5","updated":"2024-09-09T17:37:16Z","published":"2023-05-24T06:19:14Z","title":"Using Natural Language Explanations to Rescale Human Judgments","summary":"  The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.\n","authors":["Manya Wadhwa","Jifan Chen","Junyi Jessy Li","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2305.14770v5.pdf","comment":"Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling"},{"id":"http://arxiv.org/abs/2405.20611v2","updated":"2024-09-09T17:35:58Z","published":"2024-05-31T03:57:19Z","title":"Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in\n  Lifted Compiled Code","summary":"  Detecting vulnerabilities within compiled binaries is challenging due to lost\nhigh-level code structures and other factors such as architectural\ndependencies, compilers, and optimization options. To address these obstacles,\nthis research explores vulnerability detection using natural language\nprocessing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn\nsemantics from intermediate representation (LLVM IR) code. Long short-term\nmemory (LSTM) neural networks were trained on embeddings from encoders created\nusing approximately 48k LLVM functions from the Juliet dataset. This study is\npioneering in its comparison of word2vec models with multiple bidirectional\ntransformer (BERT, RoBERTa) embeddings built using LLVM code to train neural\nnetworks to detect vulnerabilities in compiled binaries. word2vec Skip-Gram\nmodels achieved 92% validation accuracy in detecting vulnerabilities,\noutperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This\nsuggests that complex contextual embeddings may not provide advantages over\nsimpler word2vec models for this task when a limited number (e.g. 48K) of data\nsamples are used to train the bidirectional transformer-based models. The\ncomparative results provide novel insights into selecting optimal embeddings\nfor learning compiler-independent semantic code representations to advance\nmachine learning detection of vulnerabilities in compiled binaries.\n","authors":["Gary A. McCully","John D. Hastings","Shengjie Xu","Adam Fortier"],"pdf_url":"https://arxiv.org/pdf/2405.20611v2.pdf","comment":"Updated with improvements\""},{"id":"http://arxiv.org/abs/2409.05816v1","updated":"2024-09-09T17:23:29Z","published":"2024-09-09T17:23:29Z","title":"Improving Pretraining Data Using Perplexity Correlations","summary":"  Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.\n","authors":["Tristan Thrush","Christopher Potts","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2409.05816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05806v1","updated":"2024-09-09T17:11:51Z","published":"2024-09-09T17:11:51Z","title":"Benchmarking Chinese Knowledge Rectification in Large Language Models","summary":"  While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Tianhe Lu","Jizhan Fang","Yunzhi Yao","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v1.pdf","comment":"Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2409.05799v1","updated":"2024-09-09T17:03:38Z","published":"2024-09-09T17:03:38Z","title":"PDAF: A Phonetic Debiasing Attention Framework For Speaker Verification","summary":"  Speaker verification systems are crucial for authenticating identity through\nvoice. Traditionally, these systems focus on comparing feature vectors,\noverlooking the speech's content. However, this paper challenges this by\nhighlighting the importance of phonetic dominance, a measure of the frequency\nor duration of phonemes, as a crucial cue in speaker verification. A novel\nPhoneme Debiasing Attention Framework (PDAF) is introduced, integrating with\nexisting attention frameworks to mitigate biases caused by phonetic dominance.\nPDAF adjusts the weighting for each phoneme and influences feature extraction,\nallowing for a more nuanced analysis of speech. This approach paves the way for\nmore accurate and reliable identity authentication through voice. Furthermore,\nby employing various weighting strategies, we evaluate the influence of\nphonetic features on the efficacy of the speaker verification system.\n","authors":["Massa Baali","Abdulhamid Aldoobi","Hira Dhamyal","Rita Singh","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2409.05799v1.pdf","comment":"Accepted to SLT"},{"id":"http://arxiv.org/abs/2408.14774v2","updated":"2024-09-09T16:41:36Z","published":"2024-08-27T04:31:58Z","title":"Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning","summary":"  We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.\n","authors":["Simran Kaur","Simon Park","Anirudh Goyal","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2408.14774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05771v1","updated":"2024-09-09T16:33:16Z","published":"2024-09-09T16:33:16Z","title":"Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models","summary":"  Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.\n","authors":["Emily Cheng","Richard J. Antonello"],"pdf_url":"https://arxiv.org/pdf/2409.05771v1.pdf","comment":"Equal contribution from both authors. Submitted to NeurIPS NeuroAI\n  workshop 2024"},{"id":"http://arxiv.org/abs/2406.10999v3","updated":"2024-09-09T16:28:09Z","published":"2024-06-16T16:25:22Z","title":"Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions","summary":"  This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.\n","authors":["Liman Wang","Hanyang Zhong","Wenting Cao","Zeyuan Sun"],"pdf_url":"https://arxiv.org/pdf/2406.10999v3.pdf","comment":"This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility"},{"id":"http://arxiv.org/abs/2311.18799v2","updated":"2024-09-09T16:00:04Z","published":"2023-11-30T18:43:51Z","title":"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning","summary":"  Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.\n","authors":["Artemis Panagopoulou","Le Xue","Ning Yu","Junnan Li","Dongxu Li","Shafiq Joty","Ran Xu","Silvio Savarese","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2311.18799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09481v2","updated":"2024-09-09T15:57:34Z","published":"2024-08-18T13:51:01Z","title":"PanoSent: A Panoptic Sextuple Extraction Benchmark for Multimodal\n  Conversational Aspect-based Sentiment Analysis","summary":"  While existing Aspect-based Sentiment Analysis (ABSA) has received extensive\neffort and advancement, there are still gaps in defining a more holistic\nresearch target seamlessly integrating multimodality, conversation context,\nfine-granularity, and also covering the changing sentiment dynamics as well as\ncognitive causal rationales. This paper bridges the gaps by introducing a\nmultimodal conversational ABSA, where two novel subtasks are proposed: 1)\nPanoptic Sentiment Sextuple Extraction, panoramically recognizing holder,\ntarget, aspect, opinion, sentiment, rationale from multi-turn multi-party\nmultimodal dialogue. 2) Sentiment Flipping Analysis, detecting the dynamic\nsentiment transformation throughout the conversation with the causal reasons.\nTo benchmark the tasks, we construct PanoSent, a dataset annotated both\nmanually and automatically, featuring high quality, large scale, multimodality,\nmultilingualism, multi-scenarios, and covering both implicit and explicit\nsentiment elements. To effectively address the tasks, we devise a novel\nChain-of-Sentiment reasoning framework, together with a novel multimodal large\nlanguage model (namely Sentica) and a paraphrase-based verification mechanism.\nExtensive evaluations demonstrate the superiority of our methods over strong\nbaselines, validating the efficacy of all our proposed methods. The work is\nexpected to open up a new era for the ABSA community, and thus all our codes\nand data are open at https://PanoSent.github.io/\n","authors":["Meng Luo","Hao Fei","Bobo Li","Shengqiong Wu","Qian Liu","Soujanya Poria","Erik Cambria","Mong-Li Lee","Wynne Hsu"],"pdf_url":"https://arxiv.org/pdf/2408.09481v2.pdf","comment":"Accepted by ACM MM 2024 (Oral)"},{"id":"http://arxiv.org/abs/2409.05732v1","updated":"2024-09-09T15:42:19Z","published":"2024-09-09T15:42:19Z","title":"Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach","summary":"  Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future.\n","authors":["Meng Zhou","Surajsinh Parmar","Anubhav Bhatti"],"pdf_url":"https://arxiv.org/pdf/2409.05732v1.pdf","comment":"Technical Report v1, work in progress"},{"id":"http://arxiv.org/abs/2409.05721v1","updated":"2024-09-09T15:33:07Z","published":"2024-09-09T15:33:07Z","title":"Referring Expression Generation in Visually Grounded Dialogue with\n  Discourse-aware Comprehension Guiding","summary":"  We propose an approach to referring expression generation (REG) in visually\ngrounded dialogue that is meant to produce referring expressions (REs) that are\nboth discriminative and discourse-appropriate. Our method constitutes a\ntwo-stage process. First, we model REG as a text- and image-conditioned\nnext-token prediction task. REs are autoregressively generated based on their\npreceding linguistic context and a visual representation of the referent.\nSecond, we propose the use of discourse-aware comprehension guiding as part of\na generate-and-rerank strategy through which candidate REs generated with our\nREG model are reranked based on their discourse-dependent discriminatory power.\nResults from our human evaluation indicate that our proposed two-stage approach\nis effective in producing discriminative REs, with higher performance in terms\nof text-image retrieval accuracy for reranked REs compared to those generated\nusing greedy decoding.\n","authors":["Bram Willemsen","Gabriel Skantze"],"pdf_url":"https://arxiv.org/pdf/2409.05721v1.pdf","comment":"Accepted for publication at INLG 2024"},{"id":"http://arxiv.org/abs/2409.05677v1","updated":"2024-09-09T14:44:19Z","published":"2024-09-09T14:44:19Z","title":"RegNLP in Action: Facilitating Compliance Through Automated Information\n  Retrieval and Answer Generation","summary":"  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance.Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary subfield aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We define an Automated Question-Passage\nGeneration task for RegNLP, create the ObliQA dataset containing 27,869\nquestions derived from the Abu Dhabi Global Markets (ADGM) financial regulation\ndocument collection, design a baseline Regulatory Information Retrieval and\nAnswer Generation system, and evaluate it with RePASs, a novel evaluation\nmetric that tests whether generated answers accurately capture all relevant\nobligations and avoid contradictions.\n","authors":["Tuba Gokhan","Kexin Wang","Iryna Gurevych","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2409.05677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05674v1","updated":"2024-09-09T14:41:57Z","published":"2024-09-09T14:41:57Z","title":"Evaluation of real-time transcriptions using end-to-end ASR models","summary":"  Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly\nevolved in the last few years. Traditional architectures based on pipelines\nhave been replaced by joint end-to-end (E2E) architectures that simplify and\nstreamline the model training process. In addition, new AI training methods,\nsuch as weak-supervised learning have reduced the need for high-quality audio\ndatasets for model training. However, despite all these advancements, little to\nno research has been done on real-time transcription. In real-time scenarios,\nthe audio is not pre-recorded, and the input audio must be fragmented to be\nprocessed by the ASR systems. To achieve real-time requirements, these\nfragments must be as short as possible to reduce latency. However, audio cannot\nbe split at any point as dividing an utterance into two separate fragments will\ngenerate an incorrect transcription. Also, shorter fragments provide less\ncontext for the ASR model. For this reason, it is necessary to design and test\ndifferent splitting algorithms to optimize the quality and delay of the\nresulting transcription. In this paper, three audio splitting algorithms are\nevaluated with different ASR models to determine their impact on both the\nquality of the transcription and the end-to-end delay. The algorithms are\nfragmentation at fixed intervals, voice activity detection (VAD), and\nfragmentation with feedback. The results are compared to the performance of the\nsame model, without audio fragmentation, to determine the effects of this\ndivision. The results show that VAD fragmentation provides the best quality\nwith the highest delay, whereas fragmentation at fixed intervals provides the\nlowest quality and the lowest delay. The newly proposed feedback algorithm\nexchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively,\nto the VAD splitting.\n","authors":["Carlos Arriaga","Alejandro Pozo","Javier Conde","Alvaro Alonso"],"pdf_url":"https://arxiv.org/pdf/2409.05674v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.16528v3","updated":"2024-09-09T14:31:26Z","published":"2024-05-26T11:29:57Z","title":"LoQT: Low-Rank Adapters for Quantized Pre-Training","summary":"  Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.\n","authors":["Sebastian Loeschcke","Mads Toftrup","Michael J. Kastoryano","Serge Belongie","Vésteinn Snæbjarnarson"],"pdf_url":"https://arxiv.org/pdf/2405.16528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05653v1","updated":"2024-09-09T14:19:21Z","published":"2024-09-09T14:19:21Z","title":"Revisiting English Winogender Schemas for Consistency, Coverage, and\n  Grammatical Case","summary":"  While measuring bias and robustness in coreference resolution are important\ngoals, such measurements are only as good as the tools we use to measure them\nwith. Winogender schemas (Rudinger et al., 2018) are an influential dataset\nproposed to evaluate gender bias in coreference resolution, but a closer look\nat the data reveals issues with the instances that compromise their use for\nreliable evaluation, including treating different grammatical cases of pronouns\nin the same way, violations of template constraints, and typographical errors.\nWe identify these issues and fix them, contributing a new dataset: Winogender\n2.0. Our changes affect performance with state-of-the-art supervised\ncoreference resolution systems as well as all model sizes of the language model\nFLAN-T5, with F1 dropping on average 0.1 points. We also propose a new method\nto evaluate pronominal bias in coreference resolution that goes beyond the\nbinary. With this method and our new dataset which is balanced for grammatical\ncase, we empirically demonstrate that bias characteristics vary not just across\npronoun sets, but also across surface forms of those sets.\n","authors":["Vagrant Gautam","Julius Steuer","Eileen Bingert","Ray Johns","Anne Lauscher","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2409.05653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00369v3","updated":"2024-09-09T13:50:30Z","published":"2024-08-31T07:10:16Z","title":"An Empirical Study on Information Extraction using Large Language Models","summary":"  Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability.\n","authors":["Ridong Han","Chaohao Yang","Tao Peng","Prayag Tiwari","Xiang Wan","Lu Liu","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.00369v3.pdf","comment":"This submission was intended instead as the replacement of\n  arXiv:2305.14450 , where it now appears as arXiv:2305.14450v2"},{"id":"http://arxiv.org/abs/2409.05601v1","updated":"2024-09-09T13:35:52Z","published":"2024-09-09T13:35:52Z","title":"Longer is (Not Necessarily) Stronger: Punctuated Long-Sequence Training\n  for Enhanced Speech Recognition and Translation","summary":"  This paper presents a new method for training sequence-to-sequence models for\nspeech recognition and translation tasks. Instead of the traditional approach\nof training models on short segments containing only lowercase or partial\npunctuation and capitalization (PnC) sentences, we propose training on longer\nutterances that include complete sentences with proper punctuation and\ncapitalization. We achieve this by using the FastConformer architecture which\nallows training 1 Billion parameter models with sequences up to 60 seconds long\nwith full attention. However, while training with PnC enhances the overall\nperformance, we observed that accuracy plateaus when training on sequences\nlonger than 40 seconds across various evaluation settings. Our proposed method\nsignificantly improves punctuation and capitalization accuracy, showing a 25%\nrelative word error rate (WER) improvement on the Earnings-21 and Earnings-22\nbenchmarks. Additionally, training on longer audio segments increases the\noverall model accuracy across speech recognition and translation benchmarks.\nThe model weights and training code are open-sourced though NVIDIA NeMo.\n","authors":["Nithin Rao Koluguri","Travis Bartley","Hainan Xu","Oleksii Hrinchuk","Jagadeesh Balam","Boris Ginsburg","Georg Kucsko"],"pdf_url":"https://arxiv.org/pdf/2409.05601v1.pdf","comment":"Accepted at SLT 2024"},{"id":"http://arxiv.org/abs/2409.05592v1","updated":"2024-09-09T13:23:14Z","published":"2024-09-09T13:23:14Z","title":"ExDDI: Explaining Drug-Drug Interaction Predictions with Natural\n  Language","summary":"  Predicting unknown drug-drug interactions (DDIs) is crucial for improving\nmedication safety. Previous efforts in DDI prediction have typically focused on\nbinary classification or predicting DDI categories, with the absence of\nexplanatory insights that could enhance trust in these predictions. In this\nwork, we propose to generate natural language explanations for DDI predictions,\nenabling the model to reveal the underlying pharmacodynamics and\npharmacokinetics mechanisms simultaneously as making the prediction. To do\nthis, we have collected DDI explanations from DDInter and DrugBank and\ndeveloped various models for extensive experiments and analysis. Our models can\nprovide accurate explanations for unknown DDIs between known drugs. This paper\ncontributes new tools to the field of DDI prediction and lays a solid\nfoundation for further research on generating explanations for DDI predictions.\n","authors":["Zhaoyue Sun","Jiazheng Li","Gabriele Pergola","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2409.05592v1.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.05583v1","updated":"2024-09-09T13:12:11Z","published":"2024-09-09T13:12:11Z","title":"Spatially-Aware Speaker for Vision-and-Language Navigation Instruction\n  Generation","summary":"  Embodied AI aims to develop robots that can \\textit{understand} and execute\nhuman language instructions, as well as communicate in natural languages. On\nthis front, we study the task of generating highly detailed navigational\ninstructions for the embodied robots to follow. Although recent studies have\ndemonstrated significant leaps in the generation of step-by-step instructions\nfrom sequences of images, the generated instructions lack variety in terms of\ntheir referral to objects and landmarks. Existing speaker models learn\nstrategies to evade the evaluation metrics and obtain higher scores even for\nlow-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker),\nan instruction generator or \\textit{Speaker} model that utilises both\nstructural and semantic knowledge of the environment to produce richer\ninstructions. For training, we employ a reward learning method in an\nadversarial setting to avoid systematic bias introduced by language evaluation\nmetrics. Empirically, our method outperforms existing instruction generation\nmodels, evaluated using standard metrics. Our code is available at\n\\url{https://github.com/gmuraleekrishna/SAS}.\n","authors":["Muraleekrishna Gopinathan","Martin Masek","Jumana Abu-Khalaf","David Suter"],"pdf_url":"https://arxiv.org/pdf/2409.05583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12325v2","updated":"2024-09-09T13:10:50Z","published":"2024-08-22T12:00:31Z","title":"Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators","summary":"  Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.\n","authors":["Dingkang Yang","Dongling Xiao","Jinjie Wei","Mingcheng Li","Zhaoyu Chen","Ke Li","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12325v2.pdf","comment":"Hallucination Mitigation in LLMs"},{"id":"http://arxiv.org/abs/2401.11944v3","updated":"2024-09-09T12:38:11Z","published":"2024-01-22T13:34:34Z","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark","summary":"  As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.\n","authors":["Ge Zhang","Xinrun Du","Bei Chen","Yiming Liang","Tongxu Luo","Tianyu Zheng","Kang Zhu","Yuyang Cheng","Chunpu Xu","Shuyue Guo","Haoran Zhang","Xingwei Qu","Junjie Wang","Ruibin Yuan","Yizhi Li","Zekun Wang","Yudong Liu","Yu-Hsuan Tsai","Fengji Zhang","Chenghua Lin","Wenhao Huang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2401.11944v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05556v1","updated":"2024-09-09T12:25:10Z","published":"2024-09-09T12:25:10Z","title":"SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning","summary":"  A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles.\n","authors":["Alireza Ghafarollahi","Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2409.05556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05530v1","updated":"2024-09-09T11:38:06Z","published":"2024-09-09T11:38:06Z","title":"QiBERT -- Classifying Online Conversations Messages with BERT as a\n  Feature","summary":"  Recent developments in online communication and their usage in everyday life\nhave caused an explosion in the amount of a new genre of text data, short text.\nThus, the need to classify this type of text based on its content has a\nsignificant implication in many areas. Online debates are no exception, once\nthese provide access to information about opinions, positions and preferences\nof its users. This paper aims to use data obtained from online social\nconversations in Portuguese schools (short text) to observe behavioural trends\nand to see if students remain engaged in the discussion when stimulated. This\nproject used the state of the art (SoA) Machine Learning (ML) algorithms and\nmethods, through BERT based models to classify if utterances are in or out of\nthe debate subject. Using SBERT embeddings as a feature, with supervised\nlearning, the proposed model achieved results above 0.95 average accuracy for\nclassifying online messages. Such improvements can help social scientists\nbetter understand human communication, behaviour, discussion and persuasion.\n","authors":["Bruno D. Ferreira-Saraiva","Zuil Pirola","João P. Matos-Carvalho","Manuel Marques-Pita"],"pdf_url":"https://arxiv.org/pdf/2409.05530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04073v2","updated":"2024-09-09T11:33:00Z","published":"2024-09-06T07:29:01Z","title":"AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model","summary":"  Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).\n","authors":["Zeyu Zhang","Paul Groth","Iacer Calixto","Sebastian Schelter"],"pdf_url":"https://arxiv.org/pdf/2409.04073v2.pdf","comment":"12 pages excluding references, 3 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2409.05521v1","updated":"2024-09-09T11:28:02Z","published":"2024-09-09T11:28:02Z","title":"Harmonic Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks.\n","authors":["Anna Kruspe"],"pdf_url":"https://arxiv.org/pdf/2409.05521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11381v2","updated":"2024-09-09T11:18:16Z","published":"2024-08-21T07:20:48Z","title":"RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation","summary":"  Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms.\n","authors":["Xuanwang Zhang","Yunze Song","Yidong Wang","Shuyun Tang","Xinfeng Li","Zhengran Zeng","Zhen Wu","Wei Ye","Wenyuan Xu","Yue Zhang","Xinyu Dai","Shikun Zhang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2408.11381v2.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.05486v1","updated":"2024-09-09T10:30:00Z","published":"2024-09-09T10:30:00Z","title":"Elsevier Arena: Human Evaluation of Chemistry/Biology/Health\n  Foundational Large Language Models","summary":"  The quality and capabilities of large language models cannot be currently\nfully assessed with automated, benchmark evaluations. Instead, human\nevaluations that expand on traditional qualitative techniques from natural\nlanguage generation literature are required. One recent best-practice consists\nin using A/B-testing frameworks, which capture preferences of human evaluators\nfor specific models. In this paper we describe a human evaluation experiment\nfocused on the biomedical domain (health, biology, chemistry/pharmacology)\ncarried out at Elsevier. In it a large but not massive (8.8B parameter)\ndecoder-only foundational transformer trained on a relatively small (135B\ntokens) but highly curated collection of Elsevier datasets is compared to\nOpenAI's GPT-3.5-turbo and Meta's foundational 7B parameter Llama 2 model\nagainst multiple criteria. Results indicate -- even if IRR scores were\ngenerally low -- a preference towards GPT-3.5-turbo, and hence towards models\nthat possess conversational abilities, are very large and were trained on very\nlarge datasets. But at the same time, indicate that for less massive models\ntraining on smaller but well-curated training sets can potentially give rise to\nviable alternatives in the biomedical domain.\n","authors":["Camilo Thorne","Christian Druckenbrodt","Kinga Szarkowska","Deepika Goyal","Pranita Marajan","Vijay Somanath","Corey Harper","Mao Yan","Tony Scerri"],"pdf_url":"https://arxiv.org/pdf/2409.05486v1.pdf","comment":"11 pages, 5 tables, 6 figures"},{"id":"http://arxiv.org/abs/2409.03753v2","updated":"2024-09-09T10:04:00Z","published":"2024-09-05T17:59:15Z","title":"WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild","summary":"  The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.\n","authors":["Yuntian Deng","Wenting Zhao","Jack Hessel","Xiang Ren","Claire Cardie","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2409.03753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02795v3","updated":"2024-09-09T09:31:30Z","published":"2024-09-04T15:11:55Z","title":"Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey","summary":"  Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.\n","authors":["Bofei Gao","Feifan Song","Yibo Miao","Zefan Cai","Zhe Yang","Liang Chen","Helan Hu","Runxin Xu","Qingxiu Dong","Ce Zheng","Wen Xiao","Ge Zhang","Daoguang Zan","Keming Lu","Bowen Yu","Dayiheng Liu","Zeyu Cui","Jian Yang","Lei Sha","Houfeng Wang","Zhifang Sui","Peiyi Wang","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2409.02795v3.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.05448v1","updated":"2024-09-09T09:04:56Z","published":"2024-09-09T09:04:56Z","title":"Representational Analysis of Binding in Large Language Models","summary":"  Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nFeng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs\nuse a abstract concept called Binding ID (BI) to internally mark\nentity-attribute pairs. However, they have not directly captured the BI\ndeterminant information from entity activations. In this work, we provide a\nnovel view of the Binding ID mechanism by localizing the prototype of BI\ninformation. Specifically, we discover that there exists a low-rank subspace in\nthe hidden state (or activation) of LMs, that primarily encodes the order of\nentity and attribute and which is used as the prototype of BI to causally\ndetermine the binding. To identify this subspace, we choose principle component\nanalysis as our first attempt and it is empirically proven to be effective.\nMoreover, we also discover that when editing representations along directions\nin the subspace, LMs tend to bind a given entity to other attributes\naccordingly. For example, by patching activations along the BI encoding\ndirection we can make the LM to infer ``Box Z contains the stone'' and ``Box Z\ncontains the map''.\n","authors":["Qin Dai","Benjamin Heinzerling","Kentaro Inui"],"pdf_url":"https://arxiv.org/pdf/2409.05448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15297v2","updated":"2024-09-09T08:53:18Z","published":"2024-08-27T11:31:12Z","title":"YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection","summary":"  Dysfluent speech detection is the bottleneck for disordered speech analysis\nand spoken language learning. Current state-of-the-art models are governed by\nrule-based systems which lack efficiency and robustness, and are sensitive to\ntemplate design. In this paper, we propose YOLO-Stutter: a first end-to-end\nmethod that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes\nimperfect speech-text alignment as input, followed by a spatial feature\naggregator, and a temporal dependency extractor to perform region-wise boundary\nand class predictions. We also introduce two dysfluency corpus, VCTK-Stutter\nand VCTK-TTS, that simulate natural spoken dysfluencies including repetition,\nblock, missing, replacement, and prolongation. Our end-to-end method achieves\nstate-of-the-art performance with a minimum number of trainable parameters for\non both simulated data and real aphasia speech. Code and datasets are\nopen-sourced at https://github.com/rorizzz/YOLO-Stutter\n","authors":["Xuanru Zhou","Anshul Kashyap","Steve Li","Ayati Sharma","Brittany Morin","David Baquirin","Jet Vonk","Zoe Ezzes","Zachary Miller","Maria Luisa Gorno Tempini","Jiachen Lian","Gopala Krishna Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.15297v2.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2409.05925v1","updated":"2024-09-09T08:29:39Z","published":"2024-09-09T08:29:39Z","title":"Assessing SPARQL capabilities of Large Language Models","summary":"  The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.\n","authors":["Lars-Peter Meyer","Johannes Frey","Felix Brei","Natanael Arndt"],"pdf_url":"https://arxiv.org/pdf/2409.05925v1.pdf","comment":"peer reviewed publication at NLP4KGc @ Semantics 2024, see\n  https://sites.google.com/view/3rdnlp4kgc"},{"id":"http://arxiv.org/abs/2409.05423v1","updated":"2024-09-09T08:24:29Z","published":"2024-09-09T08:24:29Z","title":"STLM Engineering Report: Dropout","summary":"  In this work we explore the relevance of dropout for modern language models,\nparticularly in the context of models on the scale of <100M parameters. We\nexplore it's relevance firstly in the regime of improving the sample efficiency\nof models given small, high quality datasets, and secondly in the regime of\nimproving the quality of its fit on larger datasets where models may underfit.\nWe find that concordant with conventional wisdom, dropout remains effective in\nthe overfitting scenario, and that furthermore it may have some relevance for\nimproving the fit of models even in the case of excess data, as suggested by\nprevious research. In the process we find that the existing explanation for the\nmechanism behind this performance gain is not applicable in the case of\nlanguage modelling.\n","authors":["Dylan Hillier","Leon Guertler","Bobby Cheng","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2409.05423v1.pdf","comment":"6 pages, 3 figures, For code base see\n  https://github.com/LeonGuertler/SuperTinyLanguageModels"},{"id":"http://arxiv.org/abs/2405.05966v2","updated":"2024-09-09T08:21:13Z","published":"2024-05-09T17:59:32Z","title":"Natural Language Processing RELIES on Linguistics","summary":"  Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.\n","authors":["Juri Opitz","Shira Wein","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2405.05966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05401v1","updated":"2024-09-09T07:57:43Z","published":"2024-09-09T07:57:43Z","title":"NLLB-E5: A Scalable Multilingual Retrieval Model","summary":"  Despite significant progress in multilingual information retrieval, the lack\nof models capable of effectively supporting multiple languages, particularly\nlow-resource like Indic languages, remains a critical challenge. This paper\npresents NLLB-E5: A Scalable Multilingual Retrieval Model. NLLB-E5 leverages\nthe in-built multilingual capabilities in the NLLB encoder for translation\ntasks. It proposes a distillation approach from multilingual retriever E5 to\nprovide a zero-shot retrieval approach handling multiple languages, including\nall major Indic languages, without requiring multilingual training data. We\nevaluate the model on a comprehensive suite of existing benchmarks, including\nHindi-BEIR, highlighting its robust performance across diverse languages and\ntasks. Our findings uncover task and domain-specific challenges, providing\nvaluable insights into the retrieval performance, especially for low-resource\nlanguages. NLLB-E5 addresses the urgent need for an inclusive, scalable, and\nlanguage-agnostic text retrieval model, advancing the field of multilingual\ninformation access and promoting digital inclusivity for millions of users\nglobally.\n","authors":["Arkadeep Acharya","Rudra Murthy","Vishwajeet Kumar","Jaydeep Sen"],"pdf_url":"https://arxiv.org/pdf/2409.05401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14840v2","updated":"2024-09-09T06:57:22Z","published":"2024-08-27T07:51:26Z","title":"CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding","summary":"  Knowledge graph embedding (KGE) constitutes a foundational task, directed\ntowards learning representations for entities and relations within knowledge\ngraphs (KGs), with the objective of crafting representations comprehensive\nenough to approximate the logical and symbolic interconnections among entities.\nIn this paper, we define a metric Z-counts to measure the difficulty of\ntraining each triple ($<$head entity, relation, tail entity$>$) in KGs with\ntheoretical analysis. Based on this metric, we propose \\textbf{CL4KGE}, an\nefficient \\textbf{C}urriculum \\textbf{L}earning based training strategy for\n\\textbf{KGE}. This method includes a difficulty measurer and a training\nscheduler that aids in the training of KGE models. Our approach possesses the\nflexibility to act as a plugin within a wide range of KGE models, with the\nadded advantage of adaptability to the majority of KGs in existence. The\nproposed method has been evaluated on popular KGE models, and the results\ndemonstrate that it enhances the state-of-the-art methods. The use of Z-counts\nas a metric has enabled the identification of challenging triples in KGs, which\nhelps in devising effective training strategies.\n","authors":["Yang Liu","Chuan Zhou","Peng Zhang","Yanan Cao","Yongchao Liu","Zhao Li","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2408.14840v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.05368v1","updated":"2024-09-09T06:55:38Z","published":"2024-09-09T06:55:38Z","title":"Application Specific Compression of Deep Learning Models","summary":"  Large Deep Learning models are compressed and deployed for specific\napplications. However, current Deep Learning model compression methods do not\nutilize the information about the target application. As a result, the\ncompressed models are application agnostic. Our goal is to customize the model\ncompression process to create a compressed model that will perform better for\nthe target application. Our method, Application Specific Compression (ASC),\nidentifies and prunes components of the large Deep Learning model that are\nredundant specifically for the given target application. The intuition of our\nwork is to prune the parts of the network that do not contribute significantly\nto updating the data representation for the given application. We have\nexperimented with the BERT family of models for three applications: Extractive\nQA, Natural Language Inference, and Paraphrase Identification. We observe that\ncustomized compressed models created using ASC method perform better than\nexisting model compression methods and off-the-shelf compressed models.\n","authors":["Rohit Raj Rai","Angana Borah","Amit Awekar"],"pdf_url":"https://arxiv.org/pdf/2409.05368v1.pdf","comment":"Accepted in the Proceedings of the 8th Joint International Conference\n  on Data Science & Management of Data (12th ACM IKDD CODS and 30th COMAD) for\n  the Short Research Paper track, 5 pages"},{"id":"http://arxiv.org/abs/2409.05367v1","updated":"2024-09-09T06:55:37Z","published":"2024-09-09T06:55:37Z","title":"Diagnostic Reasoning in Natural Language: Computational Model and\n  Application","summary":"  Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond.\n","authors":["Nils Dycke","Matej Zečević","Ilia Kuznetsov","Beatrix Suess","Kristian Kersting","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2409.05367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05356v1","updated":"2024-09-09T06:28:47Z","published":"2024-09-09T06:28:47Z","title":"IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech\n  Corpus for Scaling Indian TTS","summary":"  Recent advancements in text-to-speech (TTS) synthesis show that large-scale\nmodels trained with extensive web data produce highly natural-sounding output.\nHowever, such data is scarce for Indian languages due to the lack of\nhigh-quality, manually subtitled data on platforms like LibriVox or YouTube. To\naddress this gap, we enhance existing large-scale ASR datasets containing\nnatural conversations collected in low-quality environments to generate\nhigh-quality TTS training data. Our pipeline leverages the cross-lingual\ngeneralization of denoising and speech enhancement models trained on English\nand applied to Indian languages. This results in IndicVoices-R (IV-R), the\nlargest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704\nhours of high-quality speech from 10,496 speakers across 22 Indian languages.\nIV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS,\nand IndicTTS. We also introduce the IV-R Benchmark, the first to assess\nzero-shot, few-shot, and many-shot speaker generalization capabilities of TTS\nmodels on Indian voices, ensuring diversity in age, gender, and style. We\ndemonstrate that fine-tuning an English pre-trained model on a combined dataset\nof high-quality IndicTTS and our IV-R dataset results in better zero-shot\nspeaker generalization compared to fine-tuning on the IndicTTS dataset alone.\nFurther, our evaluation reveals limited zero-shot generalization for Indian\nvoices in TTS models trained on prior datasets, which we improve by fine-tuning\nthe model on our data containing diverse set of speakers across language\nfamilies. We open-source all data and code, releasing the first TTS model for\nall 22 official Indian languages.\n","authors":["Ashwin Sankar","Srija Anand","Praveen Srinivasa Varadhan","Sherry Thomas","Mehak Singal","Shridhar Kumar","Deovrat Mehendale","Aditi Krishana","Giri Raju","Mitesh Khapra"],"pdf_url":"https://arxiv.org/pdf/2409.05356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14482v2","updated":"2024-09-09T06:19:07Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Chejian Xu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v2.pdf","comment":"v2: major update with significantly improved results"},{"id":"http://arxiv.org/abs/2407.15186v3","updated":"2024-09-09T06:17:21Z","published":"2024-07-21T14:48:23Z","title":"A Survey on Employing Large Language Models for Text-to-SQL Tasks","summary":"  The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory.\n","authors":["Liang Shi","Zhengju Tang","Nan Zhang","Xiaotong Zhang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15186v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03259v3","updated":"2024-09-09T05:27:28Z","published":"2024-04-04T07:31:56Z","title":"Advancing Aspect-Based Sentiment Analysis through Deep Learning Models","summary":"  Aspect-based sentiment analysis predicts sentiment polarity with fine\ngranularity. While graph convolutional networks (GCNs) are widely utilized for\nsentimental feature extraction, their naive application for syntactic feature\nextraction can compromise information preservation. This study introduces an\ninnovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph\nwhile preserving intact feature information, leading to enhanced performance.\nSpecifically,we first integrate a bidirectional long short-term memory\n(Bi-LSTM) network and a self-attention-based transformer. This combination\nfacilitates effective text encoding, preventing the loss of information and\npredicting long dependency text. A bidirectional GCN (Bi-GCN) with message\npassing is then employed to encode relationships between entities.\nAdditionally, unnecessary information is filtered out using an aspect-specific\nmasking technique. To validate the effectiveness of our proposed model, we\nconduct extensive evaluation experiments on four benchmark datasets. The\nexperimental results demonstrate enhanced performance in aspect-based sentiment\nanalysis with the use of SentiSys.\n","authors":["Chen Li","Huidong Tang","Jinli Zhang","Xiujing Guo","Debo Cheng","Yasuhiko Morimoto"],"pdf_url":"https://arxiv.org/pdf/2404.03259v3.pdf","comment":"This paper has already been accepted by the 20th International\n  Conference on Advanced Data Mining and Applications (ADMA2024)"},{"id":"http://arxiv.org/abs/2403.19159v2","updated":"2024-09-09T04:39:31Z","published":"2024-03-28T06:03:47Z","title":"Disentangling Length from Quality in Direct Preference Optimization","summary":"  Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.\n","authors":["Ryan Park","Rafael Rafailov","Stefano Ermon","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2403.19159v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14795v3","updated":"2024-09-09T04:38:16Z","published":"2023-05-24T06:48:41Z","title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions","summary":"  The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin.\n","authors":["Zexuan Zhong","Zhengxuan Wu","Christopher D. Manning","Christopher Potts","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2305.14795v3.pdf","comment":"EMNLP 2023. Our code and datasets are available at\n  https://github.com/princeton-nlp/MQuAKE"},{"id":"http://arxiv.org/abs/2409.05292v1","updated":"2024-09-09T03:00:53Z","published":"2024-09-09T03:00:53Z","title":"Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram\n  Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis","summary":"  The world is currently experiencing an outbreak of mpox, which has been\ndeclared a Public Health Emergency of International Concern by WHO. No prior\nwork related to social media mining has focused on the development of a dataset\nof Instagram posts about the mpox outbreak. The work presented in this paper\naims to address this research gap and makes two scientific contributions to\nthis field. First, it presents a multilingual dataset of 60,127 Instagram posts\nabout mpox, published between July 23, 2022, and September 5, 2024. The\ndataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram\nposts about mpox in 52 languages. For each of these posts, the Post ID, Post\nDescription, Date of publication, language, and translated version of the post\n(translation to English was performed using the Google Translate API) are\npresented as separate attributes in the dataset. After developing this dataset,\nsentiment analysis, hate speech detection, and anxiety or stress detection were\nperformed. This process included classifying each post into (i) one of the\nsentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or\nneutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no\nanxiety/stress detected. These results are presented as separate attributes in\nthe dataset. Second, this paper presents the results of performing sentiment\nanalysis, hate speech analysis, and anxiety or stress analysis. The variation\nof the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and\nneutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and\n50.64%, respectively. In terms of hate speech detection, 95.75% of the posts\ndid not contain hate and the remaining 4.25% of the posts contained hate.\nFinally, 72.05% of the posts did not indicate any anxiety/stress, and the\nremaining 27.95% of the posts represented some form of anxiety/stress.\n","authors":["Nirmalya Thakur"],"pdf_url":"https://arxiv.org/pdf/2409.05292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05286v1","updated":"2024-09-09T02:41:00Z","published":"2024-09-09T02:41:00Z","title":"Seek and Solve Reasoning for Table Question Answering","summary":"  Table-based Question Answering (TQA) involves answering questions based on\ntabular data. The complexity of table structures and question logic makes this\ntask difficult even for Large Language Models (LLMs). This paper improves TQA\nperformance by leveraging LLMs' reasoning capabilities. Inspired by how humans\nsolve TQA tasks, we propose a Seek-and-Solve pipeline that instructs the LLM to\nfirst seek relevant information and then answer questions. The two stages are\nintegrated at the reasoning level, and their Chain of Thought (CoT) paths are\nintegrated into a coherent Seek-and-Solve CoT (SS-CoT). Furthermore, we present\na compact single-stage TQA-solving prompt distilled from the pipeline.\nExperiments demonstrate that under In-Context Learning settings, using samples\nwith SS-CoT paths as demonstrations, the TQA-solving prompt can effectively\nguide the LLM to solve complex TQA tasks, resulting in improved performance and\nreliability. Our results highlight the importance of properly eliciting LLMs'\nreasoning capabilities in solving complex TQA tasks.\n","authors":["Ruya Jiang","Chun Wang","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2409.05286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05283v1","updated":"2024-09-09T02:28:53Z","published":"2024-09-09T02:28:53Z","title":"On the Relationship between Truth and Political Bias in Language Models","summary":"  Language model alignment research often attempts to ensure that models are\nnot only helpful and harmless, but also truthful and unbiased. However,\noptimizing these objectives simultaneously can obscure how improving one aspect\nmight impact the others. In this work, we focus on analyzing the relationship\nbetween two concepts essential in both language model alignment and political\nscience: \\textit{truthfulness} and \\textit{political bias}. We train reward\nmodels on various popular truthfulness datasets and subsequently evaluate their\npolitical bias. Our findings reveal that optimizing reward models for\ntruthfulness on these datasets tends to result in a left-leaning political\nbias. We also find that existing open-source reward models (i.e. those trained\non standard human preference datasets) already show a similar bias and that the\nbias is larger for larger models. These results raise important questions about\nboth the datasets used to represent truthfulness and what language models\ncapture about the relationship between truth and politics.\n","authors":["Suyash Fulay","William Brannon","Shrestha Mohanty","Cassandra Overney","Elinor Poole-Dayan","Deb Roy","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2409.05283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05275v1","updated":"2024-09-09T01:59:29Z","published":"2024-09-09T01:59:29Z","title":"RexUniNLU: Recursive Method with Explicit Schema Instructor for\n  Universal NLU","summary":"  Information Extraction (IE) and Text Classification (CLS) serve as the\nfundamental pillars of NLU, with both disciplines relying on analyzing input\nsequences to categorize outputs into pre-established schemas. However, there is\nno existing encoder-based model that can unify IE and CLS tasks from this\nperspective. To fully explore the foundation shared within NLU tasks, we have\nproposed a Recursive Method with Explicit Schema Instructor for Universal NLU.\nSpecifically, we firstly redefine the true universal information extraction\n(UIE) with a formal formulation that covers almost all extraction schemas,\nincluding quadruples and quintuples which remain unsolved for previous UIE\nmodels. Then, we expands the formulation to all CLS and multi-modal NLU tasks.\nBased on that, we introduce RexUniNLU, an universal NLU solution that employs\nexplicit schema constraints for IE and CLS, which encompasses all IE and CLS\ntasks and prevent incorrect connections between schema and input sequence. To\navoid interference between different schemas, we reset the position ids and\nattention mask matrices. Extensive experiments are conducted on IE, CLS in both\nEnglish and Chinese, and multi-modality, revealing the effectiveness and\nsuperiority. Our codes are publicly released.\n","authors":["Chengyuan Liu","Shihang Wang","Fubang Zhao","Kun Kuang","Yangyang Kang","Weiming Lu","Changlong Sun","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.05275v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2304.14770"},{"id":"http://arxiv.org/abs/2305.03511v2","updated":"2024-09-09T01:44:27Z","published":"2023-05-02T15:33:09Z","title":"Shared Latent Space by Both Languages in Non-Autoregressive Neural\n  Machine Translation","summary":"  Non-autoregressive neural machine translation (NAT) offers substantial\ntranslation speed up compared to autoregressive neural machine translation (AT)\nat the cost of translation quality. Latent variable modeling has emerged as a\npromising approach to bridge this quality gap, particularly for addressing the\nchronic multimodality problem in NAT. In the previous works that used latent\nvariable modeling, they added an auxiliary model to estimate the posterior\ndistribution of the latent variable conditioned on the source and target\nsentences. However, it causes several disadvantages, such as redundant\ninformation extraction in the latent variable, increasing the number of\nparameters, and a tendency to ignore some information from the inputs. In this\npaper, we propose a novel latent variable modeling that integrates a dual\nreconstruction perspective and an advanced hierarchical latent modeling with a\nshared intermediate latent space across languages. This latent variable\nmodeling hypothetically alleviates or prevents the above disadvantages. In our\nexperiment results, we present comprehensive demonstrations that our proposed\napproach infers superior latent variables which lead better translation\nquality. Finally, in the benchmark translation tasks, such as WMT, we\ndemonstrate that our proposed method significantly improves translation quality\ncompared to previous NAT baselines including the state-of-the-art NAT model.\n","authors":["DongNyeong Heo","Heeyoul Choi"],"pdf_url":"https://arxiv.org/pdf/2305.03511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05257v1","updated":"2024-09-09T00:40:47Z","published":"2024-09-09T00:40:47Z","title":"UPCS: Unbiased Persona Construction for Dialogue Generation","summary":"  Narrative systems, such as dialogue and storytelling systems, often utilize\npersona profiles to enhance personalized interactions. Existing persona\nprofiles frequently exhibit biases, posing risks to system integrity and\nfairness. To address this, we introduce the UPCS framework, which categorizes\ncharacter descriptions into eight dimensions, including bias mitigation\nstrategies. Experimental results demonstrate UPCS's superiority in accuracy,\ndiversity, bias elimination, and user satisfaction, marking a significant\nadvancement in persona construction for reliable narrative systems.\n","authors":["Kuiyun Chen","Yanbin Wei"],"pdf_url":"https://arxiv.org/pdf/2409.05257v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.06111v1","updated":"2024-09-09T23:34:24Z","published":"2024-09-09T23:34:24Z","title":"PaRCE: Probabilistic and Reconstruction-Based Competency Estimation for\n  Safe Navigation Under Perception Uncertainty","summary":"  Perception-based navigation systems are useful for unmanned ground vehicle\n(UGV) navigation in complex terrains, where traditional depth-based navigation\nschemes are insufficient. However, these data-driven methods are highly\ndependent on their training data and can fail in surprising and dramatic ways\nwith little warning. To ensure the safety of the vehicle and the surrounding\nenvironment, it is imperative that the navigation system is able to recognize\nthe predictive uncertainty of the perception model and respond safely and\neffectively in the face of uncertainty. In an effort to enable safe navigation\nunder perception uncertainty, we develop a probabilistic and\nreconstruction-based competency estimation (PaRCE) method to estimate the\nmodel's level of familiarity with an input image as a whole and with specific\nregions in the image. We find that the overall competency score can correctly\npredict correctly classified, misclassified, and out-of-distribution (OOD)\nsamples. We also confirm that the regional competency maps can accurately\ndistinguish between familiar and unfamiliar regions across images. We then use\nthis competency information to develop a planning and control scheme that\nenables effective navigation while maintaining a low probability of error. We\nfind that the competency-aware scheme greatly reduces the number of collisions\nwith unfamiliar obstacles, compared to a baseline controller with no competency\nawareness. Furthermore, the regional competency information is very valuable in\nenabling efficient navigation.\n","authors":["Sara Pohland","Claire Tomlin"],"pdf_url":"https://arxiv.org/pdf/2409.06111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06105v1","updated":"2024-09-09T23:12:43Z","published":"2024-09-09T23:12:43Z","title":"SGC-VQGAN: Towards Complex Scene Representation via Semantic Guided\n  Clustering Codebook","summary":"  Vector quantization (VQ) is a method for deterministically learning features\nthrough discrete codebook representations. Recent works have utilized visual\ntokenizers to discretize visual regions for self-supervised representation\nlearning. However, a notable limitation of these tokenizers is lack of\nsemantics, as they are derived solely from the pretext task of reconstructing\nraw image pixels in an auto-encoder paradigm. Additionally, issues like\nimbalanced codebook distribution and codebook collapse can adversely impact\nperformance due to inefficient codebook utilization. To address these\nchallenges, We introduce SGC-VQGAN through Semantic Online Clustering method to\nenhance token semantics through Consistent Semantic Learning. Utilizing\ninference results from segmentation model , our approach constructs a\ntemporospatially consistent semantic codebook, addressing issues of codebook\ncollapse and imbalanced token semantics. Our proposed Pyramid Feature Learning\npipeline integrates multi-level features to capture both image details and\nsemantics simultaneously. As a result, SGC-VQGAN achieves SOTA performance in\nboth reconstruction quality and various downstream tasks. Its simplicity,\nrequiring no additional parameter learning, enables its direct application in\ndownstream tasks, presenting significant potential.\n","authors":["Chenjing Ding","Chiyu Wang","Boshi Liu","Xi Guo","Weixuan Tang","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2409.06105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06104v1","updated":"2024-09-09T23:11:46Z","published":"2024-09-09T23:11:46Z","title":"LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance\n  Fields with RGB-Event Stereo","summary":"  We present a method for reconstructing a clear Neural Radiance Field (NeRF)\neven with fast camera motions. To address blur artifacts, we leverage both\n(blurry) RGB images and event camera data captured in a binocular\nconfiguration. Importantly, when reconstructing our clear NeRF, we consider the\ncamera modeling imperfections that arise from the simple pinhole camera model\nas learned embeddings for each camera measurement, and further learn a mapper\nthat connects event camera measurements with RGB data. As no previous dataset\nexists for our binocular setting, we introduce an event camera dataset with\ncaptures from a 3D-printed stereo configuration between RGB and event cameras.\nEmpirically, we evaluate our introduced dataset and EVIMOv2 and show that our\nmethod leads to improved reconstructions. Our code and dataset are available at\nhttps://github.com/ubc-vision/LSENeRF.\n","authors":["Wei Zhi Tang","Daniel Rebain","Kostantinos G. Derpanis","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2409.06104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00158v2","updated":"2024-09-09T22:06:05Z","published":"2023-09-29T21:47:57Z","title":"Feedback-guided Data Synthesis for Imbalanced Classification","summary":"  Current status quo in machine learning is to use static datasets of real\nimages for training, which often come from long-tailed distributions. With the\nrecent advances in generative models, researchers have started augmenting these\nstatic datasets with synthetic data, reporting moderate performance\nimprovements on classification tasks. We hypothesize that these performance\ngains are limited by the lack of feedback from the classifier to the generative\nmodel, which would promote the usefulness of the generated samples to improve\nthe classifier's performance. In this work, we introduce a framework for\naugmenting static datasets with useful synthetic samples, which leverages\none-shot feedback from the classifier to drive the sampling of the generative\nmodel. In order for the framework to be effective, we find that the samples\nmust be close to the support of the real data of the task at hand, and be\nsufficiently diverse. We validate three feedback criteria on a long-tailed\ndataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On\nImageNet-LT, we achieve state-of-the-art results, with over 4 percent\nimprovement on underrepresented classes while being twice efficient in terms of\nthe number of generated synthetic samples. NICO++ also enjoys marked boosts of\nover 5 percent in worst group accuracy. With these results, our framework paves\nthe path towards effectively leveraging state-of-the-art text-to-image models\nas data sources that can be queried to improve downstream applications.\n","authors":["Reyhane Askari Hemmat","Mohammad Pezeshki","Florian Bordes","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2310.00158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08727v2","updated":"2024-09-09T21:41:42Z","published":"2023-03-15T16:12:14Z","title":"Improving Out-of-Distribution Detection with Disentangled Foreground and\n  Background Features","summary":"  Detecting out-of-distribution (OOD) inputs is a principal task for ensuring\nthe safety of deploying deep-neural-network classifiers in open-set scenarios.\nOOD samples can be drawn from arbitrary distributions and exhibit deviations\nfrom in-distribution (ID) data in various dimensions, such as foreground\nfeatures (e.g., objects in CIFAR100 images vs. those in CIFAR10 images) and\nbackground features (e.g., textural images vs. objects in CIFAR10). Existing\nmethods can confound foreground and background features in training, failing to\nutilize the background features for OOD detection. This paper considers the\nimportance of feature disentanglement in out-of-distribution detection and\nproposes the simultaneous exploitation of both foreground and background\nfeatures to support the detection of OOD inputs in in out-of-distribution\ndetection. To this end, we propose a novel framework that first disentangles\nforeground and background features from ID training samples via a dense\nprediction approach, and then learns a new classifier that can evaluate the OOD\nscores of test images from both foreground and background features. It is a\ngeneric framework that allows for a seamless combination with various existing\nOOD detection methods. Extensive experiments show that our approach 1) can\nsubstantially enhance the performance of four different state-of-the-art (SotA)\nOOD detection methods on multiple widely-used OOD datasets with diverse\nbackground features, and 2) achieves new SotA performance on these benchmarks.\n","authors":["Choubo Ding","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2303.08727v2.pdf","comment":"Accepted by ACM MM 2024, 9 pages"},{"id":"http://arxiv.org/abs/2409.06074v1","updated":"2024-09-09T21:14:44Z","published":"2024-09-09T21:14:44Z","title":"SVS-GAN: Leveraging GANs for Semantic Video Synthesis","summary":"  In recent years, there has been a growing interest in Semantic Image\nSynthesis (SIS) through the use of Generative Adversarial Networks (GANs) and\ndiffusion models. This field has seen innovations such as the implementation of\nspecialized loss functions tailored for this task, diverging from the more\ngeneral approaches in Image-to-Image (I2I) translation. While the concept of\nSemantic Video Synthesis (SVS)$\\unicode{x2013}$the generation of temporally\ncoherent, realistic sequences of images from semantic maps$\\unicode{x2013}$is\nnewly formalized in this paper, some existing methods have already explored\naspects of this field. Most of these approaches rely on generic loss functions\ndesigned for video-to-video translation or require additional data to achieve\ntemporal coherence. In this paper, we introduce the SVS-GAN, a framework\nspecifically designed for SVS, featuring a custom architecture and loss\nfunctions. Our approach includes a triple-pyramid generator that utilizes SPADE\nblocks. Additionally, we employ a U-Net-based network for the image\ndiscriminator, which performs semantic segmentation for the OASIS loss. Through\nthis combination of tailored architecture and objective engineering, our\nframework aims to bridge the existing gap between SIS and SVS, outperforming\ncurrent state-of-the-art models on datasets like Cityscapes and KITTI-360.\n","authors":["Khaled M. Seyam","Julian Wiederer","Markus Braun","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2409.06074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06065v1","updated":"2024-09-09T20:58:25Z","published":"2024-09-09T20:58:25Z","title":"DiffusionPen: Towards Controlling the Style of Handwritten Text\n  Generation","summary":"  Handwritten Text Generation (HTG) conditioned on text and style is a\nchallenging task due to the variability of inter-user characteristics and the\nunlimited combinations of characters that form new words unseen during\ntraining. Diffusion Models have recently shown promising results in HTG but\nstill remain under-explored. We present DiffusionPen (DiffPen), a 5-shot style\nhandwritten text generation approach based on Latent Diffusion Models. By\nutilizing a hybrid style extractor that combines metric learning and\nclassification, our approach manages to capture both textual and stylistic\ncharacteristics of seen and unseen words and styles, generating realistic\nhandwritten samples. Moreover, we explore several variation strategies of the\ndata with multi-style mixtures and noisy embeddings, enhancing the robustness\nand diversity of the generated data. Extensive experiments using IAM offline\nhandwriting database show that our method outperforms existing methods\nqualitatively and quantitatively, and its additional generated data can improve\nthe performance of Handwriting Text Recognition (HTR) systems. The code is\navailable at: https://github.com/koninik/DiffusionPen.\n","authors":["Konstantina Nikolaidou","George Retsinas","Giorgos Sfikas","Marcus Liwicki"],"pdf_url":"https://arxiv.org/pdf/2409.06065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19006v3","updated":"2024-09-09T20:51:07Z","published":"2024-06-27T08:45:31Z","title":"VideoMambaPro: A Leap Forward for Mamba in Video Understanding","summary":"  Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to computer vision tasks, including those in\nvideo analysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. VideoMambaPro shows state-of-the-art video action recognition\nperformance compared to transformer models, and surpasses VideoMamba by clear\nmargins: 7.9% and 8.1% top-1 on Kinetics-400 and Something-Something V2,\nrespectively. Our VideoMambaPro-M model achieves 91.9% top-1 on Kinetics-400,\nonly 0.2% below InternVideo2-6B but with only 1.2% of its parameters. The\ncombination of high performance and efficiency makes VideoMambaPro an\ninteresting alternative for transformer models.\n","authors":["Hui Lu","Albert Ali Salah","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2406.19006v3.pdf","comment":"Model weights are lost due to management error, will re-calculate and\n  update the results"},{"id":"http://arxiv.org/abs/2406.12044v2","updated":"2024-09-09T20:26:49Z","published":"2024-06-17T19:31:24Z","title":"ARTIST: Improving the Generation of Text-rich Images with Disentangled\n  Diffusion Models","summary":"  Diffusion models have demonstrated exceptional capabilities in generating a\nbroad spectrum of visual content, yet their proficiency in rendering text is\nstill limited: they often generate inaccurate characters or words that fail to\nblend well with the underlying image. To address these shortcomings, we\nintroduce a new framework named ARTIST. This framework incorporates a dedicated\ntextual diffusion model to specifically focus on the learning of text\nstructures. Initially, we pretrain this textual model to capture the\nintricacies of text representation. Subsequently, we finetune a visual\ndiffusion model, enabling it to assimilate textual structure information from\nthe pretrained textual model. This disentangled architecture design and the\ntraining strategy significantly enhance the text rendering ability of the\ndiffusion models for text-rich image generation. Additionally, we leverage the\ncapabilities of pretrained large language models to better interpret user\nintentions, contributing to improved generation quality. Empirical results on\nthe MARIO-Eval benchmark underscore the effectiveness of the proposed method,\nshowing an improvement of up to 15\\% in various metrics.\n","authors":["Jianyi Zhang","Yufan Zhou","Jiuxiang Gu","Curtis Wigington","Tong Yu","Yiran Chen","Tong Sun","Ruiyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.12044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06037v1","updated":"2024-09-09T19:58:42Z","published":"2024-09-09T19:58:42Z","title":"Online 3D reconstruction and dense tracking in endoscopic videos","summary":"  3D scene reconstruction from stereo endoscopic video data is crucial for\nadvancing surgical interventions. In this work, we present an online framework\nfor online, dense 3D scene reconstruction and tracking, aimed at enhancing\nsurgical scene understanding and assisting interventions. Our method\ndynamically extends a canonical scene representation using Gaussian splatting,\nwhile modeling tissue deformations through a sparse set of control points. We\nintroduce an efficient online fitting algorithm that optimizes the scene\nparameters, enabling consistent tracking and accurate reconstruction. Through\nexperiments on the StereoMIS dataset, we demonstrate the effectiveness of our\napproach, outperforming state-of-the-art tracking methods and achieving\ncomparable performance to offline reconstruction techniques. Our work enables\nvarious downstream applications thus contributing to advancing the capabilities\nof surgical assistance systems.\n","authors":["Michel Hayoz","Christopher Hahne","Thomas Kurmann","Max Allan","Guido Beldi","Daniel Candinas","ablo Márquez-Neila","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2409.06037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06035v1","updated":"2024-09-09T19:51:44Z","published":"2024-09-09T19:51:44Z","title":"Analyzing Tumors by Synthesis","summary":"  Computer-aided tumor detection has shown great potential in enhancing the\ninterpretation of over 80 million CT scans performed annually in the United\nStates. However, challenges arise due to the rarity of CT scans with tumors,\nespecially early-stage tumors. Developing AI with real tumor data faces issues\nof scarcity, annotation difficulty, and low prevalence. Tumor synthesis\naddresses these challenges by generating numerous tumor examples in medical\nimages, aiding AI training for tumor detection and segmentation. Successful\nsynthesis requires realistic and generalizable synthetic tumors across various\norgans. This chapter reviews AI development on real and synthetic data and\nsummarizes two key trends in synthetic data for cancer imaging research:\nmodeling-based and learning-based approaches. Modeling-based methods, like\nPixel2Cancer, simulate tumor development over time using generic rules, while\nlearning-based methods, like DiffTumor, learn from a few annotated examples in\none organ to generate synthetic tumors in others. Reader studies with expert\nradiologists show that synthetic tumors can be convincingly realistic. We also\npresent case studies in the liver, pancreas, and kidneys reveal that AI trained\non synthetic tumors can achieve performance comparable to, or better than, AI\nonly trained on real data. Tumor synthesis holds significant promise for\nexpanding datasets, enhancing AI reliability, improving tumor detection\nperformance, and preserving patient privacy.\n","authors":["Qi Chen","Yuxiang Lai","Xiaoxi Chen","Qixin Hu","Alan Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.06035v1.pdf","comment":"Accepted as a chapter in the Springer Book: \"Generative Machine\n  Learning Models in Medical Image Computing.\""},{"id":"http://arxiv.org/abs/2409.06030v1","updated":"2024-09-09T19:40:52Z","published":"2024-09-09T19:40:52Z","title":"NESI: Shape Representation via Neural Explicit Surface Intersection","summary":"  Compressed representations of 3D shapes that are compact, accurate, and can\nbe processed efficiently directly in compressed form, are extremely useful for\ndigital media applications. Recent approaches in this space focus on learned\nimplicit or parametric representations. While implicits are well suited for\ntasks such as in-out queries, they lack natural 2D parameterization,\ncomplicating tasks such as texture or normal mapping. Conversely, parametric\nrepresentations support the latter tasks but are ill-suited for occupancy\nqueries. We propose a novel learned alternative to these approaches, based on\nintersections of localized explicit, or height-field, surfaces. Since explicits\ncan be trivially expressed both implicitly and parametrically, NESI directly\nsupports a wider range of processing operations than implicit alternatives,\nincluding occupancy queries and parametric access. We represent input shapes\nusing a collection of differently oriented height-field bounded half-spaces\ncombined using volumetric Boolean intersections. We first tightly bound each\ninput using a pair of oppositely oriented height-fields, forming a Double\nHeight-Field (DHF) Hull. We refine this hull by intersecting it with additional\nlocalized height-fields (HFs) that capture surface regions in its interior. We\nminimize the number of HFs necessary to accurately capture each input and\ncompactly encode both the DHF hull and the local HFs as neural functions\ndefined over subdomains of R^2. This reduced dimensionality encoding delivers\nhigh-quality compact approximations. Given similar parameter count, or storage\ncapacity, NESI significantly reduces approximation error compared to the state\nof the art, especially at lower parameter counts.\n","authors":["Congyi Zhang","Jinfan Yang","Eric Hedlin","Suzuran Takikawa","Nicholas Vining","Kwang Moo Yi","Wenping Wang","Alla Sheffer"],"pdf_url":"https://arxiv.org/pdf/2409.06030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11133v3","updated":"2024-09-09T19:34:56Z","published":"2024-05-18T01:09:02Z","title":"XCAT-3.0: A Comprehensive Library of Personalized Digital Twins Derived\n  from CT Scans","summary":"  Virtual Imaging Trials (VIT) offer a cost-effective and scalable approach for\nevaluating medical imaging technologies. Computational phantoms, which mimic\nreal patient anatomy and physiology, play a central role in VITs. However, the\ncurrent libraries of computational phantoms face limitations, particularly in\nterms of sample size and diversity. Insufficient representation of the\npopulation hampers accurate assessment of imaging technologies across different\npatient groups. Traditionally, the more realistic computational phantoms were\ncreated by manual segmentation, which is a laborious and time-consuming task,\nimpeding the expansion of phantom libraries. This study presents a framework\nfor creating realistic computational phantoms using a suite of automatic\nsegmentation models and performing three forms of automated quality control on\nthe segmented organ masks. The result is the release of over 2500 new\ncomputational phantoms, so-named XCAT3.0 after the ubiquitous XCAT\ncomputational construct. This new formation embodies 140 structures and\nrepresents a comprehensive approach to detailed anatomical modeling. The\ndeveloped computational phantoms are formatted in both voxelized and surface\nmesh formats. The framework is combined with an in-house CT scanner simulator\nto produce realistic CT images. The framework has the potential to advance\nvirtual imaging trials, facilitating comprehensive and reliable evaluations of\nmedical imaging technologies. Phantoms may be requested at\nhttps://cvit.duke.edu/resources/. Code, model weights, and sample CT images are\navailable at https://xcat-3.github.io/.\n","authors":["Lavsen Dahal","Mobina Ghojoghnejad","Dhrubajyoti Ghosh","Yubraj Bhandari","David Kim","Fong Chi Ho","Fakrul Islam Tushar","Sheng Luoa","Kyle J. Lafata","Ehsan Abadi","Ehsan Samei","Joseph Y. Lo","W. Paul Segars"],"pdf_url":"https://arxiv.org/pdf/2405.11133v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05376v5","updated":"2024-09-09T19:28:16Z","published":"2022-12-10T23:48:27Z","title":"What's Wrong with the Absolute Trajectory Error?","summary":"  One of the limitations of the commonly used Absolute Trajectory Error (ATE)\nis that it is highly sensitive to outliers. As a result, in the presence of\njust a few outliers, it often fails to reflect the varying accuracy as the\ninlier trajectory error or the number of outliers varies. In this work, we\npropose an alternative error metric for evaluating the accuracy of the\nreconstructed camera trajectory. Our metric, named Discernible Trajectory Error\n(DTE), is computed in five steps: (1) Shift the ground-truth and estimated\ntrajectories such that both of their geometric medians are located at the\norigin. (2) Rotate the estimated trajectory such that it minimizes the sum of\ngeodesic distances between the corresponding camera orientations. (3) Scale the\nestimated trajectory such that the median distance of the cameras to their\ngeometric median is the same as that of the ground truth. (4) Compute,\nwinsorize and normalize the distances between the corresponding cameras. (5)\nObtain the DTE by taking the average of the mean and the root-mean-square (RMS)\nof the resulting distances. This metric is an attractive alternative to the\nATE, in that it is capable of discerning the varying trajectory accuracy as the\ninlier trajectory error or the number of outliers varies. Using the similar\nidea, we also propose a novel rotation error metric, named Discernible Rotation\nError (DRE), which has similar advantages to the DTE. Furthermore, we propose a\nsimple yet effective method for calibrating the camera-to-marker rotation,\nwhich is needed for the computation of our metrics. Our methods are verified\nthrough extensive simulations.\n","authors":["Seong Hun Lee","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2212.05376v5.pdf","comment":"The main part of this manuscript (except the part on DRE) has been\n  accepted to ECCV 2024 Workshop HALF-CENTURY OF STRUCTURE-FROM-MOTION (50SFM)"},{"id":"http://arxiv.org/abs/2409.06018v1","updated":"2024-09-09T19:22:17Z","published":"2024-09-09T19:22:17Z","title":"Pioneering Precision in Lumbar Spine MRI Segmentation with Advanced Deep\n  Learning and Data Enhancement","summary":"  This study presents an advanced approach to lumbar spine segmentation using\ndeep learning techniques, focusing on addressing key challenges such as class\nimbalance and data preprocessing. Magnetic resonance imaging (MRI) scans of\npatients with low back pain are meticulously preprocessed to accurately\nrepresent three critical classes: vertebrae, spinal canal, and intervertebral\ndiscs (IVDs). By rectifying class inconsistencies in the data preprocessing\nstage, the fidelity of the training data is ensured. The modified U-Net model\nincorporates innovative architectural enhancements, including an upsample block\nwith leaky Rectified Linear Units (ReLU) and Glorot uniform initializer, to\nmitigate common issues such as the dying ReLU problem and improve stability\nduring training. Introducing a custom combined loss function effectively\ntackles class imbalance, significantly improving segmentation accuracy.\nEvaluation using a comprehensive suite of metrics showcases the superior\nperformance of this approach, outperforming existing methods and advancing the\ncurrent techniques in lumbar spine segmentation. These findings hold\nsignificant advancements for enhanced lumbar spine MRI and segmentation\ndiagnostic accuracy.\n","authors":["Istiak Ahmed","Md. Tanzim Hossain","Md. Zahirul Islam Nahid","Kazi Shahriar Sanjid","Md. Shakib Shahariar Junayed","M. Monir Uddin","Mohammad Monirujjaman Khan"],"pdf_url":"https://arxiv.org/pdf/2409.06018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06013v1","updated":"2024-09-09T19:12:03Z","published":"2024-09-09T19:12:03Z","title":"Improved Visually Prompted Keyword Localisation in Real Low-Resource\n  Settings","summary":"  Given an image query, visually prompted keyword localisation (VPKL) aims to\nfind occurrences of the depicted word in a speech collection. This can be\nuseful when transcriptions are not available for a low-resource language (e.g.\nif it is unwritten). Previous work showed that VPKL can be performed with a\nvisually grounded speech model trained on paired images and unlabelled speech.\nBut all experiments were done on English. Moreover, transcriptions were used to\nget positive and negative pairs for the contrastive loss. This paper introduces\na few-shot learning scheme to mine pairs automatically without transcriptions.\nOn English, this results in only a small drop in performance. We also - for the\nfirst time - consider VPKL on a real low-resource language, Yoruba. While\nscores are reasonable, here we see a bigger drop in performance compared to\nusing ground truth pairs because the mining is less accurate in Yoruba.\n","authors":["Leanne Nortje","Dan Oneata","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2409.06013v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.05667v2","updated":"2024-09-09T23:46:59Z","published":"2024-08-11T01:14:13Z","title":"PhishLang: A Lightweight, Client-Side Phishing Detection Framework using\n  MobileBERT for Real-Time, Explainable Threat Mitigation","summary":"  In this paper, we introduce PhishLang, an open-source, lightweight language\nmodel specifically designed for phishing website detection through contextual\nanalysis of the website. Unlike traditional heuristic or machine learning\nmodels that rely on static features and struggle to adapt to new threats, and\ndeep learning models that are computationally intensive, our model leverages\nMobileBERT, a fast and memory-efficient variant of the BERT architecture, to\nlearn granular features characteristic of phishing attacks. PhishLang operates\nwith minimal data preprocessing and offers performance comparable to leading\ndeep learning anti-phishing tools, while being significantly faster and less\nresource-intensive. Over a 3.5-month testing period, PhishLang successfully\nidentified 25,796 phishing URLs, many of which were undetected by popular\nantiphishing blocklists, thus demonstrating its potential to enhance current\ndetection measures. Capitalizing on PhishLang's resource efficiency, we release\nthe first open-source fully client-side Chromium browser extension that\nprovides inference locally without requiring to consult an online blocklist and\ncan be run on low-end systems with no impact on inference times. Our\nimplementation not only outperforms prevalent (server-side) phishing tools, but\nis significantly more effective than the limited commercial client-side\nmeasures available. Furthermore, we study how PhishLang can be integrated with\nGPT-3.5 Turbo to create explainable blocklisting -- which, upon detection of a\nwebsite, provides users with detailed contextual information about the features\nthat led to a website being marked as phishing.\n","authors":["Sayak Saha Roy","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2408.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06096v1","updated":"2024-09-09T22:16:48Z","published":"2024-09-09T22:16:48Z","title":"Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer","summary":"  Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.\n","authors":["Michele Mancusi","Yurii Halychansky","Kin Wai Cheuk","Chieh-Hsin Lai","Stefan Uhlich","Junghyun Koo","Marco A. Martínez-Ramírez","Wei-Hsiang Liao","Giorgio Fabbro","Yuhki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2409.06096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05806v1","updated":"2024-09-09T17:11:51Z","published":"2024-09-09T17:11:51Z","title":"Benchmarking Chinese Knowledge Rectification in Large Language Models","summary":"  While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Tianhe Lu","Jizhan Fang","Yunzhi Yao","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v1.pdf","comment":"Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2409.05692v1","updated":"2024-09-09T15:05:27Z","published":"2024-09-09T15:05:27Z","title":"Extracting the U.S. building types from OpenStreetMap data","summary":"  Building type information is crucial for population estimation, traffic\nplanning, urban planning, and emergency response applications. Although\nessential, such data is often not readily available. To alleviate this problem,\nthis work creates a comprehensive dataset by providing\nresidential/non-residential building classification covering the entire United\nStates. We propose and utilize an unsupervised machine learning method to\nclassify building types based on building footprints and available\nOpenStreetMap information. The classification result is validated using\nauthoritative ground truth data for select counties in the U.S. The validation\nshows a high precision for non-residential building classification and a high\nrecall for residential buildings. We identified various approaches to improving\nthe quality of the classification, such as removing sheds and garages from the\ndataset. Furthermore, analyzing the misclassifications revealed that they are\nmainly due to missing and scarce metadata in OSM. A major result of this work\nis the resulting dataset of classifying 67,705,475 buildings. We hope that this\ndata is of value to the scientific community, including urban and\ntransportation planners.\n","authors":["Henrique F. de Arruda","Sandro M. Reia","Shiyang Ruan","Kuldip S. Atwal","Hamdi Kavak","Taylor Anderson","Dieter Pfoser"],"pdf_url":"https://arxiv.org/pdf/2409.05692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05677v1","updated":"2024-09-09T14:44:19Z","published":"2024-09-09T14:44:19Z","title":"RegNLP in Action: Facilitating Compliance Through Automated Information\n  Retrieval and Answer Generation","summary":"  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance.Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary subfield aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We define an Automated Question-Passage\nGeneration task for RegNLP, create the ObliQA dataset containing 27,869\nquestions derived from the Abu Dhabi Global Markets (ADGM) financial regulation\ndocument collection, design a baseline Regulatory Information Retrieval and\nAnswer Generation system, and evaluate it with RePASs, a novel evaluation\nmetric that tests whether generated answers accurately capture all relevant\nobligations and avoid contradictions.\n","authors":["Tuba Gokhan","Kexin Wang","Iryna Gurevych","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2409.05677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05633v1","updated":"2024-09-09T14:04:17Z","published":"2024-09-09T14:04:17Z","title":"Enhancing Graph Contrastive Learning with Reliable and Informative\n  Augmentation for Recommendation","summary":"  Graph neural network (GNN) has been a powerful approach in collaborative\nfiltering (CF) due to its ability to model high-order user-item relationships.\nRecently, to alleviate the data sparsity and enhance representation learning,\nmany efforts have been conducted to integrate contrastive learning (CL) with\nGNNs. Despite the promising improvements, the contrastive view generation based\non structure and representation perturbations in existing methods potentially\ndisrupts the collaborative information in contrastive views, resulting in\nlimited effectiveness of positive alignment. To overcome this issue, we propose\nCoGCL, a novel framework that aims to enhance graph contrastive learning by\nconstructing contrastive views with stronger collaborative information via\ndiscrete codes. The core idea is to map users and items into discrete codes\nrich in collaborative information for reliable and informative contrastive view\ngeneration. To this end, we initially introduce a multi-level vector quantizer\nin an end-to-end manner to quantize user and item representations into discrete\ncodes. Based on these discrete codes, we enhance the collaborative information\nof contrastive views by considering neighborhood structure and semantic\nrelevance respectively. For neighborhood structure, we propose virtual neighbor\naugmentation by treating discrete codes as virtual neighbors, which expands an\nobserved user-item interaction into multiple edges involving discrete codes.\nRegarding semantic relevance, we identify similar users/items based on shared\ndiscrete codes and interaction targets to generate the semantically relevant\nview. Through these strategies, we construct contrastive views with stronger\ncollaborative information and develop a triple-view graph contrastive learning\napproach. Extensive experiments on four public datasets demonstrate the\neffectiveness of our proposed approach.\n","authors":["Bowen Zheng","Junjie Zhang","Hongyu Lu","Yu Chen","Ming Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2409.05633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05570v1","updated":"2024-09-09T12:53:06Z","published":"2024-09-09T12:53:06Z","title":"Rs4rs: Semantically Find Recent Publications from Top Recommendation\n  System-Related Venues","summary":"  Rs4rs is a web application designed to perform semantic search on recent\npapers from top conferences and journals related to Recommender Systems.\nCurrent scholarly search engine tools like Google Scholar, Semantic Scholar,\nand ResearchGate often yield broad results that fail to target the most\nrelevant high-quality publications. Moreover, manually visiting individual\nconference and journal websites is a time-consuming process that primarily\nsupports only syntactic searches. Rs4rs addresses these issues by providing a\nuser-friendly platform where researchers can input their topic of interest and\nreceive a list of recent, relevant papers from top Recommender Systems venues.\nUtilizing semantic search techniques, Rs4rs ensures that the search results are\nnot only precise and relevant but also comprehensive, capturing papers\nregardless of variations in wording. This tool significantly enhances research\nefficiency and accuracy, thereby benefitting the research community and public\nby facilitating access to high-quality, pertinent academic resources in the\nfield of Recommender Systems. Rs4rs is available at https://rs4rs.com.\n","authors":["Tri Kurniawan Wijaya","Edoardo D'Amico","Gabor Fodor","Manuel V. Loureiro"],"pdf_url":"https://arxiv.org/pdf/2409.05570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05546v1","updated":"2024-09-09T12:11:53Z","published":"2024-09-09T12:11:53Z","title":"End-to-End Learnable Item Tokenization for Generative Recommendation","summary":"  Recently, generative recommendation has emerged as a promising new paradigm\nthat directly generates item identifiers for recommendation. However, a key\nchallenge lies in how to effectively construct item identifiers that are\nsuitable for recommender systems. Existing methods typically decouple item\ntokenization from subsequent generative recommendation training, likely\nresulting in suboptimal performance. To address this limitation, we propose\nETEGRec, a novel End-To-End Generative Recommender by seamlessly integrating\nitem tokenization and generative recommendation. Our framework is developed\nbased on the dual encoder-decoder architecture, which consists of an item\ntokenizer and a generative recommender. In order to achieve mutual enhancement\nbetween the two components, we propose a recommendation-oriented alignment\napproach by devising two specific optimization objectives: sequence-item\nalignment and preference-semantic alignment. These two alignment objectives can\neffectively couple the learning of item tokenizer and generative recommender,\nthereby fostering the mutual enhancement between the two components. Finally,\nwe further devise an alternating optimization method, to facilitate stable and\neffective end-to-end learning of the entire framework. Extensive experiments\ndemonstrate the effectiveness of our proposed framework compared to a series of\ntraditional sequential recommendation models and generative recommendation\nbaselines.\n","authors":["Enze Liu","Bowen Zheng","Cheng Ling","Lantao Hu","Han Li","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.05546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05512v1","updated":"2024-09-09T11:10:45Z","published":"2024-09-09T11:10:45Z","title":"DatAasee -- A Metadata-Lake as Metadata Catalog for a Virtual Data-Lake","summary":"  Metadata management for distributed data sources is a long-standing but\never-growing problem. To counter this challenge in a research-data and\nlibrary-oriented setting, this work constructs a data architecture, derived\nfrom the data-lake: the metadata-lake. A proof-of-concept implementation of\nthis proposed metadata system is presented and evaluated as well.\n","authors":["Christian Himpe"],"pdf_url":"https://arxiv.org/pdf/2409.05512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03753v2","updated":"2024-09-09T10:04:00Z","published":"2024-09-05T17:59:15Z","title":"WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild","summary":"  The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.\n","authors":["Yuntian Deng","Wenting Zhao","Jack Hessel","Xiang Ren","Claire Cardie","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2409.03753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05462v1","updated":"2024-09-09T09:42:46Z","published":"2024-09-09T09:42:46Z","title":"Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning","summary":"  For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples.\n","authors":["Jibin Jia","Peihao Dong","Fuhui Zhou","Qihui Wu"],"pdf_url":"https://arxiv.org/pdf/2409.05462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05461v1","updated":"2024-09-09T09:42:31Z","published":"2024-09-09T09:42:31Z","title":"Recommender Systems Algorithm Selection for Ranking Prediction on\n  Implicit Feedback Datasets","summary":"  The recommender systems algorithm selection problem for ranking prediction on\nimplicit feedback datasets is under-explored. Traditional approaches in\nrecommender systems algorithm selection focus predominantly on rating\nprediction on explicit feedback datasets, leaving a research gap for ranking\nprediction on implicit feedback datasets. Algorithm selection is a critical\nchallenge for nearly every practitioner in recommender systems. In this work,\nwe take the first steps toward addressing this research gap. We evaluate the\nNDCG@10 of 24 recommender systems algorithms, each with two hyperparameter\nconfigurations, on 72 recommender systems datasets. We train four optimized\nmachine-learning meta-models and one automated machine-learning meta-model with\nthree different settings on the resulting meta-dataset. Our results show that\nthe predictions of all tested meta-models exhibit a median Spearman correlation\nranging from 0.857 to 0.918 with the ground truth. We show that the median\nSpearman correlation between meta-model predictions and the ground truth\nincreases by an average of 0.124 when the meta-model is optimized to predict\nthe ranking of algorithms instead of their performance. Furthermore, in terms\nof predicting the best algorithm for an unknown dataset, we demonstrate that\nthe best optimized traditional meta-model, e.g., XGBoost, achieves a recall of\n48.6%, outperforming the best tested automated machine learning meta-model,\ne.g., AutoGluon, which achieves a recall of 47.2%.\n","authors":["Lukas Wegmeth","Tobias Vente","Joeran Beel"],"pdf_url":"https://arxiv.org/pdf/2409.05461v1.pdf","comment":"Accepted for presentation at the 18th ACM Conference on Recommender\n  Systems in the Late-Breaking Results Track"},{"id":"http://arxiv.org/abs/2409.05925v1","updated":"2024-09-09T08:29:39Z","published":"2024-09-09T08:29:39Z","title":"Assessing SPARQL capabilities of Large Language Models","summary":"  The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.\n","authors":["Lars-Peter Meyer","Johannes Frey","Felix Brei","Natanael Arndt"],"pdf_url":"https://arxiv.org/pdf/2409.05925v1.pdf","comment":"peer reviewed publication at NLP4KGc @ Semantics 2024, see\n  https://sites.google.com/view/3rdnlp4kgc"},{"id":"http://arxiv.org/abs/2409.05417v1","updated":"2024-09-09T08:19:43Z","published":"2024-09-09T08:19:43Z","title":"Replicability Measures for Longitudinal Information Retrieval Evaluation","summary":"  Information Retrieval (IR) systems are exposed to constant changes in most\ncomponents. Documents are created, updated, or deleted, the information needs\nare changing, and even relevance might not be static. While it is generally\nexpected that the IR systems retain a consistent utility for the users, test\ncollection evaluations rely on a fixed experimental setup. Based on the\nLongEval shared task and test collection, this work explores how the\neffectiveness measured in evolving experiments can be assessed. Specifically,\nthe persistency of effectiveness is investigated as a replicability task. It is\nobserved how the effectiveness progressively deteriorates over time compared to\nthe initial measurement. Employing adapted replicability measures provides\nfurther insight into the persistence of effectiveness. The ranking of systems\nvaries across retrieval measures and time. In conclusion, it was found that the\nmost effective systems are not necessarily the ones with the most persistent\nperformance.\n","authors":["Jüri Keller","Timo Breuer","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2409.05417v1.pdf","comment":"Experimental IR Meets Multilinguality, Multimodality, and Interaction\n  - 15th International Conference of the CLEF Association, CLEF 2024, Grenoble,\n  France, September 9-12, 2024, Proceedings. arXiv admin note: text overlap\n  with arXiv:2308.10549"},{"id":"http://arxiv.org/abs/2409.05405v1","updated":"2024-09-09T08:06:50Z","published":"2024-09-09T08:06:50Z","title":"A Survey of Multimodal Composite Editing and Retrieval","summary":"  In the real world, where information is abundant and diverse across different\nmodalities, understanding and utilizing various data types to improve retrieval\nsystems is a key focus of research. Multimodal composite retrieval integrates\ndiverse modalities such as text, image and audio, etc. to provide more\naccurate, personalized, and contextually relevant results. To facilitate a\ndeeper understanding of this promising direction, this survey explores\nmultimodal composite editing and retrieval in depth, covering image-text\ncomposite editing, image-text composite retrieval, and other multimodal\ncomposite retrieval. In this survey, we systematically organize the application\nscenarios, methods, benchmarks, experiments, and future directions. Multimodal\nlearning is a hot topic in large model era, and have also witnessed some\nsurveys in multimodal learning and vision-language models with transformers\npublished in the PAMI journal. To the best of our knowledge, this survey is the\nfirst comprehensive review of the literature on multimodal composite retrieval,\nwhich is a timely complement of multimodal fusion to existing reviews. To help\nreaders' quickly track this field, we build the project page for this survey,\nwhich can be found at\nhttps://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.\n","authors":["Suyan Li","Fuxiang Huang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05405v1.pdf","comment":"22 pages, 3 figures, and 11 tables"},{"id":"http://arxiv.org/abs/2409.05401v1","updated":"2024-09-09T07:57:43Z","published":"2024-09-09T07:57:43Z","title":"NLLB-E5: A Scalable Multilingual Retrieval Model","summary":"  Despite significant progress in multilingual information retrieval, the lack\nof models capable of effectively supporting multiple languages, particularly\nlow-resource like Indic languages, remains a critical challenge. This paper\npresents NLLB-E5: A Scalable Multilingual Retrieval Model. NLLB-E5 leverages\nthe in-built multilingual capabilities in the NLLB encoder for translation\ntasks. It proposes a distillation approach from multilingual retriever E5 to\nprovide a zero-shot retrieval approach handling multiple languages, including\nall major Indic languages, without requiring multilingual training data. We\nevaluate the model on a comprehensive suite of existing benchmarks, including\nHindi-BEIR, highlighting its robust performance across diverse languages and\ntasks. Our findings uncover task and domain-specific challenges, providing\nvaluable insights into the retrieval performance, especially for low-resource\nlanguages. NLLB-E5 addresses the urgent need for an inclusive, scalable, and\nlanguage-agnostic text retrieval model, advancing the field of multilingual\ninformation access and promoting digital inclusivity for millions of users\nglobally.\n","authors":["Arkadeep Acharya","Rudra Murthy","Vishwajeet Kumar","Jaydeep Sen"],"pdf_url":"https://arxiv.org/pdf/2409.05401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03893v2","updated":"2024-09-09T07:47:58Z","published":"2024-09-05T19:59:42Z","title":"Understanding Fairness in Recommender Systems: A Healthcare Perspective","summary":"  Fairness in AI-driven decision-making systems has become a critical concern,\nespecially when these systems directly affect human lives. This paper explores\nthe public's comprehension of fairness in healthcare recommendations. We\nconducted a survey where participants selected from four fairness metrics --\nDemographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive\nValue -- across different healthcare scenarios to assess their understanding of\nthese concepts. Our findings reveal that fairness is a complex and often\nmisunderstood concept, with a generally low level of public understanding\nregarding fairness metrics in recommender systems. This study highlights the\nneed for enhanced information and education on algorithmic fairness to support\ninformed decision-making in using these systems. Furthermore, the results\nsuggest that a one-size-fits-all approach to fairness may be insufficient,\npointing to the importance of context-sensitive designs in developing equitable\nAI systems.\n","authors":["Veronica Kecki","Alan Said"],"pdf_url":"https://arxiv.org/pdf/2409.03893v2.pdf","comment":"Accepted to the 18th ACM Conference on Recommender Systems"},{"id":"http://arxiv.org/abs/2407.14482v2","updated":"2024-09-09T06:19:07Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Chejian Xu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v2.pdf","comment":"v2: major update with significantly improved results"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.05667v2","updated":"2024-09-09T23:46:59Z","published":"2024-08-11T01:14:13Z","title":"PhishLang: A Lightweight, Client-Side Phishing Detection Framework using\n  MobileBERT for Real-Time, Explainable Threat Mitigation","summary":"  In this paper, we introduce PhishLang, an open-source, lightweight language\nmodel specifically designed for phishing website detection through contextual\nanalysis of the website. Unlike traditional heuristic or machine learning\nmodels that rely on static features and struggle to adapt to new threats, and\ndeep learning models that are computationally intensive, our model leverages\nMobileBERT, a fast and memory-efficient variant of the BERT architecture, to\nlearn granular features characteristic of phishing attacks. PhishLang operates\nwith minimal data preprocessing and offers performance comparable to leading\ndeep learning anti-phishing tools, while being significantly faster and less\nresource-intensive. Over a 3.5-month testing period, PhishLang successfully\nidentified 25,796 phishing URLs, many of which were undetected by popular\nantiphishing blocklists, thus demonstrating its potential to enhance current\ndetection measures. Capitalizing on PhishLang's resource efficiency, we release\nthe first open-source fully client-side Chromium browser extension that\nprovides inference locally without requiring to consult an online blocklist and\ncan be run on low-end systems with no impact on inference times. Our\nimplementation not only outperforms prevalent (server-side) phishing tools, but\nis significantly more effective than the limited commercial client-side\nmeasures available. Furthermore, we study how PhishLang can be integrated with\nGPT-3.5 Turbo to create explainable blocklisting -- which, upon detection of a\nwebsite, provides users with detailed contextual information about the features\nthat led to a website being marked as phishing.\n","authors":["Sayak Saha Roy","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2408.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06206v2","updated":"2024-09-09T22:11:39Z","published":"2024-05-10T02:44:25Z","title":"Concealing Backdoor Model Updates in Federated Learning by\n  Trigger-Optimized Data Poisoning","summary":"  Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on analyzing clients' model updates\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign client model updates. To effectively\nconceal malicious model updates among benign ones, we propose DPOT, a backdoor\nattack strategy in FL that dynamically constructs backdoor objectives by\noptimizing a backdoor trigger, making backdoor data have minimal effect on\nmodel updates. We provide theoretical justifications for DPOT's attacking\nprinciple and display experimental results showing that DPOT, via only a\ndata-poisoning attack, effectively undermines state-of-the-art defenses and\noutperforms existing backdoor attack techniques on various datasets.\n","authors":["Yujie Zhang","Neil Gong","Michael K. Reiter"],"pdf_url":"https://arxiv.org/pdf/2405.06206v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00158v2","updated":"2024-09-09T22:06:05Z","published":"2023-09-29T21:47:57Z","title":"Feedback-guided Data Synthesis for Imbalanced Classification","summary":"  Current status quo in machine learning is to use static datasets of real\nimages for training, which often come from long-tailed distributions. With the\nrecent advances in generative models, researchers have started augmenting these\nstatic datasets with synthetic data, reporting moderate performance\nimprovements on classification tasks. We hypothesize that these performance\ngains are limited by the lack of feedback from the classifier to the generative\nmodel, which would promote the usefulness of the generated samples to improve\nthe classifier's performance. In this work, we introduce a framework for\naugmenting static datasets with useful synthetic samples, which leverages\none-shot feedback from the classifier to drive the sampling of the generative\nmodel. In order for the framework to be effective, we find that the samples\nmust be close to the support of the real data of the task at hand, and be\nsufficiently diverse. We validate three feedback criteria on a long-tailed\ndataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On\nImageNet-LT, we achieve state-of-the-art results, with over 4 percent\nimprovement on underrepresented classes while being twice efficient in terms of\nthe number of generated synthetic samples. NICO++ also enjoys marked boosts of\nover 5 percent in worst group accuracy. With these results, our framework paves\nthe path towards effectively leveraging state-of-the-art text-to-image models\nas data sources that can be queried to improve downstream applications.\n","authors":["Reyhane Askari Hemmat","Mohammad Pezeshki","Florian Bordes","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2310.00158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15056v3","updated":"2024-09-09T22:02:13Z","published":"2023-06-26T20:40:29Z","title":"Optimal Differentially Private Model Training with Public Data","summary":"  Differential privacy (DP) ensures that training a machine learning model does\nnot leak private data. In practice, we may have access to auxiliary public data\nthat is free of privacy concerns. In this work, we assume access to a given\namount of public data and settle the following fundamental open questions: 1.\nWhat is the optimal (worst-case) error of a DP model trained over a private\ndata set while having access to side public data? 2. How can we harness public\ndata to improve DP model training in practice? We consider these questions in\nboth the local and central models of pure and approximate DP. To answer the\nfirst question, we prove tight (up to log factors) lower and upper bounds that\ncharacterize the optimal error rates of three fundamental problems: mean\nestimation, empirical risk minimization, and stochastic convex optimization. We\nshow that the optimal error rates can be attained (up to log factors) by either\ndiscarding private data and training a public model, or treating public data\nlike it is private and using an optimal DP algorithm. To address the second\nquestion, we develop novel algorithms that are \"even more optimal\" (i.e. better\nconstants) than the asymptotically optimal approaches described above. For\nlocal DP mean estimation, our algorithm is optimal including constants.\nEmpirically, our algorithms show benefits over the state-of-the-art.\n","authors":["Andrew Lowy","Zeman Li","Tianjian Huang","Meisam Razaviyayn"],"pdf_url":"https://arxiv.org/pdf/2306.15056v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2409.06091v1","updated":"2024-09-09T21:59:27Z","published":"2024-09-09T21:59:27Z","title":"Scalable Multitask Learning Using Gradient-based Estimation of Task\n  Affinity","summary":"  Multitask learning is a widely used paradigm for training models on diverse\ntasks, with applications ranging from graph neural networks to language model\nfine-tuning. Since tasks may interfere with each other, a key notion for\nmodeling their relationships is task affinity. This includes pairwise task\naffinity, computed among pairs of tasks, and higher-order affinity, computed\namong subsets of tasks. Naively computing either of them requires repeatedly\ntraining on data from various task combinations, which is computationally\nintensive. We present a new algorithm Grad-TAG that can estimate task\naffinities without this repeated training.\n  The key idea of Grad-TAG is to train a \"base\" model for all tasks and then\nuse a linearization technique to estimate the loss of the model for a specific\ntask combination. The linearization works by computing a gradient-based\napproximation of the loss, using low-dimensional projections of gradients as\nfeatures in a logistic regression to predict labels for the task combination.\nWe show that the linearized model can provably approximate the loss when the\ngradient-based approximation is accurate, and also empirically verify that on\nseveral large models. Then, given the estimated task affinity, we design a\nsemi-definite program for clustering similar tasks by maximizing the average\ndensity of clusters.\n  We evaluate Grad-TAG's performance across seven datasets, including\nmulti-label classification on graphs, and instruction fine-tuning of language\nmodels. Our task affinity estimates are within 2.7% distance to the true\naffinities while needing only 3% of FLOPs in full training. On our largest\ngraph with 21M edges and 500 labeling tasks, our algorithm delivers estimates\nwithin 5% distance to the true affinities, using only 112 GPU hours. Our\nresults show that Grad-TAG achieves excellent performance and runtime tradeoffs\ncompared to existing approaches.\n","authors":["Dongyue Li","Aneesh Sharma","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.06091v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.06085v1","updated":"2024-09-09T21:36:38Z","published":"2024-09-09T21:36:38Z","title":"Differentiable programming across the PDE and Machine Learning barrier","summary":"  The combination of machine learning and physical laws has shown immense\npotential for solving scientific problems driven by partial differential\nequations (PDEs) with the promise of fast inference, zero-shot generalisation,\nand the ability to discover new physics. Examples include the use of\nfundamental physical laws as inductive bias to machine learning algorithms,\nalso referred to as physics-driven machine learning, and the application of\nmachine learning to represent features not represented in the differential\nequations such as closures for unresolved spatiotemporal scales. However, the\nsimulation of complex physical systems by coupling advanced numerics for PDEs\nwith state-of-the-art machine learning demands the composition of specialist\nPDE solving frameworks with industry-standard machine learning tools.\nHand-rolling either the PDE solver or the neural net will not cut it. In this\nwork, we introduce a generic differentiable programming abstraction that\nprovides scientists and engineers with a highly productive way of specifying\nend-to-end differentiable models coupling machine learning and PDE-based\ncomponents, while relying on code generation for high performance. Our\ninterface automates the coupling of arbitrary PDE-based systems and machine\nlearning models and unlocks new applications that could not hitherto be\ntackled, while only requiring trivial changes to existing code. Our framework\nhas been adopted in the Firedrake finite-element library and supports the\nPyTorch and JAX ecosystems, as well as downstream libraries.\n","authors":["Nacime Bouziani","David A. Ham","Ado Farsi"],"pdf_url":"https://arxiv.org/pdf/2409.06085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06084v1","updated":"2024-09-09T21:36:08Z","published":"2024-09-09T21:36:08Z","title":"Symmetry constrained neural networks for detection and localization of\n  damage in metal plates","summary":"  The present paper is concerned with deep learning techniques applied to\ndetection and localization of damage in a thin aluminum plate. We used data\ngenerated on a tabletop apparatus by mounting to the plate four piezoelectric\ntransducers, each of which took turn to generate a Lamb wave that then\ntraversed the region of interest before being received by the remaining three\nsensors. On training a neural network to analyze time-series data of the\nmaterial response, which displayed damage-reflective features whenever the\nplate guided waves interacted with a contact load, we achieved a model that\ndetected with greater than 99% accuracy in addition to a model that localized\nwith $3.14 \\pm 0.21$ mm mean distance error and captured more than 60% of test\nexamples within the diffraction limit. For each task, the best-performing model\nwas designed according to the inductive bias that our transducers were both\nsimilar and arranged in a square pattern on a nearly uniform plate.\n","authors":["James Amarel","Christopher Rudolf","Athanasios Iliopoulos","John Michopoulos","Leslie N. Smith"],"pdf_url":"https://arxiv.org/pdf/2409.06084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06080v1","updated":"2024-09-09T21:26:32Z","published":"2024-09-09T21:26:32Z","title":"Regression with Large Language Models for Materials and Molecular\n  Property Prediction","summary":"  We demonstrate the ability of large language models (LLMs) to perform\nmaterial and molecular property regression tasks, a significant deviation from\nthe conventional LLM use case. We benchmark the Large Language Model Meta AI\n(LLaMA) 3 on several molecular properties in the QM9 dataset and 24 materials\nproperties. Only composition-based input strings are used as the model input\nand we fine tune on only the generative loss. We broadly find that LLaMA 3,\nwhen fine-tuned using the SMILES representation of molecules, provides useful\nregression results which can rival standard materials property prediction\nmodels like random forest or fully connected neural networks on the QM9\ndataset. Not surprisingly, LLaMA 3 errors are 5-10x higher than those of the\nstate-of-the-art models that were trained using far more granular\nrepresentation of molecules (e.g., atom types and their coordinates) for the\nsame task. Interestingly, LLaMA 3 provides improved predictions compared to\nGPT-3.5 and GPT-4o. This work highlights the versatility of LLMs, suggesting\nthat LLM-like generative models can potentially transcend their traditional\napplications to tackle complex physical phenomena, thus paving the way for\nfuture research and applications in chemistry, materials science and other\nscientific domains.\n","authors":["Ryan Jacobs","Maciej P. Polak","Lane E. Schultz","Hamed Mahdavi","Vasant Honavar","Dane Morgan"],"pdf_url":"https://arxiv.org/pdf/2409.06080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06077v1","updated":"2024-09-09T21:20:36Z","published":"2024-09-09T21:20:36Z","title":"MTLSO: A Multi-Task Learning Approach for Logic Synthesis Optimization","summary":"  Electronic Design Automation (EDA) is essential for IC design and has\nrecently benefited from AI-based techniques to improve efficiency. Logic\nsynthesis, a key EDA stage, transforms high-level hardware descriptions into\noptimized netlists. Recent research has employed machine learning to predict\nQuality of Results (QoR) for pairs of And-Inverter Graphs (AIGs) and synthesis\nrecipes. However, the severe scarcity of data due to a very limited number of\navailable AIGs results in overfitting, significantly hindering performance.\nAdditionally, the complexity and large number of nodes in AIGs make plain GNNs\nless effective for learning expressive graph-level representations. To tackle\nthese challenges, we propose MTLSO - a Multi-Task Learning approach for Logic\nSynthesis Optimization. On one hand, it maximizes the use of limited data by\ntraining the model across different tasks. This includes introducing an\nauxiliary task of binary multi-label graph classification alongside the primary\nregression task, allowing the model to benefit from diverse supervision\nsources. On the other hand, we employ a hierarchical graph representation\nlearning strategy to improve the model's capacity for learning expressive\ngraph-level representations of large AIGs, surpassing traditional plain GNNs.\nExtensive experiments across multiple datasets and against state-of-the-art\nbaselines demonstrate the superiority of our method, achieving an average\nperformance gain of 8.22\\% for delay and 5.95\\% for area.\n","authors":["Faezeh Faez","Raika Karimi","Yingxue Zhang","Xing Li","Lei Chen","Mingxuan Yuan","Mahdi Biparva"],"pdf_url":"https://arxiv.org/pdf/2409.06077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06072v1","updated":"2024-09-09T21:12:03Z","published":"2024-09-09T21:12:03Z","title":"DetoxBench: Benchmarking Large Language Models for Multitask Fraud &\n  Abuse Detection","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing tasks. However, their practical application in\nhigh-stake domains, such as fraud and abuse detection, remains an area that\nrequires further exploration. The existing applications often narrowly focus on\nspecific tasks like toxicity or hate speech detection. In this paper, we\npresent a comprehensive benchmark suite designed to assess the performance of\nLLMs in identifying and mitigating fraudulent and abusive language across\nvarious real-world scenarios. Our benchmark encompasses a diverse set of tasks,\nincluding detecting spam emails, hate speech, misogynistic language, and more.\nWe evaluated several state-of-the-art LLMs, including models from Anthropic,\nMistral AI, and the AI21 family, to provide a comprehensive assessment of their\ncapabilities in this critical domain. The results indicate that while LLMs\nexhibit proficient baseline performance in individual fraud and abuse detection\ntasks, their performance varies considerably across tasks, particularly\nstruggling with tasks that demand nuanced pragmatic reasoning, such as\nidentifying diverse forms of misogynistic language. These findings have\nimportant implications for the responsible development and deployment of LLMs\nin high-risk applications. Our benchmark suite can serve as a tool for\nresearchers and practitioners to systematically evaluate LLMs for multi-task\nfraud detection and drive the creation of more robust, trustworthy, and\nethically-aligned systems for fraud and abuse detection.\n","authors":["Joymallya Chakraborty","Wei Xia","Anirban Majumder","Dan Ma","Walid Chaabene","Naveed Janvekar"],"pdf_url":"https://arxiv.org/pdf/2409.06072v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2409.06069v1","updated":"2024-09-09T21:07:13Z","published":"2024-09-09T21:07:13Z","title":"Privacy-Preserving Data Linkage Across Private and Public Datasets for\n  Collaborative Agriculture Research","summary":"  Digital agriculture leverages technology to enhance crop yield, disease\nresilience, and soil health, playing a critical role in agricultural research.\nHowever, it raises privacy concerns such as adverse pricing, price\ndiscrimination, higher insurance costs, and manipulation of resources,\ndeterring farm operators from sharing data due to potential misuse. This study\nintroduces a privacy-preserving framework that addresses these risks while\nallowing secure data sharing for digital agriculture. Our framework enables\ncomprehensive data analysis while protecting privacy. It allows stakeholders to\nharness research-driven policies that link public and private datasets. The\nproposed algorithm achieves this by: (1) identifying similar farmers based on\nprivate datasets, (2) providing aggregate information like time and location,\n(3) determining trends in price and product availability, and (4) correlating\ntrends with public policy data, such as food insecurity statistics. We validate\nthe framework with real-world Farmer's Market datasets, demonstrating its\nefficacy through machine learning models trained on linked privacy-preserved\ndata. The results support policymakers and researchers in addressing food\ninsecurity and pricing issues. This work significantly contributes to digital\nagriculture by providing a secure method for integrating and analyzing data,\ndriving advancements in agricultural technology and development.\n","authors":["Osama Zafar","Rosemarie Santa Gonzalez","Gabriel Wilkins","Alfonso Morales","Erman Ayday"],"pdf_url":"https://arxiv.org/pdf/2409.06069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09056v3","updated":"2024-09-09T20:54:39Z","published":"2024-02-14T10:07:05Z","title":"Is Epistemic Uncertainty Faithfully Represented by Evidential Deep\n  Learning Methods?","summary":"  Trustworthy ML systems should not only return accurate predictions, but also\na reliable representation of their uncertainty. Bayesian methods are commonly\nused to quantify both aleatoric and epistemic uncertainty, but alternative\napproaches, such as evidential deep learning methods, have become popular in\nrecent years. The latter group of methods in essence extends empirical risk\nminimization (ERM) for predicting second-order probability distributions over\noutcomes, from which measures of epistemic (and aleatoric) uncertainty can be\nextracted. This paper presents novel theoretical insights of evidential deep\nlearning, highlighting the difficulties in optimizing second-order loss\nfunctions and interpreting the resulting epistemic uncertainty measures. With a\nsystematic setup that covers a wide range of approaches for classification,\nregression and counts, it provides novel insights into issues of\nidentifiability and convergence in second-order loss minimization, and the\nrelative (rather than absolute) nature of epistemic uncertainty measures.\n","authors":["Mira Jürgens","Nis Meinert","Viktor Bengs","Eyke Hüllermeier","Willem Waegeman"],"pdf_url":"https://arxiv.org/pdf/2402.09056v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14302v2","updated":"2024-09-09T20:43:26Z","published":"2024-06-20T13:30:25Z","title":"Identifiable Exchangeable Mechanisms for Causal Structure and\n  Representation Learning","summary":"  Identifying latent representations or causal structures is important for good\ngeneralization and downstream task performance. However, both fields have been\ndeveloped rather independently. We observe that several methods in both\nrepresentation and causal structure learning rely on the same data-generating\nprocess (DGP), namely, exchangeable but not i.i.d. (independent and identically\ndistributed) data. We provide a unified framework, termed Identifiable\nExchangeable Mechanisms (IEM), for representation and structure learning under\nthe lens of exchangeability. IEM provides new insights that let us relax the\nnecessary conditions for causal structure identification in exchangeable\nnon--i.i.d. data. We also demonstrate the existence of a duality condition in\nidentifiable representation learning, leading to new identifiability results.\nWe hope this work will pave the way for further research in causal\nrepresentation learning.\n","authors":["Patrik Reizinger","Siyuan Guo","Ferenc Huszár","Bernhard Schölkopf","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2406.14302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02191v3","updated":"2024-09-09T20:26:25Z","published":"2024-06-04T10:35:16Z","title":"On the Recoverability of Causal Relations from Temporally Aggregated\n  I.I.D. Data","summary":"  We consider the effect of temporal aggregation on instantaneous\n(non-temporal) causal discovery in general setting. This is motivated by the\nobservation that the true causal time lag is often considerably shorter than\nthe observational interval. This discrepancy leads to high aggregation, causing\ntime-delay causality to vanish and instantaneous dependence to manifest.\nAlthough we expect such instantaneous dependence has consistency with the true\ncausal relation in certain sense to make the discovery results meaningful, it\nremains unclear what type of consistency we need and when will such consistency\nbe satisfied. We proposed functional consistency and conditional independence\nconsistency in formal way correspond functional causal model-based methods and\nconditional independence-based methods respectively and provide the conditions\nunder which these consistencies will hold. We show theoretically and\nexperimentally that causal discovery results may be seriously distorted by\naggregation especially in complete nonlinear case and we also find causal\nrelationship still recoverable from aggregated data if we have partial\nlinearity or appropriate prior. Our findings suggest community should take a\ncautious and meticulous approach when interpreting causal discovery results\nfrom such data and show why and when aggregation will distort the performance\nof causal discovery methods.\n","authors":["Shunxing Fan","Mingming Gong","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.02191v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2409.06053v1","updated":"2024-09-09T20:24:19Z","published":"2024-09-09T20:24:19Z","title":"Statistical Mechanics of Min-Max Problems","summary":"  Min-max optimization problems, also known as saddle point problems, have\nattracted significant attention due to their applications in various fields,\nsuch as fair beamforming, generative adversarial networks (GANs), and\nadversarial learning. However, understanding the properties of these min-max\nproblems has remained a substantial challenge. This study introduces a\nstatistical mechanical formalism for analyzing the equilibrium values of\nmin-max problems in the high-dimensional limit, while appropriately addressing\nthe order of operations for min and max. As a first step, we apply this\nformalism to bilinear min-max games and simple GANs, deriving the relationship\nbetween the amount of training data and generalization error and indicating the\noptimal ratio of fake to real data for effective learning. This formalism\nprovides a groundwork for a deeper theoretical analysis of the equilibrium\nproperties in various machine learning methods based on min-max problems and\nencourages the development of new algorithms and architectures.\n","authors":["Yuma Ichikawa","Koji Hukushima"],"pdf_url":"https://arxiv.org/pdf/2409.06053v1.pdf","comment":"16 pages, 1 figures"},{"id":"http://arxiv.org/abs/2404.00297v3","updated":"2024-09-09T19:57:17Z","published":"2024-03-30T09:20:43Z","title":"RoBERTa and Attention-based BiLSTM for Interpretable Sentiment Analysis\n  of Tweets","summary":"  Sentiment analysis is crucial for understanding public opinion and consumer\nbehavior. Existing models face challenges with linguistic diversity,\ngeneralizability, and explainability. We propose TRABSA, a hybrid framework\nintegrating transformer-based architectures, attention mechanisms, and BiLSTM\nnetworks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge\ngaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.\nAugmenting datasets with tweets from 32 countries and US states, we compare six\nword-embedding techniques and three lexicon-based labeling techniques,\nselecting the best for optimal sentiment analysis. TRABSA outperforms\ntraditional ML and deep learning models with 94% accuracy and significant\nprecision, recall, and F1-score gains. Evaluation across diverse datasets\ndemonstrates consistent superiority and generalizability. SHAP and LIME\nanalyses enhance interpretability, improving confidence in predictions. Our\nstudy facilitates pandemic resource management, aiding resource planning,\npolicy formation, and vaccination tactics.\n","authors":["Md Abrar Jahin","Md Sakib Hossain Shovon","M. F. Mridha","Md Rashedul Islam","Yutaka Watanobe"],"pdf_url":"https://arxiv.org/pdf/2404.00297v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.06051v1","updated":"2024-09-09T20:22:00Z","published":"2024-09-09T20:22:00Z","title":"REVISION: A Roadmap on Adaptive Video Streaming Optimization","summary":"  Due to the soaring popularity of video applications and the consequent rise\nin video traffic on the Internet, technologies like HTTP Adaptive Streaming\n(HAS) are crucial for delivering high Quality of Experience (QoE) to consumers.\nHAS technology enables video players on consumer devices to enhance viewer\nengagement by dynamically adapting video content quality based on network\nconditions. This is especially relevant for consumer electronics as it ensures\nan optimized viewing experience across a variety of devices, from smartphones\nto smart TVs. This paper introduces REVISION, an efficient roadmap designed to\nenhance adaptive video streaming, a core feature of modern consumer\nelectronics. The REVISION optimization triangle highlights three essential\naspects for improving streaming: Objective, Input Space, and Action Domain.\nAdditionally, REVISION proposes a novel layer-based architecture tailored to\nrefine video streaming systems, comprising Application, Control and Management,\nand Resource layers. Each layer is designed to optimize different components of\nthe streaming process, which is directly linked to the performance and\nefficiency of consumer devices. By adopting the principles of the REVISION,\nmanufacturers and developers can significantly improve the streaming\ncapabilities of consumer electronics, thereby enriching the consumer's\nmultimedia experience and accommodating the increasing demand for high-quality,\nreal-time video content. This approach addresses the complexities of today's\ndiverse video streaming ecosystem and paves the way for future advancements in\nconsumer technology.\n","authors":["Farzad Tashtarian","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2409.06051v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.05772v1","updated":"2024-09-09T16:33:40Z","published":"2024-09-09T16:33:40Z","title":"A CLIP-based siamese approach for meme classification","summary":"  Memes are an increasingly prevalent element of online discourse in social\nnetworks, especially among young audiences. They carry ideas and messages that\nrange from humorous to hateful, and are widely consumed. Their potentially high\nimpact requires adequate means of control to moderate their use in large scale.\nIn this work, we propose SimCLIP a deep learning-based architecture for\ncross-modal understanding of memes, leveraging a pre-trained CLIP encoder to\nproduce context-aware embeddings and a Siamese fusion technique to capture the\ninteractions between text and image. We perform an extensive experimentation on\nseven meme classification tasks across six datasets. We establish a new state\nof the art in Memotion7k with a 7.25% relative F1-score improvement, and\nachieve super-human performance on Harm-P with 13.73% F1-Score improvement. Our\napproach demonstrates the potential for compact meme classification models,\nenabling accurate and efficient meme monitoring. We share our code at\nhttps://github.com/jahuerta92/meme-classification-simclip\n","authors":["Javier Huertas-Tato","Christos Koutlis","Symeon Papadopoulos","David Camacho","Ioannis Kompatsiaris"],"pdf_url":"https://arxiv.org/pdf/2409.05772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05750v1","updated":"2024-09-09T16:05:40Z","published":"2024-09-09T16:05:40Z","title":"A Toolkit for Joint Speaker Diarization and Identification with\n  Application to Speaker-Attributed ASR","summary":"  We present a modular toolkit to perform joint speaker diarization and speaker\nidentification. The toolkit can leverage on multiple models and algorithms\nwhich are defined in a configuration file. Such flexibility allows our system\nto work properly in various conditions (e.g., multiple registered speakers'\nsets, acoustic conditions and languages) and across application domains (e.g.\nmedia monitoring, institutional, speech analytics). In this demonstration we\nshow a practical use-case in which speaker-related information is used jointly\nwith automatic speech recognition engines to generate speaker-attributed\ntranscriptions. To achieve that, we employ a user-friendly web-based interface\nto process audio and video inputs with the chosen configuration.\n","authors":["Giovanni Morrone","Enrico Zovato","Fabio Brugnara","Enrico Sartori","Leonardo Badino"],"pdf_url":"https://arxiv.org/pdf/2409.05750v1.pdf","comment":"Show and Tell paper. Presented at Interspeech 2024"},{"id":"http://arxiv.org/abs/2409.04398v2","updated":"2024-09-09T15:08:06Z","published":"2024-09-06T16:43:04Z","title":"HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale\n  Space Using Wearable IMUs and LiDAR","summary":"  We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture\nmethod, aimed at accurately and efficiently creating a dynamic digital world,\ncontaining large-scale indoor-outdoor scenes, diverse human motions, rich\nhuman-human interactions, and human-environment interactions. By utilizing\nbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human\nmotions in unconstrained space without the need for external devices and\npre-built maps. This affords great flexibility and accessibility for\nhuman-centered interaction and 4D scene capturing in various environments.\nTaking into account that IMUs can capture human spatially unrestricted poses\nbut are prone to drifting for long-period using, and while LiDAR is stable for\nglobal localization but rough for local positions and orientations, HiSC4D\nemploys a joint optimization method, harmonizing all sensors and utilizing\nenvironment cues, yielding promising results for long-term capture in large\nscenes. To promote research of egocentric human interaction in large scenes and\nfacilitate downstream tasks, we also present a dataset, containing 8 sequences\nin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D\nhuman motions with SMPL annotations and dynamic scenes, 31k frames of cropped\nhuman point clouds, and scene mesh of the environment. A variety of scenarios,\nsuch as the basketball gym and commercial street, alongside challenging human\nmotions, such as daily greeting, one-on-one basketball playing, and tour\nguiding, demonstrate the effectiveness and the generalization ability of\nHiSC4D. The dataset and code will be publicated on\nwww.lidarhumanmotion.net/hisc4d available for research purposes.\n","authors":["Yudi Dai","Zhiyong Wang","Xiping Lin","Chenglu Wen","Lan Xu","Siqi Shen","Yuexin Ma","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04398v2.pdf","comment":"17 pages, 10 figures, Jornal"},{"id":"http://arxiv.org/abs/2309.11500v4","updated":"2024-09-09T14:52:15Z","published":"2023-09-20T17:59:32Z","title":"Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning","summary":"  Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.\n","authors":["Luoyi Sun","Xuenan Xu","Mengyue Wu","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2309.11500v4.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2409.05659v1","updated":"2024-09-09T14:29:22Z","published":"2024-09-09T14:29:22Z","title":"Audio-Visual Speaker Diarization: Current Databases, Approaches and\n  Challenges","summary":"  Nowadays, the large amount of audio-visual content available has fostered the\nneed to develop new robust automatic speaker diarization systems to analyse and\ncharacterise it. This kind of system helps to reduce the cost of doing this\nprocess manually and allows the use of the speaker information for different\napplications, as a huge quantity of information is present, for example, images\nof faces, or audio recordings. Therefore, this paper aims to address a critical\narea in the field of speaker diarization systems, the integration of\naudio-visual content of different domains. This paper seeks to push beyond\ncurrent state-of-the-art practices by developing a robust audio-visual speaker\ndiarization framework adaptable to various data domains, including TV\nscenarios, meetings, and daily activities. Unlike most of the existing\naudio-visual speaker diarization systems, this framework will also include the\nproposal of an approach to lead the precise assignment of specific identities\nin TV scenarios where celebrities appear. In addition, in this work, we have\nconducted an extensive compilation of the current state-of-the-art approaches\nand the existing databases for developing audio-visual speaker diarization.\n","authors":["Victoria Mingote","Alfonso Ortega","Antonio Miguel","Eduardo Lleida"],"pdf_url":"https://arxiv.org/pdf/2409.05659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05606v1","updated":"2024-09-09T13:39:47Z","published":"2024-09-09T13:39:47Z","title":"CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven\n  Text-to-Image Customization","summary":"  Subject-driven text-to-image (T2I) customization has drawn significant\ninterest in academia and industry. This task enables pre-trained models to\ngenerate novel images based on unique subjects. Existing studies adopt a\nself-reconstructive perspective, focusing on capturing all details of a single\nimage, which will misconstrue the specific image's irrelevant attributes (e.g.,\nview, pose, and background) as the subject intrinsic attributes. This\nmisconstruction leads to both overfitting or underfitting of irrelevant and\nintrinsic attributes of the subject, i.e., these attributes are\nover-represented or under-represented simultaneously, causing a trade-off\nbetween similarity and controllability. In this study, we argue an ideal\nsubject representation can be achieved by a cross-differential perspective,\ni.e., decoupling subject intrinsic attributes from irrelevant attributes via\ncontrastive learning, which allows the model to focus more on intrinsic\nattributes through intra-consistency (features of the same subject are\nspatially closer) and inter-distinctiveness (features of different subjects\nhave distinguished differences). Specifically, we propose CustomContrast, a\nnovel framework, which includes a Multilevel Contrastive Learning (MCL)\nparadigm and a Multimodal Feature Injection (MFI) Encoder. The MCL paradigm is\nused to extract intrinsic features of subjects from high-level semantics to\nlow-level appearance through crossmodal semantic contrastive learning and\nmultiscale appearance contrastive learning. To facilitate contrastive learning,\nwe introduce the MFI encoder to capture cross-modal representations. Extensive\nexperiments show the effectiveness of CustomContrast in subject similarity and\ntext controllability.\n","authors":["Nan Chen","Mengqi Huang","Zhuowei Chen","Yang Zheng","Lei Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2409.05606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03632v3","updated":"2024-09-09T12:26:04Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v3.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2409.05540v1","updated":"2024-09-09T12:00:17Z","published":"2024-09-09T12:00:17Z","title":"Exploring Rich Subjective Quality Information for Image Quality\n  Assessment in the Wild","summary":"  Traditional in the wild image quality assessment (IQA) models are generally\ntrained with the quality labels of mean opinion score (MOS), while missing the\nrich subjective quality information contained in the quality ratings, for\nexample, the standard deviation of opinion scores (SOS) or even distribution of\nopinion scores (DOS). In this paper, we propose a novel IQA method named\nRichIQA to explore the rich subjective rating information beyond MOS to predict\nimage quality in the wild. RichIQA is characterized by two key novel designs:\n(1) a three-stage image quality prediction network which exploits the powerful\nfeature representation capability of the Convolutional vision Transformer (CvT)\nand mimics the short-term and long-term memory mechanisms of human brain; (2) a\nmulti-label training strategy in which rich subjective quality information like\nMOS, SOS and DOS are concurrently used to train the quality prediction network.\nPowered by these two novel designs, RichIQA is able to predict the image\nquality in terms of a distribution, from which the mean image quality can be\nsubsequently obtained. Extensive experimental results verify that the\nthree-stage network is tailored to predict rich quality information, while the\nmulti-label training strategy can fully exploit the potentials within\nsubjective quality rating and enhance the prediction performance and\ngeneralizability of the network. RichIQA outperforms state-of-the-art\ncompetitors on multiple large-scale in the wild IQA databases with rich\nsubjective rating labels. The code of RichIQA will be made publicly available\non GitHub.\n","authors":["Xiongkuo Min","Yixuan Gao","Yuqin Cao","Guangtao Zhai","Wenjun Zhang","Huifang Sun","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05496v1","updated":"2024-09-09T10:48:33Z","published":"2024-09-09T10:48:33Z","title":"Educational Virtual Field Trips based on Social VR and 360° Spaces","summary":"  Virtual field trips (VFTs) have proven to be valuable learning tools. Such\napplications are mostly based on 360{\\deg} technology and are to be\ncharacterized as single-user applications in technological terms. In contrast,\nSocial VR applications are characterized by multi-user capability and\nuser-specific avatars. From a learning perspective, the concepts of\ncollaborative learning and embodiment have long been proposed as conducive to\nlearning. Both concepts might be supported using Social VR. However, little is\ncurrently known about the use of Social VR for VFTs. Accordingly, the research\nquestions are to what extent VFTs can be implemented in Social VR environments\nand how these Social VR-based VFTs are perceived by learners. This article\npresents an evaluation study on the development and evaluation of a VFT\nenvironment using the Social VR platform Mozilla Hubs. It describes the design\ndecisions to create the environment and evaluation results from a mixed-method\nstudy (N=16) using a questionnaire and focus group discussions. The study\nhighlighted the opportunities offered by Social VR-based VFTs but also revealed\nseveral challenges that need to be addressed to embrace the potential of Social\nVR-based VFTs to be utilized regularly in education.\n","authors":["Surya Kalvakolu","Heinrich Söbke","Jannicke Baalsrud Hauge","Eckhard Kraft"],"pdf_url":"https://arxiv.org/pdf/2409.05496v1.pdf","comment":"9 pages, 7 figures, 1 table, submitted to Games and Learning Alliance\n  Conference"},{"id":"http://arxiv.org/abs/2409.05405v1","updated":"2024-09-09T08:06:50Z","published":"2024-09-09T08:06:50Z","title":"A Survey of Multimodal Composite Editing and Retrieval","summary":"  In the real world, where information is abundant and diverse across different\nmodalities, understanding and utilizing various data types to improve retrieval\nsystems is a key focus of research. Multimodal composite retrieval integrates\ndiverse modalities such as text, image and audio, etc. to provide more\naccurate, personalized, and contextually relevant results. To facilitate a\ndeeper understanding of this promising direction, this survey explores\nmultimodal composite editing and retrieval in depth, covering image-text\ncomposite editing, image-text composite retrieval, and other multimodal\ncomposite retrieval. In this survey, we systematically organize the application\nscenarios, methods, benchmarks, experiments, and future directions. Multimodal\nlearning is a hot topic in large model era, and have also witnessed some\nsurveys in multimodal learning and vision-language models with transformers\npublished in the PAMI journal. To the best of our knowledge, this survey is the\nfirst comprehensive review of the literature on multimodal composite retrieval,\nwhich is a timely complement of multimodal fusion to existing reviews. To help\nreaders' quickly track this field, we build the project page for this survey,\nwhich can be found at\nhttps://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.\n","authors":["Suyan Li","Fuxiang Huang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05405v1.pdf","comment":"22 pages, 3 figures, and 11 tables"},{"id":"http://arxiv.org/abs/2409.05384v1","updated":"2024-09-09T07:32:18Z","published":"2024-09-09T07:32:18Z","title":"Look One and More: Distilling Hybrid Order Relational Knowledge for\n  Cross-Resolution Image Recognition","summary":"  In spite of great success in many image recognition tasks achieved by recent\ndeep models, directly applying them to recognize low-resolution images may\nsuffer from low accuracy due to the missing of informative details during\nresolution degradation. However, these images are still recognizable for\nsubjects who are familiar with the corresponding high-resolution ones. Inspired\nby that, we propose a teacher-student learning approach to facilitate\nlow-resolution image recognition via hybrid order relational knowledge\ndistillation. The approach refers to three streams: the teacher stream is\npretrained to recognize high-resolution images in high accuracy, the student\nstream is learned to identify low-resolution images by mimicking the teacher's\nbehaviors, and the extra assistant stream is introduced as bridge to help\nknowledge transfer across the teacher to the student. To extract sufficient\nknowledge for reducing the loss in accuracy, the learning of student is\nsupervised with multiple losses, which preserves the similarities in various\norder relational structures. In this way, the capability of recovering missing\ndetails of familiar low-resolution images can be effectively enhanced, leading\nto a better knowledge transfer. Extensive experiments on metric learning,\nlow-resolution image classification and low-resolution face recognition tasks\nshow the effectiveness of our approach, while taking reduced models.\n","authors":["Shiming Ge","Kangkai Zhang","Haolin Liu","Yingying Hua","Shengwei Zhao","Xin Jin","Hao Wen"],"pdf_url":"https://arxiv.org/pdf/2409.05384v1.pdf","comment":"Accepted by AAAI 2020"},{"id":"http://arxiv.org/abs/2409.05330v1","updated":"2024-09-09T05:20:02Z","published":"2024-09-09T05:20:02Z","title":"KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks\n  Generation","summary":"  Audio-driven talking face generation is a widely researched topic due to its\nhigh applicability. Reconstructing a talking face using audio significantly\ncontributes to fields such as education, healthcare, online conversations,\nvirtual assistants, and virtual reality. Early studies often focused solely on\nchanging the mouth movements, which resulted in outcomes with limited practical\napplications. Recently, researchers have proposed a new approach of\nconstructing the entire face, including face pose, neck, and shoulders. To\nachieve this, they need to generate through landmarks. However, creating stable\nlandmarks that align well with the audio is a challenge. In this paper, we\npropose the KFusion of Dual-Domain model, a robust model that generates\nlandmarks from audio. We separate the audio into two distinct domains to learn\nemotional information and facial context, then use a fusion mechanism based on\nthe KAN model. Our model demonstrates high efficiency compared to recent\nmodels. This will lay the groundwork for the development of the audio-driven\ntalking face generation problem in the future.\n","authors":["Hoang-Son Vo-Thanh","Quang-Vinh Nguyen","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2409.05330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05297v1","updated":"2024-09-09T03:10:40Z","published":"2024-09-09T03:10:40Z","title":"Adaptive Offloading and Enhancement for Low-Light Video Analytics on\n  Mobile Devices","summary":"  In this paper, we explore adaptive offloading and enhancement strategies for\nvideo analytics tasks on computing-constrained mobile devices in low-light\nconditions. We observe that the accuracy of low-light video analytics varies\nfrom different enhancement algorithms. The root cause could be the disparities\nin the effectiveness of enhancement algorithms for feature extraction in\nanalytic models. Specifically, the difference in class activation maps (CAMs)\nbetween enhanced and low-light frames demonstrates a positive correlation with\nvideo analytics accuracy. Motivated by such observations, a novel enhancement\nquality assessment method is proposed on CAMs to evaluate the effectiveness of\ndifferent enhancement algorithms for low-light videos. Then, we design a\nmulti-edge system, which adaptively offloads and enhances low-light video\nanalytics tasks from mobile devices. To achieve the trade-off between the\nenhancement quality and the latency for all system-served mobile devices, we\npropose a genetic-based scheduling algorithm, which can find a near-optimal\nsolution in a reasonable time to meet the latency requirement. Thereby, the\noffloading strategies and the enhancement algorithms are properly selected\nunder the condition of limited end-edge bandwidth and edge computation\nresources. Simulation experiments demonstrate the superiority of the proposed\nsystem, improving accuracy up to 20.83\\% compared to existing benchmarks.\n","authors":["Yuanyi He","Peng Yang","Tian Qin","Jiawei Hou","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14066v3","updated":"2024-09-09T03:06:21Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  Head-mounted 360{\\deg} displays and portable 360{\\deg} cameras have\nsignificantly progressed, providing viewers a realistic and immersive\nexperience. However, many omnidirectional videos have low frame rates that can\nlead to visual fatigue, and the prevailing plane frame interpolation\nmethodologies are unsuitable for omnidirectional video interpolation because\nthey are designed solely for traditional videos. This paper introduces the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. Specifically,\nwe propose a pyramid distortion-sensitive feature extractor that uses the\nunique characteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto further facilitate the synthesis of intermediate frames. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we present four different\ndistortion condition scenes in the proposed 360VFI dataset to evaluate the\nchallenges triggered by distortion during interpolation. Besides, experimental\nresults demonstrate that Omnidirectional Video Interpolation can be effectively\nimproved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v3.pdf","comment":"This is a preprint version"}]},"2024-09-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.05247v1","updated":"2024-09-08T23:51:04Z","published":"2024-09-08T23:51:04Z","title":"Socially Responsible Data for Large Multilingual Language Models","summary":"  Large Language Models (LLMs) have rapidly increased in size and apparent\ncapabilities in the last three years, but their training data is largely\nEnglish text. There is growing interest in multilingual LLMs, and various\nefforts are striving for models to accommodate languages of communities outside\nof the Global North, which include many languages that have been historically\nunderrepresented in digital realms. These languages have been coined as \"low\nresource languages\" or \"long-tail languages\", and LLMs performance on these\nlanguages is generally poor. While expanding the use of LLMs to more languages\nmay bring many potential benefits, such as assisting cross-community\ncommunication and language preservation, great care must be taken to ensure\nthat data collection on these languages is not extractive and that it does not\nreproduce exploitative practices of the past. Collecting data from languages\nspoken by previously colonized people, indigenous people, and non-Western\nlanguages raises many complex sociopolitical and ethical questions, e.g.,\naround consent, cultural safety, and data sovereignty. Furthermore, linguistic\ncomplexity and cultural nuances are often lost in LLMs. This position paper\nbuilds on recent scholarship, and our own work, and outlines several relevant\nsocial, cultural, and ethical considerations and potential ways to mitigate\nthem through qualitative research, community partnerships, and participatory\ndesign approaches. We provide twelve recommendations for consideration when\ncollecting language data on underrepresented language communities outside of\nthe Global North.\n","authors":["Andrew Smart","Ben Hutchinson","Lameck Mbangula Amugongo","Suzanne Dikker","Alex Zito","Amber Ebinama","Zara Wudiri","Ding Wang","Erin van Liemt","João Sedoc","Seyi Olojo","Stanley Uwakwe","Edem Wornyo","Sonja Schmer-Galunder","Jamila Smith-Loud"],"pdf_url":"https://arxiv.org/pdf/2409.05247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13440v2","updated":"2024-09-08T21:47:29Z","published":"2024-08-24T02:40:28Z","title":"Knowledge-Aware Conversation Derailment Forecasting Using Graph\n  Convolutional Networks","summary":"  Online conversations are particularly susceptible to derailment, which can\nmanifest itself in the form of toxic communication patterns including\ndisrespectful comments and abuse. Forecasting conversation derailment predicts\nsigns of derailment in advance enabling proactive moderation of conversations.\nState-of-the-art approaches to conversation derailment forecasting sequentially\nencode conversations and use graph neural networks to model dialogue user\ndynamics. However, existing graph models are not able to capture complex\nconversational characteristics such as context propagation and emotional\nshifts. The use of common sense knowledge enables a model to capture such\ncharacteristics, thus improving performance. Following this approach, here we\nderive commonsense statements from a knowledge base of dialogue contextual\ninformation to enrich a graph neural network classification architecture. We\nfuse the multi-source information on utterance into capsules, which are used by\na transformer-based forecaster to predict conversation derailment. Our model\ncaptures conversation dynamics and context propagation, outperforming the\nstate-of-the-art models on the CGA and CMV benchmark datasets\n","authors":["Enas Altarawneh","Ameeta Agrawal","Michael Jenkin","Manos Papagelis"],"pdf_url":"https://arxiv.org/pdf/2408.13440v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2306.12982;\n  text overlap with arXiv:2106.01071 by other authors"},{"id":"http://arxiv.org/abs/2409.05224v1","updated":"2024-09-08T21:40:44Z","published":"2024-09-08T21:40:44Z","title":"Exploring Intrinsic Language-specific Subspaces in Fine-tuning\n  Multilingual Neural Machine Translation","summary":"  Multilingual neural machine translation models support fine-tuning hundreds\nof languages simultaneously. However, fine-tuning on full parameters solely is\ninefficient potentially leading to negative interactions among languages. In\nthis work, we demonstrate that the fine-tuning for a language occurs in its\nintrinsic language-specific subspace with a tiny fraction of entire parameters.\nThus, we propose language-specific LoRA to isolate intrinsic language-specific\nsubspaces. Furthermore, we propose architecture learning techniques and\nintroduce a gradual pruning schedule during fine-tuning to exhaustively explore\nthe optimal setting and the minimal intrinsic subspaces for each language,\nresulting in a lightweight yet effective fine-tuning procedure. The\nexperimental results on a 12-language subset and a 30-language subset of\nFLORES-101 show that our methods not only outperform full-parameter fine-tuning\nup to 2.25 spBLEU scores but also reduce trainable parameters to $0.4\\%$ for\nhigh and medium-resource languages and $1.6\\%$ for low-resource ones.\n","authors":["Zhe Cao","Zhi Qu","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2409.05224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06682v2","updated":"2024-09-08T20:33:03Z","published":"2024-05-05T18:56:46Z","title":"Self-Reflection in LLM Agents: Effects on Problem-Solving Performance","summary":"  In this study, we investigated the effects of self-reflection in large\nlanguage models (LLMs) on problem-solving performance. We instructed nine\npopular LLMs to answer a series of multiple-choice questions to provide a\nperformance baseline. For each incorrectly answered question, we instructed\neight types of self-reflecting LLM agents to reflect on their mistakes and\nprovide themselves with guidance to improve problem-solving. Then, using this\nguidance, each self-reflecting agent attempted to re-answer the same questions.\nOur results indicate that LLM agents are able to significantly improve their\nproblem-solving performance through self-reflection ($p < 0.001$). In addition,\nwe compared the various types of self-reflection to determine their individual\ncontribution to performance. All code and data are available on GitHub at\nhttps://github.com/matthewrenze/self-reflection\n","authors":["Matthew Renze","Erhan Guven"],"pdf_url":"https://arxiv.org/pdf/2405.06682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00217v2","updated":"2024-09-08T20:08:16Z","published":"2024-08-30T19:14:17Z","title":"ProGRes: Prompted Generative Rescoring on ASR n-Best","summary":"  Large Language Models (LLMs) have shown their ability to improve the\nperformance of speech recognizers by effectively rescoring the n-best\nhypotheses generated during the beam search process. However, the best way to\nexploit recent generative instruction-tuned LLMs for hypothesis rescoring is\nstill unclear. This paper proposes a novel method that uses instruction-tuned\nLLMs to dynamically expand the n-best speech recognition hypotheses with new\nhypotheses generated through appropriately-prompted LLMs. Specifically, we\nintroduce a new zero-shot method for ASR n-best rescoring, which combines\nconfidence scores, LLM sequence scoring, and prompt-based hypothesis\ngeneration. We compare Llama-3-Instruct, GPT-3.5 Turbo, and GPT-4 Turbo as\nprompt-based generators with Llama-3 as sequence scorer LLM. We evaluated our\napproach using different speech recognizers and observed significant relative\nimprovement in the word error rate (WER) ranging from 5% to 25%.\n","authors":["Ada Defne Tur","Adel Moumen","Mirco Ravanelli"],"pdf_url":"https://arxiv.org/pdf/2409.00217v2.pdf","comment":"IEEE Spoken Language Technology Workshop"},{"id":"http://arxiv.org/abs/2408.01527v2","updated":"2024-09-08T19:59:06Z","published":"2024-08-02T18:40:10Z","title":"Using LLMs to Establish Implicit User Sentiment of Software Desirability","summary":"  This study explores the use of LLMs for providing quantitative zero-shot\nsentiment analysis of implicit software desirability, addressing a critical\nchallenge in product evaluation where traditional review scores, though\nconvenient, fail to capture the richness of qualitative user feedback.\nInnovations include establishing a method that 1) works with qualitative user\nexperience data without the need for explicit review scores, 2) focuses on\nimplicit user satisfaction, and 3) provides scaled numerical sentiment\nanalysis, offering a more nuanced understanding of user sentiment, instead of\nsimply classifying sentiment as positive, neutral, or negative.\n  Data is collected using the Microsoft Product Desirability Toolkit (PDT), a\nwell-known qualitative user experience analysis tool. For initial exploration,\nthe PDT metric was given to users of two software systems. PDT data was fed\nthrough several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a\nleading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader,\na leading sentiment analysis tool. Each system was asked to evaluate the data\nin two ways, by looking at the sentiment expressed in the PDT word/explanation\npairs; and by looking at the sentiment expressed by the users in their grouped\nselection of five words and explanations, as a whole. Each LLM provided a\nsentiment score, its confidence (low, medium, high) in the score, and an\nexplanation of the score.\n  All LLMs tested were able to statistically detect user sentiment from the\nusers' grouped data, whereas TRBS and Vader were not. The confidence and\nexplanation of confidence provided by the LLMs assisted in understanding user\nsentiment. This study adds deeper understanding of evaluating user experiences,\ntoward the goal of creating a universal tool that quantifies implicit\nsentiment.\n","authors":["Sherri Weitl-Harms","John D. Hastings","Jonah Lum"],"pdf_url":"https://arxiv.org/pdf/2408.01527v2.pdf","comment":"6 pages, 2 figures, 2 tables, updated to incorporate feedback"},{"id":"http://arxiv.org/abs/2409.05199v1","updated":"2024-09-08T19:24:14Z","published":"2024-09-08T19:24:14Z","title":"Interactive Machine Teaching by Labeling Rules and Instances","summary":"  Weakly supervised learning aims to reduce the cost of labeling data by using\nexpert-designed labeling rules. However, existing methods require experts to\ndesign effective rules in a single shot, which is difficult in the absence of\nproper guidance and tooling. Therefore, it is still an open question whether\nexperts should spend their limited time writing rules or instead providing\ninstance labels via active learning. In this paper, we investigate how to\nexploit an expert's limited time to create effective supervision. First, to\ndevelop practical guidelines for rule creation, we conduct an exploratory\nanalysis of diverse collections of existing expert-designed rules and find that\nrule precision is more important than coverage across datasets. Second, we\ncompare rule creation to individual instance labeling via active learning and\ndemonstrate the importance of both across 6 datasets. Third, we propose an\ninteractive learning framework, INTERVAL, that achieves efficiency by\nautomatically extracting candidate rules based on rich patterns (e.g., by\nprompting a language model), and effectiveness by soliciting expert feedback on\nboth candidate rules and individual instances. Across 6 datasets, INTERVAL\noutperforms state-of-the-art weakly supervised approaches by 7% in F1.\nFurthermore, it requires as few as 10 queries for expert feedback to reach F1\nvalues that existing active learning methods cannot match even with 100\nqueries.\n","authors":["Giannis Karamanolakis","Daniel Hsu","Luis Gravano"],"pdf_url":"https://arxiv.org/pdf/2409.05199v1.pdf","comment":"Accepted to TACL 2024"},{"id":"http://arxiv.org/abs/2409.05197v1","updated":"2024-09-08T19:22:58Z","published":"2024-09-08T19:22:58Z","title":"Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?","summary":"  State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.\n","authors":["Neeladri Bhuiya","Viktor Schlegel","Stefan Winkler"],"pdf_url":"https://arxiv.org/pdf/2409.05197v1.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2402.17124v2","updated":"2024-09-08T19:17:32Z","published":"2024-02-27T01:37:23Z","title":"Fact-and-Reflection (FaR) Improves Confidence Calibration of Large\n  Language Models","summary":"  For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances.\n","authors":["Xinran Zhao","Hongming Zhang","Xiaoman Pan","Wenlin Yao","Dong Yu","Tongshuang Wu","Jianshu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.17124v2.pdf","comment":"17 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.15695v2","updated":"2024-09-08T19:06:37Z","published":"2024-06-22T00:14:48Z","title":"SS-GEN: A Social Story Generation Framework with Large Language Models","summary":"  Children with Autism Spectrum Disorder (ASD) often misunderstand social\nsituations and struggle to participate in daily routines. Social Stories are\ntraditionally crafted by psychology experts under strict constraints to address\nthese challenges but are costly and limited in diversity. As Large Language\nModels (LLMs) advance, there's an opportunity to develop more automated,\naffordable, and accessible methods to generate Social Stories in real-time with\nbroad coverage. However, adapting LLMs to meet the unique and strict\nconstraints of Social Stories is a challenging issue. To this end, we propose\n\\textbf{SS-GEN}, a \\textbf{S}ocial \\textbf{S}tory \\textbf{GEN}eration framework\nwith LLMs. Firstly, we develop a constraint-driven sophisticated strategy named\n\\textbf{\\textsc{StarSow}} to hierarchically prompt LLMs to generate Social\nStories at scale, followed by rigorous human filtering to build a high-quality\ndataset. Additionally, we introduce \\textbf{quality assessment criteria} to\nevaluate the effectiveness of these generated stories. Considering that\npowerful closed-source large models require very complex instructions and\nexpensive API fees, we finally fine-tune smaller language models with our\ncurated high-quality dataset, achieving comparable results at lower costs and\nwith simpler instruction and deployment. This work marks a significant step in\nleveraging AI to personalize Social Stories cost-effectively for autistic\nchildren at scale, which we hope can encourage future research. The prompt,\ncode and data will release in the \\texttt{Technical Appendix} and \\texttt{Code\n\\& Data Appendix} at \\url{https://github.com/MIMIFY/SS-GEN}.\n","authors":["Yi Feng","Mingyang Song","Jiaqi Wang","Zhuang Chen","Guanqun Bi","Minlie Huang","Liping Jing","Jian Yu"],"pdf_url":"https://arxiv.org/pdf/2406.15695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16570v2","updated":"2024-09-08T18:08:00Z","published":"2024-08-29T14:37:05Z","title":"Predictability maximization and the origins of word order harmony","summary":"  We address the linguistic problem of the sequential arrangement of a head and\nits dependents from an information theoretic perspective. In particular, we\nconsider the optimal placement of a head that maximizes the predictability of\nthe sequence. We assume that dependents are statistically independent given a\nhead, in line with the open-choice principle and the core assumptions of\ndependency grammar. We demonstrate the optimality of harmonic order, i.e.,\nplacing the head last maximizes the predictability of the head whereas placing\nthe head first maximizes the predictability of dependents. We also show that\npostponing the head is the optimal strategy to maximize its predictability\nwhile bringing it forward is the optimal strategy to maximize the\npredictability of dependents. We unravel the advantages of the strategy of\nmaximizing the predictability of the head over maximizing the predictability of\ndependents. Our findings shed light on the placements of the head adopted by\nreal languages or emerging in different kinds of experiments.\n","authors":["Ramon Ferrer-i-Cancho"],"pdf_url":"https://arxiv.org/pdf/2408.16570v2.pdf","comment":"Local reorganization of the text; many typos corrected"},{"id":"http://arxiv.org/abs/2404.13043v2","updated":"2024-09-08T17:46:36Z","published":"2024-04-19T17:57:29Z","title":"Data Alignment for Zero-Shot Concept Generation in Dermatology AI","summary":"  AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.\n","authors":["Soham Gadgil","Mahtab Bigverdi"],"pdf_url":"https://arxiv.org/pdf/2404.13043v2.pdf","comment":"Accepted as a workshop paper to ICLR 2024"},{"id":"http://arxiv.org/abs/2405.14105v3","updated":"2024-09-08T17:15:17Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference of Large Language Models is Provably\n  Faster","summary":"  Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces Distributed\nSpeculative Inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI)\n[leviathan2023fast,chen2023accelerating,miao2023specinfer] and traditional\nautoregressive inference (non-SI). Like other SI algorithms, DSI works on\nfrozen LLMs, requiring no training or architectural modifications, and it\npreserves the target distribution. Prior studies on SI have demonstrated\nempirical speedups (compared to non-SI) but require fast and accurate drafters,\nwhich are often unavailable in practice. We identify a gap where SI can be\nslower than non-SI given slower or less accurate drafters. We close this gap by\nproving that DSI is faster than both SI and non-SI--given any drafters. DSI\nintroduces a novel type of task parallelism called Speculation Parallelism\n(SP), which orchestrates target and drafter instances to overlap in time,\ncreating a new foundational tradeoff between computational resources and\nlatency. DSI is not only faster than SI but also supports LLMs that cannot be\naccelerated with SI. Our simulations show speedups of off-the-shelf LLMs in\nrealistic single-node settings where DSI is 1.29-1.92x faster than SI.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21384v2","updated":"2024-09-08T16:42:28Z","published":"2024-07-31T07:15:33Z","title":"GEGA: Graph Convolutional Networks and Evidence Retrieval Guided\n  Attention for Enhanced Document-level Relation Extraction","summary":"  Document-level relation extraction (DocRE) aims to extract relations between\nentities from unstructured document text. Compared to sentence-level relation\nextraction, it requires more complex semantic understanding from a broader text\ncontext. Currently, some studies are utilizing logical rules within evidence\nsentences to enhance the performance of DocRE. However, in the data without\nprovided evidence sentences, researchers often obtain a list of evidence\nsentences for the entire document through evidence retrieval (ER). Therefore,\nDocRE suffers from two challenges: firstly, the relevance between evidence and\nentity pairs is weak; secondly, there is insufficient extraction of complex\ncross-relations between long-distance multi-entities. To overcome these\nchallenges, we propose GEGA, a novel model for DocRE. The model leverages graph\nneural networks to construct multiple weight matrices, guiding attention\nallocation to evidence sentences. It also employs multi-scale representation\naggregation to enhance ER. Subsequently, we integrate the most efficient\nevidence information to implement both fully supervised and weakly supervised\ntraining processes for the model. We evaluate the GEGA model on three widely\nused benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The\nexperimental results indicate that our model has achieved comprehensive\nimprovements compared to the existing SOTA model.\n","authors":["Yanxu Mao","Xiaohui Chen","Peipei Liu","Tiehan Cui","Zuhui Yue","Zheng Li"],"pdf_url":"https://arxiv.org/pdf/2407.21384v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05152v1","updated":"2024-09-08T16:35:19Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.05148v1","updated":"2024-09-08T16:25:38Z","published":"2024-09-08T16:25:38Z","title":"Better Spanish Emotion Recognition In-the-wild: Bringing Attention to\n  Deep Spectrum Voice Analysis","summary":"  Within the context of creating new Socially Assistive Robots, emotion\nrecognition has become a key development factor, as it allows the robot to\nadapt to the user's emotional state in the wild. In this work, we focused on\nthe analysis of two voice recording Spanish datasets: ELRA-S0329 and\nEmoMatchSpanishDB. Specifically, we centered our work in the paralanguage,\ne.~g. the vocal characteristics that go along with the message and clarifies\nthe meaning. We proposed the use of the DeepSpectrum method, which consists of\nextracting a visual representation of the audio tracks and feeding them to a\npretrained CNN model. For the classification task, DeepSpectrum is often paired\nwith a Support Vector Classifier --DS-SVC--, or a Fully-Connected deep-learning\nclassifier --DS-FC--. We compared the results of the DS-SVC and DS-FC\narchitectures with the state-of-the-art (SOTA) for ELRA-S0329 and\nEmoMatchSpanishDB. Moreover, we proposed our own classifier based upon\nAttention Mechanisms, namely DS-AM. We trained all models against both\ndatasets, and we found that our DS-AM model outperforms the SOTA models for the\ndatasets and the SOTA DeepSpectrum architectures. Finally, we trained our DS-AM\nmodel in one dataset and tested it in the other, to simulate real-world\nconditions on how biased is the model to the dataset.\n","authors":["Elena Ortega-Beltrán","Josep Cabacas-Maso","Ismael Benito-Altamirano","Carles Ventura"],"pdf_url":"https://arxiv.org/pdf/2409.05148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05965v3","updated":"2024-09-08T16:19:53Z","published":"2024-07-08T14:04:58Z","title":"T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models","summary":"  The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.\n","authors":["Yibo Miao","Yifan Zhu","Yinpeng Dong","Lijia Yu","Jun Zhu","Xiao-Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2407.05965v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05137v1","updated":"2024-09-08T15:42:48Z","published":"2024-09-08T15:42:48Z","title":"READoc: A Unified Benchmark for Realistic Document Structured Extraction","summary":"  Document Structured Extraction (DSE) aims to extract structured content from\nraw documents. Despite the emergence of numerous DSE systems, their unified\nevaluation remains inadequate, significantly hindering the field's advancement.\nThis problem is largely attributed to existing benchmark paradigms, which\nexhibit fragmented and localized characteristics. To address these limitations\nand offer a thorough evaluation of DSE systems, we introduce a novel benchmark\nnamed READoc, which defines DSE as a realistic task of converting unstructured\nPDFs into semantically rich Markdown. The READoc dataset is derived from 2,233\ndiverse and real-world documents from arXiv and GitHub. In addition, we develop\na DSE Evaluation S$^3$uite comprising Standardization, Segmentation and Scoring\nmodules, to conduct a unified evaluation of state-of-the-art DSE approaches. By\nevaluating a range of pipeline tools, expert visual models, and general VLMs,\nwe identify the gap between current work and the unified, realistic DSE\nobjective for the first time. We aspire that READoc will catalyze future\nresearch in DSE, fostering more comprehensive and practical solutions.\n","authors":["Zichao Li","Aizier Abulaiti","Yaojie Lu","Xuanang Chen","Jia Zheng","Hongyu Lin","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2409.05137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05136v1","updated":"2024-09-08T15:42:18Z","published":"2024-09-08T15:42:18Z","title":"MHS-STMA: Multimodal Hate Speech Detection via Scalable\n  Transformer-Based Multilevel Attention Framework","summary":"  Social media has a significant impact on people's lives. Hate speech on\nsocial media has emerged as one of society's most serious issues recently. Text\nand pictures are two forms of multimodal data distributed within articles.\nUnimodal analysis has been the primary emphasis of earlier approaches.\nAdditionally, when doing multimodal analysis, researchers neglect to preserve\nthe distinctive qualities associated with each modality. The present article\nsuggests a scalable architecture for multimodal hate content detection called\ntransformer-based multilevel attention (STMA) to address these shortcomings.\nThis architecture consists of three main parts: a combined attention-based deep\nlearning mechanism, a vision attention mechanism encoder, and a caption\nattention-mechanism encoder. To identify hate content, each component uses\nvarious attention processes and uniquely handles multimodal data. Several\nstudies employing multiple assessment criteria on three hate speech datasets:\nHateful memes, MultiOff, and MMHS150K, validate the suggested architecture's\nefficacy. The outcomes demonstrate that on all three datasets, the suggested\nstrategy performs better than the baseline approaches.\n","authors":["Anusha Chhabra","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2409.05136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05134v1","updated":"2024-09-08T15:32:17Z","published":"2024-09-08T15:32:17Z","title":"Hate Content Detection via Novel Pre-Processing Sequencing and Ensemble\n  Methods","summary":"  Social media, particularly Twitter, has seen a significant increase in\nincidents like trolling and hate speech. Thus, identifying hate speech is the\nneed of the hour. This paper introduces a computational framework to curb the\nhate content on the web. Specifically, this study presents an exhaustive study\nof pre-processing approaches by studying the impact of changing the sequence of\ntext pre-processing operations for the identification of hate content. The\nbest-performing pre-processing sequence, when implemented with popular\nclassification approaches like Support Vector Machine, Random Forest, Decision\nTree, Logistic Regression and K-Neighbor provides a considerable boost in\nperformance. Additionally, the best pre-processing sequence is used in\nconjunction with different ensemble methods, such as bagging, boosting and\nstacking to improve the performance further. Three publicly available benchmark\ndatasets (WZ-LS, DT, and FOUNTA), were used to evaluate the proposed approach\nfor hate speech identification. The proposed approach achieves a maximum\naccuracy of 95.14% highlighting the effectiveness of the unique pre-processing\napproach along with an ensemble classifier.\n","authors":["Anusha Chhabra","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2409.05134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15737v2","updated":"2024-09-08T15:03:14Z","published":"2024-04-24T08:52:40Z","title":"No Train but Gain: Language Arithmetic for training-free Language\n  Adapters enhancement","summary":"  Modular deep learning is the state-of-the-art solution for lifting the curse\nof multilinguality, preventing the impact of negative interference and enabling\ncross-lingual performance in Multilingual Pre-trained Language Models. However,\na trade-off of this approach is the reduction in positive transfer learning\nfrom closely related languages. In response, we introduce a novel method called\nlanguage arithmetic, which enables training-free post-processing to address\nthis limitation. Extending the task arithmetic framework, we apply learning via\naddition to the language adapters, transitioning the framework from a\nmulti-task to a multilingual setup. The effectiveness of the proposed solution\nis demonstrated on three downstream tasks in a MAD-X-based set of cross-lingual\nschemes, acting as a post-processing procedure. Language arithmetic\nconsistently improves the baselines with significant gains, especially in the\nmost challenging case of zero-shot application. Our code and models are\navailable at https://github.com/mklimasz/language-arithmetic .\n","authors":["Mateusz Klimaszewski","Piotr Andruszkiewicz","Alexandra Birch"],"pdf_url":"https://arxiv.org/pdf/2404.15737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03627v5","updated":"2024-09-08T15:01:56Z","published":"2024-07-04T04:30:04Z","title":"DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.\n","authors":["Taeho Hwang","Soyeong Jeong","Sukmin Cho","SeungYoon Han","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2407.03627v5.pdf","comment":"20 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2405.19612v2","updated":"2024-09-08T20:32:47Z","published":"2024-05-30T02:00:03Z","title":"Keyword-driven Retrieval-Augmented Large Language Models for Cold-start\n  User Recommendations","summary":"  Recent advancements in Large Language Models (LLMs) have shown significant\npotential in enhancing recommender systems. However, addressing the cold-start\nrecommendation problem, where users lack historical data, remains a\nconsiderable challenge. In this paper, we introduce KALM4Rec (Keyword-driven\nRetrieval-Augmented Large Language Models for Cold-start User Recommendations),\na novel framework specifically designed to tackle this problem by requiring\nonly a few input keywords from users in a practical scenario of cold-start user\nrestaurant recommendations. KALM4Rec operates in two main stages: candidates\nretrieval and LLM-based candidates re-ranking. In the first stage,\nkeyword-driven retrieval models are used to identify potential candidates,\naddressing LLMs' limitations in processing extensive tokens and reducing the\nrisk of generating misleading information. In the second stage, we employ LLMs\nwith various prompting strategies, including zero-shot and few-shot techniques,\nto re-rank these candidates by integrating multiple examples directly into the\nLLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews\nfrom three English-speaking cities, shows that our proposed framework\nsignificantly improves recommendation quality. Specifically, the integration of\nin-context instructions with LLMs for re-ranking markedly enhances the\nperformance of the cold-start user recommender system.\n","authors":["Hai-Dang Kieu","Minh Duc Nguyen","Thanh-Son Nguyen","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2405.19612v2.pdf","comment":"10 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.05152v1","updated":"2024-09-08T16:35:19Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.05033v1","updated":"2024-09-08T08:57:12Z","published":"2024-09-08T08:57:12Z","title":"A Survey on Diffusion Models for Recommender Systems","summary":"  While traditional recommendation techniques have made significant strides in\nthe past decades, they still suffer from limited generalization performance\ncaused by factors like inadequate collaborative signals, weak latent\nrepresentations, and noisy data. In response, diffusion models (DMs) have\nemerged as promising solutions for recommender systems due to their robust\ngenerative capabilities, solid theoretical foundations, and improved training\nstability. To this end, in this paper, we present the first comprehensive\nsurvey on diffusion models for recommendation, and draw a bird's-eye view from\nthe perspective of the whole pipeline in real-world recommender systems. We\nsystematically categorize existing research works into three primary domains:\n(1) diffusion for data engineering & encoding, focusing on data augmentation\nand representation enhancement; (2) diffusion as recommender models, employing\ndiffusion models to directly estimate user preferences and rank items; and (3)\ndiffusion for content presentation, utilizing diffusion models to generate\npersonalized content such as fashion and advertisement creatives. Our taxonomy\nhighlights the unique strengths of diffusion models in capturing complex data\ndistributions and generating high-quality, diverse samples that closely align\nwith user preferences. We also summarize the core characteristics of the\nadapting diffusion models for recommendation, and further identify key areas\nfor future exploration, which helps establish a roadmap for researchers and\npractitioners seeking to advance recommender systems through the innovative\napplication of diffusion models. To further facilitate the research community\nof recommender systems based on diffusion models, we actively maintain a GitHub\nrepository for papers and other related resources in this rising direction\nhttps://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.\n","authors":["Jianghao Lin","Jiaqi Liu","Jiachen Zhu","Yunjia Xi","Chengkai Liu","Yangtian Zhang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05033v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.05022v1","updated":"2024-09-08T08:27:22Z","published":"2024-09-08T08:27:22Z","title":"Sequential Recommendation via Adaptive Robust Attention with\n  Multi-dimensional Embeddings","summary":"  Sequential recommendation models have achieved state-of-the-art performance\nusing self-attention mechanism. It has since been found that moving beyond only\nusing item ID and positional embeddings leads to a significant accuracy boost\nwhen predicting the next item. In recent literature, it was reported that a\nmulti-dimensional kernel embedding with temporal contextual kernels to capture\nusers' diverse behavioral patterns results in a substantial performance\nimprovement. In this study, we further improve the sequential recommender\nmodel's robustness and generalization by introducing a mix-attention mechanism\nwith a layer-wise noise injection (LNI) regularization. We refer to our\nproposed model as adaptive robust sequential recommendation framework (ADRRec),\nand demonstrate through extensive experiments that our model outperforms\nexisting self-attention architectures.\n","authors":["Linsey Pang","Amir Hossein Raffiee","Wei Liu","Keld Lundgaard"],"pdf_url":"https://arxiv.org/pdf/2409.05022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11119v2","updated":"2024-09-08T04:25:32Z","published":"2024-04-17T07:07:41Z","title":"DREAM: A Dual Representation Learning Model for Multimodal\n  Recommendation","summary":"  Multimodal recommendation focuses primarily on effectively exploiting both\nbehavioral and multimodal information for the recommendation task. However,\nmost existing models suffer from the following issues when fusing information\nfrom two different domains: (1) Previous works do not pay attention to the\nsufficient utilization of modal information by only using direct concatenation,\naddition, or simple linear layers for modal information extraction. (2)\nPrevious works treat modal features as learnable embeddings, which causes the\nmodal embeddings to gradually deviate from the original modal features during\nlearning. We refer to this issue as Modal Information Forgetting. (3) Previous\napproaches fail to account for the significant differences in the distribution\nbetween behavior and modality, leading to the issue of representation\nmisalignment. To address these challenges, this paper proposes a novel Dual\nREpresentAtion learning model for Multimodal Recommendation called DREAM. For\nsufficient information extraction, we introduce separate dual lines, including\nBehavior Line and Modal Line, in which the Modal-specific Encoder is applied to\nempower modal representations. To address the issue of Modal Information\nForgetting, we introduce the Similarity Supervised Signal to constrain the\nmodal representations. Additionally, we design a Behavior-Modal Alignment\nmodule to fuse the dual representations through Intra-Alignment and\nInter-Alignment. Extensive experiments on three public datasets demonstrate\nthat the proposed DREAM method achieves state-of-the-art (SOTA) results. The\nsource code will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Jiarui Jin","Yifan Liu","Ruilong Su","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v2.pdf","comment":"10 pages, 11 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.03336v2","updated":"2024-09-08T14:21:13Z","published":"2024-09-05T08:28:36Z","title":"Estimating Indoor Scene Depth Maps from Ultrasonic Echoes","summary":"  Measuring 3D geometric structures of indoor scenes requires dedicated depth\nsensors, which are not always available. Echo-based depth estimation has\nrecently been studied as a promising alternative solution. All previous studies\nhave assumed the use of echoes in the audible range. However, one major problem\nis that audible echoes cannot be used in quiet spaces or other situations where\nproducing audible sounds is prohibited. In this paper, we consider echo-based\ndepth estimation using inaudible ultrasonic echoes. While ultrasonic waves\nprovide high measurement accuracy in theory, the actual depth estimation\naccuracy when ultrasonic echoes are used has remained unclear, due to its\ndisadvantage of being sensitive to noise and susceptible to attenuation. We\nfirst investigate the depth estimation accuracy when the frequency of the sound\nsource is restricted to the high-frequency band, and found that the accuracy\ndecreased when the frequency was limited to ultrasonic ranges. Based on this\nobservation, we propose a novel deep learning method to improve the accuracy of\nultrasonic echo-based depth estimation by using audible echoes as auxiliary\ndata only during training. Experimental results with a public dataset\ndemonstrate that our method improves the estimation accuracy.\n","authors":["Junpei Honma","Akisato Kimura","Go Irie"],"pdf_url":"https://arxiv.org/pdf/2409.03336v2.pdf","comment":"ICIP 2024"},{"id":"http://arxiv.org/abs/2404.13306v2","updated":"2024-09-08T12:07:52Z","published":"2024-04-20T07:28:55Z","title":"FakeBench: Probing Explainable Fake Image Detection via Large Multimodal\n  Models","summary":"  The ability to distinguish whether an image is generated by artificial\nintelligence (AI) is a crucial ingredient in human intelligence, usually\naccompanied by a complex and dialectical forensic and reasoning process.\nHowever, current fake image detection models and databases focus on binary\nclassification without understandable explanations for the general populace.\nThis weakens the credibility of authenticity judgment and may conceal potential\nmodel biases. Meanwhile, large multimodal models (LMMs) have exhibited immense\nvisual-text capabilities on various tasks, bringing the potential for\nexplainable fake image detection. Therefore, we pioneer the probe of LMMs for\nexplainable fake image detection by presenting a multimodal database\nencompassing textual authenticity descriptions, the FakeBench. For\nconstruction, we first introduce a fine-grained taxonomy of generative visual\nforgery concerning human perception, based on which we collect forgery\ndescriptions in human natural language with a human-in-the-loop strategy.\nFakeBench examines LMMs with four evaluation criteria: detection, reasoning,\ninterpretation and fine-grained forgery analysis, to obtain deeper insights\ninto image authenticity-relevant capabilities. Experiments on various LMMs\nconfirm their merits and demerits in different aspects of fake image detection\ntasks. This research presents a paradigm shift towards transparency for the\nfake image detection area and reveals the need for greater emphasis on forensic\nelements in visual-language research and AI risk control. FakeBench will be\navailable at https://github.com/Yixuan423/FakeBench.\n","authors":["Yixuan Li","Xuelin Liu","Xiaoyang Wang","Bu Sung Lee","Shiqi Wang","Anderson Rocha","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2404.13306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04999v1","updated":"2024-09-08T07:08:58Z","published":"2024-09-08T07:08:58Z","title":"Visual Grounding with Multi-modal Conditional Adaptation","summary":"  Visual grounding is the task of locating objects specified by natural\nlanguage expressions. Existing methods extend generic object detection\nframeworks to tackle this task. They typically extract visual and textual\nfeatures separately using independent visual and textual encoders, then fuse\nthese features in a multi-modal decoder for final prediction. However, visual\ngrounding presents unique challenges. It often involves locating objects with\ndifferent text descriptions within the same image. Existing methods struggle\nwith this task because the independent visual encoder produces identical visual\nfeatures for the same image, limiting detection performance. Some recently\napproaches propose various language-guided visual encoders to address this\nissue, but they mostly rely solely on textual information and require\nsophisticated designs. In this paper, we introduce Multi-modal Conditional\nAdaptation (MMCA), which enables the visual encoder to adaptively update\nweights, directing its focus towards text-relevant regions. Specifically, we\nfirst integrate information from different modalities to obtain multi-modal\nembeddings. Then we utilize a set of weighting coefficients, which generated\nfrom the multimodal embeddings, to reorganize the weight update matrices and\napply them to the visual encoder of the visual grounding model. Extensive\nexperiments on four widely used datasets demonstrate that MMCA achieves\nsignificant improvements and state-of-the-art results. Ablation experiments\nfurther demonstrate the lightweight and efficiency of our method. Our source\ncode is available at: https://github.com/Mr-Bigworth/MMCA.\n","authors":["Ruilin Yao","Shengwu Xiong","Yichen Zhao","Yi Rong"],"pdf_url":"https://arxiv.org/pdf/2409.04999v1.pdf","comment":"Accepted by ACM MM 2024 [Oral]"},{"id":"http://arxiv.org/abs/2404.11119v2","updated":"2024-09-08T04:25:32Z","published":"2024-04-17T07:07:41Z","title":"DREAM: A Dual Representation Learning Model for Multimodal\n  Recommendation","summary":"  Multimodal recommendation focuses primarily on effectively exploiting both\nbehavioral and multimodal information for the recommendation task. However,\nmost existing models suffer from the following issues when fusing information\nfrom two different domains: (1) Previous works do not pay attention to the\nsufficient utilization of modal information by only using direct concatenation,\naddition, or simple linear layers for modal information extraction. (2)\nPrevious works treat modal features as learnable embeddings, which causes the\nmodal embeddings to gradually deviate from the original modal features during\nlearning. We refer to this issue as Modal Information Forgetting. (3) Previous\napproaches fail to account for the significant differences in the distribution\nbetween behavior and modality, leading to the issue of representation\nmisalignment. To address these challenges, this paper proposes a novel Dual\nREpresentAtion learning model for Multimodal Recommendation called DREAM. For\nsufficient information extraction, we introduce separate dual lines, including\nBehavior Line and Modal Line, in which the Modal-specific Encoder is applied to\nempower modal representations. To address the issue of Modal Information\nForgetting, we introduce the Similarity Supervised Signal to constrain the\nmodal representations. Additionally, we design a Behavior-Modal Alignment\nmodule to fuse the dual representations through Intra-Alignment and\nInter-Alignment. Extensive experiments on three public datasets demonstrate\nthat the proposed DREAM method achieves state-of-the-art (SOTA) results. The\nsource code will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Jiarui Jin","Yifan Liu","Ruilong Su","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v2.pdf","comment":"10 pages, 11 figures"}]},"2024-09-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.14774v2","updated":"2024-09-07T16:11:36Z","published":"2024-04-23T06:29:48Z","title":"CoST: Contrastive Quantization based Semantic Tokenization for\n  Generative Recommendation","summary":"  Embedding-based retrieval serves as a dominant approach to candidate item\nmatching for industrial recommender systems. With the success of generative AI,\ngenerative retrieval has recently emerged as a new retrieval paradigm for\nrecommendation, which casts item retrieval as a generation problem. Its model\nconsists of two stages: semantic tokenization and autoregressive generation.\nThe first stage involves item tokenization that constructs discrete semantic\ntokens to index items, while the second stage autoregressively generates\nsemantic tokens of candidate items. Therefore, semantic tokenization serves as\na crucial preliminary step for training generative recommendation models.\nExisting research usually employs a vector quantizier with reconstruction loss\n(e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to\ncapture the essential neighborhood relationships that are vital for effective\nitem modeling in recommender systems. In this paper, we propose a contrastive\nquantization-based semantic tokenization approach, named CoST, which harnesses\nboth item relationships and semantic information to learn semantic tokens. Our\nexperimental results highlight the significant impact of semantic tokenization\non generative recommendation performance, with CoST achieving up to a 43%\nimprovement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over\nprevious baselines.\n","authors":["Jieming Zhu","Mengqun Jin","Qijiong Liu","Zexuan Qiu","Zhenhua Dong","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.14774v2.pdf","comment":"Accepted by RecSys'2024"},{"id":"http://arxiv.org/abs/2407.21300v3","updated":"2024-09-07T15:02:48Z","published":"2024-07-31T03:00:59Z","title":"Implementing Streaming algorithm and k-means clusters to RAG","summary":"  Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02335v2","updated":"2024-09-07T14:29:11Z","published":"2024-02-04T04:13:31Z","title":"Video Editing for Video Retrieval","summary":"  Though pre-training vision-language models have demonstrated significant\nbenefits in boosting video-text retrieval performance from large-scale web\nvideos, fine-tuning still plays a critical role with manually annotated clips\nwith start and end times, which requires considerable human effort. To address\nthis issue, we explore an alternative cheaper source of annotations, single\ntimestamps, for video-text retrieval. We initialise clips from timestamps in a\nheuristic way to warm up a retrieval model. Then a video clip editing method is\nproposed to refine the initial rough boundaries to improve retrieval\nperformance. A student-teacher network is introduced for video clip editing.\nThe teacher model is employed to edit the clips in the training set whereas the\nstudent model trains on the edited clips. The teacher weights are updated from\nthe student's after the student's performance increases. Our method is model\nagnostic and applicable to any retrieval models. We conduct experiments based\non three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip.\nExperiments conducted on three video retrieval datasets, YouCook2, DiDeMo and\nActivityNet-Captions show that our edited clips consistently improve retrieval\nperformance over initial clips across all the three retrieval models.\n","authors":["Bin Zhu","Kevin Flanagan","Adriano Fragomeni","Michael Wray","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2402.02335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04827v1","updated":"2024-09-07T13:41:37Z","published":"2024-09-07T13:41:37Z","title":"Incorporate LLMs with Influential Recommender System","summary":"  Recommender systems have achieved increasing accuracy over the years.\nHowever, this precision often leads users to narrow their interests, resulting\nin issues such as limited diversity and the creation of echo chambers. Current\nresearch addresses these challenges through proactive recommender systems by\nrecommending a sequence of items (called influence path) to guide user interest\nin the target item. However, existing methods struggle to construct a coherent\ninfluence path that builds up with items the user is likely to enjoy. In this\npaper, we leverage the Large Language Model's (LLMs) exceptional ability for\npath planning and instruction following, introducing a novel approach named\nLLM-based Influence Path Planning (LLM-IPP). Our approach maintains coherence\nbetween consecutive recommendations and enhances user acceptability of the\nrecommended items. To evaluate LLM-IPP, we implement various user simulators\nand metrics to measure user acceptability and path coherence. Experimental\nresults demonstrate that LLM-IPP significantly outperforms traditional\nproactive recommender systems. This study pioneers the integration of LLMs into\nproactive recommender systems, offering a reliable and user-engaging\nmethodology for future recommendation technologies.\n","authors":["Mingze Wang","Shuxian Bi","Wenjie Wang","Chongming Gao","Yangyang Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2409.04827v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.04810v1","updated":"2024-09-07T12:42:58Z","published":"2024-09-07T12:42:58Z","title":"Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing\n  Recommendation","summary":"  Recent work has improved recommendation models remarkably by equipping them\nwith debiasing methods. Due to the unavailability of fully-exposed datasets,\nmost existing approaches resort to randomly-exposed datasets as a proxy for\nevaluating debiased models, employing traditional evaluation scheme to\nrepresent the recommendation performance. However, in this study, we reveal\nthat traditional evaluation scheme is not suitable for randomly-exposed\ndatasets, leading to inconsistency between the Recall performance obtained\nusing randomly-exposed datasets and that obtained using fully-exposed datasets.\nSuch inconsistency indicates the potential unreliability of experiment\nconclusions on previous debiasing techniques and calls for unbiased Recall\nevaluation using randomly-exposed datasets. To bridge the gap, we propose the\nUnbiased Recall Evaluation (URE) scheme, which adjusts the utilization of\nrandomly-exposed datasets to unbiasedly estimate the true Recall performance on\nfully-exposed datasets. We provide theoretical evidence to demonstrate the\nrationality of URE and perform extensive experiments on real-world datasets to\nvalidate its soundness.\n","authors":["Chengbing Wang","Wentao Shi","Jizhi Zhang","Wenjie Wang","Hang Pan","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2409.04810v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.06809v2","updated":"2024-09-07T09:02:29Z","published":"2024-08-13T10:58:29Z","title":"Reformulating Conversational Recommender Systems as Tri-Phase Offline\n  Policy Learning","summary":"  Existing Conversational Recommender Systems (CRS) predominantly utilize user\nsimulators for training and evaluating recommendation policies. These\nsimulators often oversimplify the complexity of user interactions by focusing\nsolely on static item attributes, neglecting the rich, evolving preferences\nthat characterize real-world user behavior. This limitation frequently leads to\nmodels that perform well in simulated environments but falter in actual\ndeployment. Addressing these challenges, this paper introduces the Tri-Phase\nOffline Policy Learning-based Conversational Recommender System (TCRS), which\nsignificantly reduces dependency on real-time interactions and mitigates\noverfitting issues prevalent in traditional approaches. TCRS integrates a\nmodel-based offline learning strategy with a controllable user simulation that\ndynamically aligns with both personalized and evolving user preferences.\nThrough comprehensive experiments, TCRS demonstrates enhanced robustness,\nadaptability, and accuracy in recommendations, outperforming traditional CRS\nmodels in diverse user scenarios. This approach not only provides a more\nrealistic evaluation environment but also facilitates a deeper understanding of\nuser behavior dynamics, thereby refining the recommendation process.\n","authors":["Gangyi Zhang","Chongming Gao","Hang Pan","Runzhe Teng","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2408.06809v2.pdf","comment":"Accepted at CIKM 2024"},{"id":"http://arxiv.org/abs/2408.11345v2","updated":"2024-09-07T07:54:32Z","published":"2024-08-21T05:09:53Z","title":"Deep Tree-based Retrieval for Efficient Recommendation: Theory and\n  Method","summary":"  With the development of deep learning techniques, deep recommendation models\nalso achieve remarkable improvements in terms of recommendation accuracy.\nHowever, due to the large number of candidate items in practice and the high\ncost of preference computation, these methods also suffer from low efficiency\nof recommendation. The recently proposed tree-based deep recommendation models\nalleviate the problem by directly learning tree structure and representations\nunder the guidance of recommendation objectives. However, such models have\nshortcomings. The max-heap assumption in the hierarchical tree, in which the\npreference for a parent node should be the maximum between the preferences for\nits children, is difficult to satisfy in their binary classification\nobjectives. To this end, we propose Tree-based Deep Retrieval (TDR for short)\nfor efficient recommendation. In TDR, all the trees generated during the\ntraining process are retained to form the forest. When learning the node\nrepresentation of each tree, we have to satisfy the max-heap assumption as much\nas possible and mimic beam search behavior over the tree in the training stage.\nThis is achieved by TDR to regard the training task as multi-classification\nover tree nodes at the same level. However, the number of tree nodes grows\nexponentially with levels, making us train the preference model with the\nguidance of the sampled-softmax technique. The experiments are conducted on\nreal-world datasets, validating the effectiveness of the proposed preference\nmodel learning method and tree learning method.\n","authors":["Ze Liu","Jin Zhang","Chao Feng","Defu Lian","Jie Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.11345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04701v1","updated":"2024-09-07T03:54:46Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be \"over-compressed\" in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in suboptimal\nrepresentations. In this paper, we introduce a novel method called \"late\nchunking,\" which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling. The resulting chunk embeddings capture the full\ncontextual information, leading to superior results across various retrieval\ntasks without the need for additional training. Moreover, our method is generic\nenough to be applied to any long-context embedding model.\n","authors":["Michael Günther","Isabelle Mohr","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v1.pdf","comment":"4 pages, early draft"},{"id":"http://arxiv.org/abs/2312.09425v3","updated":"2024-09-07T01:01:44Z","published":"2023-11-21T23:35:44Z","title":"YouTube Videos for Public Health Literacy? A Machine Learning Pipeline\n  to Curate Covid-19 Videos","summary":"  The COVID-19 pandemic has highlighted the dire necessity to improve public\nhealth literacy for societal resilience. YouTube, the largest video-sharing\nsocial media platform, provides a vast repository of user-generated health\ninformation in a multi-media-rich format which may be easier for the public to\nunderstand and use if major concerns about content quality and accuracy are\naddressed. This study develops an automated solution to identify, retrieve and\nshortlist medically relevant and understandable YouTube videos that domain\nexperts can subsequently review and recommend for disseminating and educating\nthe public on the COVID-19 pandemic and similar public health outbreaks. Our\napproach leverages domain knowledge from human experts and machine learning and\nnatural language processing methods to provide a scalable, replicable, and\ngeneralizable approach that can also be applied to enhance the management of\nmany health conditions.\n","authors":["Yawen Guo","Xiao Liu","Anjana Susarla","Rema Padman"],"pdf_url":"https://arxiv.org/pdf/2312.09425v3.pdf","comment":"Studies in health technology and informatics(MedInfo) 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2209.03275v3","updated":"2024-09-07T20:10:07Z","published":"2022-09-07T16:27:34Z","title":"Multimodal Speech Enhancement Using Burst Propagation","summary":"  This paper proposes the MBURST, a novel multimodal solution for audio-visual\nspeech enhancements that consider the most recent neurological discoveries\nregarding pyramidal cells of the prefrontal cortex and other brain regions. The\nso-called burst propagation implements several criteria to address the credit\nassignment problem in a more biologically plausible manner: steering the sign\nand magnitude of plasticity through feedback, multiplexing the feedback and\nfeedforward information across layers through different weight connections,\napproximating feedback and feedforward connections, and linearizing the\nfeedback signals. MBURST benefits from such capabilities to learn correlations\nbetween the noisy signal and the visual stimuli, thus attributing meaning to\nthe speech by amplifying relevant information and suppressing noise.\nExperiments conducted over a Grid Corpus and CHiME3-based dataset show that\nMBURST can reproduce similar mask reconstructions to the multimodal\nbackpropagation-based baseline while demonstrating outstanding energy\nefficiency management, reducing the neuron firing rates to values up to\n\\textbf{$70\\%$} lower. Such a feature implies more sustainable implementations,\nsuitable and desirable for hearing aids or any other similar embedded systems.\n","authors":["Mohsin Raza","Leandro A. Passos","Ahmed Khubaib","Ahsan Adeel"],"pdf_url":"https://arxiv.org/pdf/2209.03275v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04828v1","updated":"2024-09-07T13:41:37Z","published":"2024-09-07T13:41:37Z","title":"POINTS: Improving Your Vision-language Model with Affordable Strategies","summary":"  In recent years, vision-language models have made significant strides,\nexcelling in tasks like optical character recognition and geometric\nproblem-solving. However, several critical issues remain: 1) Proprietary models\noften lack transparency about their architectures, while open-source models\nneed more detailed ablations of their training strategies. 2) Pre-training data\nin open-source works is under-explored, with datasets added empirically, making\nthe process cumbersome. 3) Fine-tuning often focuses on adding datasets,\nleading to diminishing returns. To address these issues, we propose the\nfollowing contributions: 1) We trained a robust baseline model using the latest\nadvancements in vision-language models, introducing effective improvements and\nconducting comprehensive ablation and validation for each technique. 2)\nInspired by recent work on large language models, we filtered pre-training data\nusing perplexity, selecting the lowest perplexity data for training. This\napproach allowed us to train on a curated 1M dataset, achieving competitive\nperformance. 3) During visual instruction tuning, we used model soup on\ndifferent datasets when adding more datasets yielded marginal improvements.\nThese innovations resulted in a 9B parameter model that performs competitively\nwith state-of-the-art models. Our strategies are efficient and lightweight,\nmaking them easily adoptable by the community.\n","authors":["Yuan Liu","Zhongyin Zhao","Ziyuan Zhuang","Le Tian","Xiao Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.04828v1.pdf","comment":null}]},"2024-09-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.04649v1","updated":"2024-09-06T23:16:06Z","published":"2024-09-06T23:16:06Z","title":"Preserving Individuality while Following the Crowd: Understanding the\n  Role of User Taste and Crowd Wisdom in Online Product Rating Prediction","summary":"  Numerous algorithms have been developed for online product rating prediction,\nbut the specific influence of user and product information in determining the\nfinal prediction score remains largely unexplored. Existing research often\nrelies on narrowly defined data settings, which overlooks real-world challenges\nsuch as the cold-start problem, cross-category information utilization, and\nscalability and deployment issues. To delve deeper into these aspects, and\nparticularly to uncover the roles of individual user taste and collective\nwisdom, we propose a unique and practical approach that emphasizes historical\nratings at both the user and product levels, encapsulated using a continuously\nupdated dynamic tree representation. This representation effectively captures\nthe temporal dynamics of users and products, leverages user information across\nproduct categories, and provides a natural solution to the cold-start problem.\nFurthermore, we have developed an efficient data processing strategy that makes\nthis approach highly scalable and easily deployable. Comprehensive experiments\nin real industry settings demonstrate the effectiveness of our approach.\nNotably, our findings reveal that individual taste dominates over collective\nwisdom in online product rating prediction, a perspective that contrasts with\nthe commonly observed wisdom of the crowd phenomenon in other domains. This\ndominance of individual user taste is consistent across various model types,\nincluding the boosting tree model, recurrent neural network (RNN), and\ntransformer-based architectures. This observation holds true across the overall\npopulation, within individual product categories, and in cold-start scenarios.\nOur findings underscore the significance of individual user tastes in the\ncontext of online product rating prediction and the robustness of our approach\nacross different model architectures.\n","authors":["Liang Wang","Shubham Jain","Yingtong Dou","Junpeng Wang","Chin-Chia Michael Yeh","Yujie Fan","Prince Aboagye","Yan Zheng","Xin Dai","Zhongfang Zhuang","Uday Singh Saini","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04649v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2302.11157v2","updated":"2024-09-06T19:30:26Z","published":"2023-02-22T05:41:27Z","title":"FiNER-ORD: Financial Named Entity Recognition Open Research Dataset","summary":"  Over the last two decades, the development of the CoNLL-2003 named entity\nrecognition (NER) dataset has helped enhance the capabilities of deep learning\nand natural language processing (NLP). The finance domain, characterized by its\nunique semantic and lexical variations for the same entities, presents specific\nchallenges to the NER task; thus, a domain-specific customized dataset is\ncrucial for advancing research in this field. In our work, we develop the first\nhigh-quality English Financial NER Open Research Dataset (FiNER-ORD). We\nbenchmark multiple pre-trained language models (PLMs) and large-language models\n(LLMs) on FiNER-ORD. We believe our proposed FiNER-ORD dataset will open future\nopportunities to use FiNER-ORD as a benchmark for financial domain-specific NER\nand NLP tasks. Our dataset, models, and code are publicly available on GitHub\nand Hugging Face under CC BY-NC 4.0 license.\n","authors":["Agam Shah","Abhinav Gullapalli","Ruchit Vithani","Michael Galarnyk","Sudheer Chava"],"pdf_url":"https://arxiv.org/pdf/2302.11157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03140v2","updated":"2024-09-06T18:41:50Z","published":"2024-09-05T00:25:37Z","title":"GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase\n  Recommendation","summary":"  Online sellers and advertisers are recommended keyphrases for their listed\nproducts, which they bid on to enhance their sales. One popular paradigm that\ngenerates such recommendations is Extreme Multi-Label Classification (XMC),\nwhich involves tagging/mapping keyphrases to items. We outline the limitations\nof using traditional item-query based tagging or mapping techniques for\nkeyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an\ninnovative graph-based approach that recommends keyphrases to sellers using\nextraction of token permutations from item titles. Additionally, we demonstrate\nthat relying on traditional metrics such as precision/recall can be misleading\nin practical applications, thereby necessitating a combination of metrics to\nevaluate performance in real-world scenarios. These metrics are designed to\nassess the relevance of keyphrases to items and the potential for buyer\noutreach. GraphEx outperforms production models at eBay, achieving the\nobjectives mentioned above. It supports near real-time inferencing in\nresource-constrained production environments and scales effectively for\nbillions of items.\n","authors":["Ashirbad Mishra","Soumik Dey","Marshall Wu","Jinyu Zhao","He Yu","Kaichen Ni","Binbin Li","Kamesh Madduri"],"pdf_url":"https://arxiv.org/pdf/2409.03140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04540v1","updated":"2024-09-06T18:10:42Z","published":"2024-09-06T18:10:42Z","title":"A Unified Framework for Cross-Domain Recommendation","summary":"  In addressing the persistent challenges of data-sparsity and cold-start\nissues in domain-expert recommender systems, Cross-Domain Recommendation (CDR)\nemerges as a promising methodology. CDR aims at enhancing prediction\nperformance in the target domain by leveraging interaction knowledge from\nrelated source domains, particularly through users or items that span across\nmultiple domains (e.g., Short-Video and Living-Room). For academic research\npurposes, there are a number of distinct aspects to guide CDR method designing,\nincluding the auxiliary domain number, domain-overlapped element, user-item\ninteraction types, and downstream tasks. With so many different CDR combination\nscenario settings, the proposed scenario-expert approaches are tailored to\naddress a specific vertical CDR scenario, and often lack the capacity to adapt\nto multiple horizontal scenarios. In an effect to coherently adapt to various\nscenarios, and drawing inspiration from the concept of domain-invariant\ntransfer learning, we extend the former SOTA model UniCDR in five different\naspects, named as UniCDR+. Our work was successfully deployed on the Kuaishou\nLiving-Room RecSys.\n","authors":["Jiangxia Cao","Shen Wang","Gaode Chen","Rui Huang","Shuang Yang","Zhaojie Liu","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.04540v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.04432v1","updated":"2024-09-06T17:54:43Z","published":"2024-09-06T17:54:43Z","title":"A Survey on Knowledge Organization Systems of Research Fields: Resources\n  and Challenges","summary":"  Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.\n","authors":["Angelo Salatino","Tanay Aggarwal","Andrea Mannocci","Francesco Osborne","Enrico Motta"],"pdf_url":"https://arxiv.org/pdf/2409.04432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04339v1","updated":"2024-09-06T15:17:40Z","published":"2024-09-06T15:17:40Z","title":"How Fair is Your Diffusion Recommender Model?","summary":"  Diffusion-based recommender systems have recently proven to outperform\ntraditional generative recommendation approaches, such as variational\nautoencoders and generative adversarial networks. Nevertheless, the machine\nlearning literature has raised several concerns regarding the possibility that\ndiffusion models, while learning the distribution of data samples, may\ninadvertently carry information bias and lead to unfair outcomes. In light of\nthis aspect, and considering the relevance that fairness has held in\nrecommendations over the last few decades, we conduct one of the first fairness\ninvestigations in the literature on DiffRec, a pioneer approach in\ndiffusion-based recommendation. First, we propose an experimental setting\ninvolving DiffRec (and its variant L-DiffRec) along with nine state-of-the-art\nrecommendation models, two popular recommendation datasets from the\nfairness-aware literature, and six metrics accounting for accuracy and\nconsumer/provider fairness. Then, we perform a twofold analysis, one assessing\nmodels' performance under accuracy and recommendation fairness separately, and\nthe other identifying if and to what extent such metrics can strike a\nperformance trade-off. Experimental results from both studies confirm the\ninitial unfairness warnings but pave the way for how to address them in future\nresearch directions.\n","authors":["Daniele Malitesta","Giacomo Medda","Erasmo Purificato","Ludovico Boratto","Fragkiskos D. Malliaros","Mirko Marras","Ernesto William De Luca"],"pdf_url":"https://arxiv.org/pdf/2409.04339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04329v1","updated":"2024-09-06T15:05:12Z","published":"2024-09-06T15:05:12Z","title":"Enhancing Sequential Music Recommendation with Personalized Popularity\n  Awareness","summary":"  In the realm of music recommendation, sequential recommender systems have\nshown promise in capturing the dynamic nature of music consumption.\nNevertheless, traditional Transformer-based models, such as SASRec and\nBERT4Rec, while effective, encounter challenges due to the unique\ncharacteristics of music listening habits. In fact, existing models struggle to\ncreate a coherent listening experience due to rapidly evolving preferences.\nMoreover, music consumption is characterized by a prevalence of repeated\nlistening, i.e., users frequently return to their favourite tracks, an\nimportant signal that could be framed as individual or personalized popularity.\n  This paper addresses these challenges by introducing a novel approach that\nincorporates personalized popularity information into sequential\nrecommendation. By combining user-item popularity scores with model-generated\nscores, our method effectively balances the exploration of new music with the\nsatisfaction of user preferences. Experimental results demonstrate that a\nPersonalized Most Popular recommender, a method solely based on user-specific\npopularity, outperforms existing state-of-the-art models. Furthermore,\naugmenting Transformer-based models with personalized popularity awareness\nyields superior performance, showing improvements ranging from 25.2% to 69.8%.\nThe code for this paper is available at\nhttps://github.com/sisinflab/personalized-popularity-awareness.\n","authors":["Davide Abbattista","Vito Walter Anelli","Tommaso Di Noia","Craig Macdonald","Aleksandr Vladimirovich Petrov"],"pdf_url":"https://arxiv.org/pdf/2409.04329v1.pdf","comment":"Accepted by RecSys'24 as an LBR paper"},{"id":"http://arxiv.org/abs/2403.00884v3","updated":"2024-09-06T14:49:21Z","published":"2024-03-01T10:01:36Z","title":"Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment","summary":"  Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.\n","authors":["Margherita Martorana","Tobias Kuhn","Lise Stork","Jacco van Ossenbruggen"],"pdf_url":"https://arxiv.org/pdf/2403.00884v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03708v2","updated":"2024-09-06T14:18:20Z","published":"2024-09-05T17:14:23Z","title":"RAG based Question-Answering for Contextual Response Prediction System","summary":"  Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.\n","authors":["Sriram Veturi","Saurabh Vaichal","Reshma Lal Jagadheesh","Nafis Irtiza Tripto","Nian Yan"],"pdf_url":"https://arxiv.org/pdf/2409.03708v2.pdf","comment":"Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages"},{"id":"http://arxiv.org/abs/2408.09236v3","updated":"2024-09-06T13:34:16Z","published":"2024-08-17T16:04:31Z","title":"Hybrid Semantic Search: Unveiling User Intent Beyond Keywords","summary":"  This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.\n","authors":["Aman Ahluwalia","Bishwajit Sutradhar","Karishma Ghosh","Indrapal Yadav","Arpan Sheetal","Prashant Patil"],"pdf_url":"https://arxiv.org/pdf/2408.09236v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15675v2","updated":"2024-09-06T13:20:40Z","published":"2024-04-24T06:05:35Z","title":"Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce\n  Search","summary":"  Leveraging generative retrieval (GR) techniques to enhance search systems is\nan emerging methodology that has shown promising results in recent years. In\nGR, a text-to-text model maps string queries directly to relevant document\nidentifiers (docIDs), dramatically simplifying the retrieval process. However,\nwhen applying most GR models in large-scale E-commerce for personalized item\nsearch, we must face two key problems in encoding and decoding. (1) Existing\ndocID generation methods ignore the encoding of efficiency information, which\nis critical in E-commerce. (2) The positional information is important in\ndecoding docIDs, while prior studies have not adequately discriminated the\nsignificance of positional information or well exploited the inherent\ninterrelation among these positions. To overcome these problems, we introduce\nan efficient Hierarchical encoding-decoding Generative retrieval method\n(Hi-Gen) for large-scale personalized E-commerce search systems. Specifically,\nwe first design a representation learning model using metric learning to learn\ndiscriminative feature representations of items to capture semantic relevance\nand efficiency information. Then, we propose a category-guided hierarchical\nclustering scheme that makes full use of the semantic and efficiency\ninformation of items to facilitate docID generation. Finally, we design a\nposition-aware loss to discriminate the importance of positions and mine the\ninherent interrelation between different tokens at the same position. This loss\nboosts the performance of the language model used in the decoding stage.\nBesides, we propose two variants of Hi-Gen (Hi-Gen-I2I and Hi-Gen-Cluster) to\nsupport online real-time large-scale recall in the online serving process.\nHi-Gen gets 3.30% and 4.62% improvements over SOTA for Recall@1 on the public\nand industry datasets, respectively.\n","authors":["Yanjing Wu","Yinfu Feng","Jian Wang","Wenji Zhou","Yunan Ye","Rong Xiao","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.15675v2.pdf","comment":"Accepted by ICDM 2024"},{"id":"http://arxiv.org/abs/2409.04244v1","updated":"2024-09-06T12:51:10Z","published":"2024-09-06T12:51:10Z","title":"WarpAdam: A new Adam optimizer based on Meta-Learning approach","summary":"  Optimal selection of optimization algorithms is crucial for training deep\nlearning models. The Adam optimizer has gained significant attention due to its\nefficiency and wide applicability. However, to enhance the adaptability of\noptimizers across diverse datasets, we propose an innovative optimization\nstrategy by integrating the 'warped gradient descend'concept from Meta Learning\ninto the Adam optimizer. In the conventional Adam optimizer, gradients are\nutilized to compute estimates of gradient mean and variance, subsequently\nupdating model parameters. Our approach introduces a learnable distortion\nmatrix, denoted as P, which is employed for linearly transforming gradients.\nThis transformation slightly adjusts gradients during each iteration, enabling\nthe optimizer to better adapt to distinct dataset characteristics. By learning\nan appropriate distortion matrix P, our method aims to adaptively adjust\ngradient information across different data distributions, thereby enhancing\noptimization performance. Our research showcases the potential of this novel\napproach through theoretical insights and empirical evaluations. Experimental\nresults across various tasks and datasets validate the superiority of our\noptimizer that integrates the 'warped gradient descend' concept in terms of\nadaptability. Furthermore, we explore effective strategies for training the\nadaptation matrix P and identify scenarios where this method can yield optimal\nresults. In summary, this study introduces an innovative approach that merges\nthe 'warped gradient descend' concept from Meta Learning with the Adam\noptimizer. By introducing a learnable distortion matrix P within the optimizer,\nwe aim to enhance the model's generalization capability across diverse data\ndistributions, thus opening up new possibilities in the field of deep learning\noptimization.\n","authors":["Chengxi Pan","Junshang Chen","Jingrui Ye"],"pdf_url":"https://arxiv.org/pdf/2409.04244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02664v3","updated":"2024-09-06T11:38:00Z","published":"2024-05-04T13:25:06Z","title":"MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering","summary":"  Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering\n","authors":["Roomani Srivastava","Suraj Prasad","Lipika Bhat","Sarvesh Deshpande","Barnali Das","Kshitij Jadhav"],"pdf_url":"https://arxiv.org/pdf/2405.02664v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04056v1","updated":"2024-09-06T06:53:45Z","published":"2024-09-06T06:53:45Z","title":"Refining Wikidata Taxonomy using Large Language Models","summary":"  Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC.\n","authors":["Yiwen Peng","Thomas Bonald","Mehwish Alam"],"pdf_url":"https://arxiv.org/pdf/2409.04056v1.pdf","comment":"ACM International Conference on Information and Knowledge Management,\n  Oct 2024, Boise, Idaho, United States"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16879v2","updated":"2024-09-06T17:17:16Z","published":"2024-08-29T20:05:02Z","title":"MSLIQA: Enhancing Learning Representations for Image Quality Assessment\n  through Multi-Scale Learning","summary":"  No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due\nto the diversity of distortions and the lack of large annotated datasets. Many\nstudies have attempted to tackle these challenges by developing more accurate\nNR-IQA models, often employing complex and computationally expensive networks,\nor by bridging the domain gap between various distortions to enhance\nperformance on test datasets. In our work, we improve the performance of a\ngeneric lightweight NR-IQA model by introducing a novel augmentation strategy\nthat boosts its performance by almost 28\\%. This augmentation strategy enables\nthe network to better discriminate between different distortions in various\nparts of the image by zooming in and out. Additionally, the inclusion of\ntest-time augmentation further enhances performance, making our lightweight\nnetwork's results comparable to the current state-of-the-art models, simply\nthrough the use of augmentations.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.16879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17057v2","updated":"2024-09-06T17:15:49Z","published":"2024-08-30T07:32:19Z","title":"LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model","summary":"  Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.17057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04013v1","updated":"2024-09-06T03:53:59Z","published":"2024-09-06T03:53:59Z","title":"3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian\n  Geometric Priors","summary":"  Multi-view image compression is vital for 3D-related applications. To\neffectively model correlations between views, existing methods typically\npredict disparity between two views on a 2D plane, which works well for small\ndisparities, such as in stereo images, but struggles with larger disparities\ncaused by significant view changes. To address this, we propose a novel\napproach: learning-based multi-view image coding with 3D Gaussian geometric\npriors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive\ngeometric priors of the 3D scene, enabling more accurate disparity estimation\nacross views within the compression model. Additionally, we introduce a depth\nmap compression model to reduce redundancy in geometric information between\nviews. A multi-view sequence ordering method is also proposed to enhance\ncorrelations between adjacent views. Experimental results demonstrate that\n3D-GP-LMVIC surpasses both traditional and learning-based methods in\nperformance, while maintaining fast encoding and decoding speed.\n","authors":["Yujun Huang","Bin Chen","Niu Lian","Baoyi An","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2409.04013v1.pdf","comment":"19pages, 8 figures, conference"}]},"2024-09-05T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.03928v1","updated":"2024-09-05T22:22:57Z","published":"2024-09-05T22:22:57Z","title":"RETAIN: Interactive Tool for Regression Testing Guided LLM Migration","summary":"  Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame.\n","authors":["Tanay Dixit","Daniel Lee","Sally Fang","Sai Sree Harsha","Anirudh Sureshan","Akash Maharaj","Yunyao Li"],"pdf_url":"https://arxiv.org/pdf/2409.03928v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.12580v2","updated":"2024-09-05T21:29:32Z","published":"2024-06-18T13:06:58Z","title":"Behavior-Dependent Linear Recurrent Units for Efficient Sequential\n  Recommendation","summary":"  Sequential recommender systems aims to predict the users' next interaction\nthrough user behavior modeling with various operators like RNNs and attentions.\nHowever, existing models generally fail to achieve the three golden principles\nfor sequential recommendation simultaneously, i.e., training efficiency,\nlow-cost inference, and strong performance. To this end, we propose RecBLR, an\nEfficient Sequential Recommendation Model based on Behavior-Dependent Linear\nRecurrent Units to accomplish the impossible triangle of the three principles.\nBy incorporating gating mechanisms and behavior-dependent designs into linear\nrecurrent units, our model significantly enhances user behavior modeling and\nrecommendation performance. Furthermore, we unlock the parallelizable training\nas well as inference efficiency for our model by designing a hardware-aware\nscanning acceleration algorithm with a customized CUDA kernel. Extensive\nexperiments on real-world datasets with varying lengths of user behavior\nsequences demonstrate RecBLR's remarkable effectiveness in simultaneously\nachieving all three golden principles - strong recommendation performance,\ntraining efficiency, and low-cost inference, while exhibiting excellent\nscalability to datasets with long user interaction histories.\n","authors":["Chengkai Liu","Jianghao Lin","Hanzhou Liu","Jianling Wang","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2406.12580v2.pdf","comment":"Accepted to CIKM 2024"},{"id":"http://arxiv.org/abs/2409.03504v1","updated":"2024-09-05T13:18:01Z","published":"2024-09-05T13:18:01Z","title":"HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual\n  POI Retrieval at Baidu Maps","summary":"  The increasing interest in international travel has raised the demand of\nretrieving point of interests in multiple languages. This is even superior to\nfind local venues such as restaurants and scenic spots in unfamiliar languages\nwhen traveling abroad. Multilingual POI retrieval, enabling users to find\ndesired POIs in a demanded language using queries in numerous languages, has\nbecome an indispensable feature of today's global map applications such as\nBaidu Maps. This task is non-trivial because of two key challenges: (1)\nvisiting sparsity and (2) multilingual query-POI matching. To this end, we\npropose a Heterogeneous Graph Attention Matching Network (HGAMN) to\nconcurrently address both challenges. Specifically, we construct a\nheterogeneous graph that contains two types of nodes: POI node and query node\nusing the search logs of Baidu Maps. To alleviate challenge \\#1, we construct\nedges between different POI nodes to link the low-frequency POIs with the\nhigh-frequency ones, which enables the transfer of knowledge from the latter to\nthe former. To mitigate challenge \\#2, we construct edges between POI and query\nnodes based on the co-occurrences between queries and POIs, where queries in\ndifferent languages and formulations can be aggregated for individual POIs.\nMoreover, we develop an attention-based network to jointly learn node\nrepresentations of the heterogeneous graph and further design a cross-attention\nmodule to fuse the representations of both types of nodes for query-POI\nrelevance scoring. Extensive experiments conducted on large-scale real-world\ndatasets from Baidu Maps demonstrate the superiority and effectiveness of\nHGAMN. In addition, HGAMN has already been deployed in production at Baidu\nMaps, and it successfully keeps serving hundreds of millions of requests every\nday.\n","authors":["Jizhou Huang","Haifeng Wang","Yibo Sun","Miao Fan","Zhengjie Huang","Chunyuan Yuan","Yawen Li"],"pdf_url":"https://arxiv.org/pdf/2409.03504v1.pdf","comment":"Accepted by KDD'21"},{"id":"http://arxiv.org/abs/2409.03449v1","updated":"2024-09-05T11:56:40Z","published":"2024-09-05T11:56:40Z","title":"MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's\n  Sponsored Search","summary":"  Baidu runs the largest commercial web search engine in China, serving\nhundreds of millions of online users every day in response to a great variety\nof queries. In order to build a high-efficiency sponsored search engine, we\nused to adopt a three-layer funnel-shaped structure to screen and sort hundreds\nof ads from billions of ad candidates subject to the requirement of low\nresponse latency and the restraints of computing resources. Given a user query,\nthe top matching layer is responsible for providing semantically relevant ad\ncandidates to the next layer, while the ranking layer at the bottom concerns\nmore about business indicators (e.g., CPM, ROI, etc.) of those ads. The clear\nseparation between the matching and ranking objectives results in a lower\ncommercial return. The Mobius project has been established to address this\nserious issue. It is our first attempt to train the matching layer to consider\nCPM as an additional optimization objective besides the query-ad relevance, via\ndirectly predicting CTR (click-through rate) from billions of query-ad pairs.\nSpecifically, this paper will elaborate on how we adopt active learning to\novercome the insufficiency of click history at the matching layer when training\nour neural click networks offline, and how we use the SOTA ANN search technique\nfor retrieving ads more efficiently (Here ``ANN'' stands for approximate\nnearest neighbor search). We contribute the solutions to Mobius-V1 as the first\nversion of our next generation query-ad matching system.\n","authors":["Miao Fan","Jiacheng Guo","Shuai Zhu","Shuo Miao","Mingming Sun","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2409.03449v1.pdf","comment":"Accepted by KDD'19"},{"id":"http://arxiv.org/abs/2409.02727v2","updated":"2024-09-05T07:17:59Z","published":"2024-09-04T14:01:48Z","title":"Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?","summary":"  The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.02727v2.pdf","comment":"https://github.com/yixuantt/PoolingAndAttn"},{"id":"http://arxiv.org/abs/2409.03294v1","updated":"2024-09-05T06:59:56Z","published":"2024-09-05T06:59:56Z","title":"Federated Prototype-based Contrastive Learning for Privacy-Preserving\n  Cross-domain Recommendation","summary":"  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR methods often assume the availability of user-item interaction\ndata across domains, overlooking user privacy concerns. Furthermore, these\nmethods suffer from performance degradation in scenarios with sparse\noverlapping users, as they typically depend on a large number of fully shared\nusers for effective knowledge transfer. To address these challenges, we propose\na Federated Prototype-based Contrastive Learning (CL) method for\nPrivacy-Preserving CDR, named FedPCL-CDR. This approach utilizes\nnon-overlapping user information and prototypes to improve multi-domain\nperformance while protecting user privacy. FedPCL-CDR comprises two modules:\nlocal domain (client) learning and global server aggregation. In the local\ndomain, FedPCL-CDR clusters all user data to learn representative prototypes,\neffectively utilizing non-overlapping user information and addressing the\nsparse overlapping user issue. It then facilitates knowledge transfer by\nemploying both local and global prototypes returned from the server in a CL\nmanner. Simultaneously, the global server aggregates representative prototypes\nfrom local domains to learn both local and global prototypes. The combination\nof prototypes and federated learning (FL) ensures that sensitive user data\nremains decentralized, with only prototypes being shared across domains,\nthereby protecting user privacy. Extensive experiments on four CDR tasks using\ntwo real-world datasets demonstrate that FedPCL-CDR outperforms the\nstate-of-the-art baselines.\n","authors":["Li Wang","Quangui Zhang","Lei Sang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2409.03294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03284v1","updated":"2024-09-05T06:49:14Z","published":"2024-09-05T06:49:14Z","title":"iText2KG: Incremental Knowledge Graphs Construction Using Large Language\n  Models","summary":"  Most available data is unstructured, making it challenging to access valuable\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\nstructuring data and making it accessible, allowing users to search for\ninformation effectively. KGs also facilitate insights, inference, and\nreasoning. Traditional NLP methods, such as named entity recognition and\nrelation extraction, are key in information retrieval but face limitations,\nincluding the use of predefined entity types and the need for supervised\nlearning. Current research leverages large language models' capabilities, such\nas zero- or few-shot learning. However, unresolved and semantically duplicated\nentities and relations still pose challenges, leading to inconsistent graphs\nand requiring extensive post-processing. Additionally, most approaches are\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\ntopic-independent KG construction without post-processing. This plug-and-play,\nzero-shot method is applicable across a wide range of KG construction scenarios\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\nmethod demonstrates superior performance compared to baseline methods across\nthree scenarios: converting scientific papers to graphs, websites to graphs,\nand CVs to graphs.\n","authors":["Yassir Lairgi","Ludovic Moncla","Rémy Cazabet","Khalid Benabdeslem","Pierre Cléau"],"pdf_url":"https://arxiv.org/pdf/2409.03284v1.pdf","comment":"Accepted at The International Web Information Systems Engineering\n  conference (the WISE conference) 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.03902v1","updated":"2024-09-05T20:22:01Z","published":"2024-09-05T20:22:01Z","title":"WaterMAS: Sharpness-Aware Maximization for Neural Network Watermarking","summary":"  Nowadays, deep neural networks are used for solving complex tasks in several\ncritical applications and protecting both their integrity and intellectual\nproperty rights (IPR) has become of utmost importance. To this end, we advance\nWaterMAS, a substitutive, white-box neural network watermarking method that\nimproves the trade-off among robustness, imperceptibility, and computational\ncomplexity, while making provisions for increased data payload and security.\nWasterMAS insertion keeps unchanged the watermarked weights while sharpening\ntheir underlying gradient space. The robustness is thus ensured by limiting the\nattack's strength: even small alterations of the watermarked weights would\nimpact the model's performance. The imperceptibility is ensured by inserting\nthe watermark during the training process. The relationship among the WaterMAS\ndata payload, imperceptibility, and robustness properties is discussed. The\nsecret key is represented by the positions of the weights conveying the\nwatermark, randomly chosen through multiple layers of the model. The security\nis evaluated by investigating the case in which an attacker would intercept the\nkey. The experimental validations consider 5 models and 2 tasks (VGG16,\nResNet18, MobileNetV3, SwinT for CIFAR10 image classification, and DeepLabV3\nfor Cityscapes image segmentation) as well as 4 types of attacks (Gaussian\nnoise addition, pruning, fine-tuning, and quantization). The code will be\nreleased open-source upon acceptance of the article.\n","authors":["Carl De Sousa Trias","Mihai Mitrea","Attilio Fiandrotti","Marco Cagnazzo","Sumanta Chaudhuri","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2409.03902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03844v1","updated":"2024-09-05T18:12:11Z","published":"2024-09-05T18:12:11Z","title":"MetaBGM: Dynamic Soundtrack Transformation For Continuous Multi-Scene\n  Experiences With Ambient Awareness And Personalization","summary":"  This paper introduces MetaBGM, a groundbreaking framework for generating\nbackground music that adapts to dynamic scenes and real-time user interactions.\nWe define multi-scene as variations in environmental contexts, such as\ntransitions in game settings or movie scenes. To tackle the challenge of\nconverting backend data into music description texts for audio generation\nmodels, MetaBGM employs a novel two-stage generation approach that transforms\ncontinuous scene and user state data into these texts, which are then fed into\nan audio generation model for real-time soundtrack creation. Experimental\nresults demonstrate that MetaBGM effectively generates contextually relevant\nand dynamic background music for interactive applications.\n","authors":["Haoxuan Liu","Zihao Wang","Haorong Hong","Youwei Feng","Jiaxin Yu","Han Diao","Yunfei Xu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03605v1","updated":"2024-09-05T15:11:40Z","published":"2024-09-05T15:11:40Z","title":"SegTalker: Segmentation-based Talking Face Generation with Mask-guided\n  Local Editing","summary":"  Audio-driven talking face generation aims to synthesize video with lip\nmovements synchronized to input audio. However, current generative techniques\nface challenges in preserving intricate regional textures (skin, teeth). To\naddress the aforementioned challenges, we propose a novel framework called\nSegTalker to decouple lip movements and image textures by introducing\nsegmentation as intermediate representation. Specifically, given the mask of\nimage employed by a parsing network, we first leverage the speech to drive the\nmask and generate talking segmentation. Then we disentangle semantic regions of\nimage into style codes using a mask-guided encoder. Ultimately, we inject the\npreviously generated talking segmentation and style codes into a mask-guided\nStyleGAN to synthesize video frame. In this way, most of textures are fully\npreserved. Moreover, our approach can inherently achieve background separation\nand facilitate mask-guided facial local editing. In particular, by editing the\nmask and swapping the region textures from a given reference image (e.g. hair,\nlip, eyebrows), our approach enables facial editing seamlessly when generating\ntalking face video. Experiments demonstrate that our proposed approach can\neffectively preserve texture details and generate temporally consistent video\nwhile remaining competitive in lip synchronization. Quantitative and\nqualitative results on the HDTF and MEAD datasets illustrate the superior\nperformance of our method over existing methods.\n","authors":["Lingyu Xiong","Xize Cheng","Jintao Tan","Xianjia Wu","Xiandong Li","Lei Zhu","Fei Ma","Minglei Li","Huang Xu","Zhihu Hu"],"pdf_url":"https://arxiv.org/pdf/2409.03605v1.pdf","comment":"10 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.03385v1","updated":"2024-09-05T09:44:43Z","published":"2024-09-05T09:44:43Z","title":"Make Graph-based Referring Expression Comprehension Great Again through\n  Expression-guided Dynamic Gating and Regression","summary":"  One common belief is that with complex models and pre-training on large-scale\ndatasets, transformer-based methods for referring expression comprehension\n(REC) perform much better than existing graph-based methods. We observe that\nsince most graph-based methods adopt an off-the-shelf detector to locate\ncandidate objects (i.e., regions detected by the object detector), they face\ntwo challenges that result in subpar performance: (1) the presence of\nsignificant noise caused by numerous irrelevant objects during reasoning, and\n(2) inaccurate localization outcomes attributed to the provided detector. To\naddress these issues, we introduce a plug-and-adapt module guided by\nsub-expressions, called dynamic gate constraint (DGC), which can adaptively\ndisable irrelevant proposals and their connections in graphs during reasoning.\nWe further introduce an expression-guided regression strategy (EGR) to refine\nlocation prediction. Extensive experimental results on the RefCOCO, RefCOCO+,\nRefCOCOg, Flickr30K, RefClef, and Ref-reasoning datasets demonstrate the\neffectiveness of the DGC module and the EGR strategy in consistently boosting\nthe performances of various graph-based REC methods. Without any pretaining,\nthe proposed graph-based method achieves better performance than the\nstate-of-the-art (SOTA) transformer-based methods.\n","authors":["Jingcheng Ke","Dele Wang","Jun-Cheng Chen","I-Hong Jhuo","Chia-Wen Lin","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2409.03385v1.pdf","comment":"12 pages to appear in IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2404.13993v4","updated":"2024-09-05T02:21:42Z","published":"2024-04-22T08:59:35Z","title":"Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion","summary":"  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n","authors":["Yingxuan Li","Ryota Hinami","Kiyoharu Aizawa","Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2404.13993v4.pdf","comment":"Accepted to ACM Multimedia 2024. Project page:\n  https://liyingxuan1012.github.io/zeroshot-speaker-prediction ; Github repo:\n  https://github.com/liyingxuan1012/zeroshot-speaker-prediction"}]},"2024-09-04T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.02864v1","updated":"2024-09-04T16:43:14Z","published":"2024-09-04T16:43:14Z","title":"Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant","summary":"  We present a prototype for a Bioinformatics Retrieval Augmentation Data\n(BRAD) digital assistant. BRAD integrates a suite of tools to handle a wide\nrange of bioinformatics tasks, from code execution to online search. We\ndemonstrate BRAD's capabilities through (1) improved question-and-answering\nwith retrieval augmented generation (RAG), (2) BRAD's ability to run and write\ncomplex software pipelines, and (3) BRAD's ability to organize and distribute\ntasks across individual and teams of agents. We use BRAD for automation of\nbioinformatics workflows, performing tasks ranging from gene enrichment and\nsearching the archive to automatic code generation and running biomarker\nidentification pipelines. BRAD is a step toward the ultimate goal to develop a\ndigital twin of laboratories driven by self-contained loops for hypothesis\ngeneration and testing of digital biology experiments.\n","authors":["Joshua Pickard","Marc Andrew Choi","Natalie Oliven","Cooper Stansbury","Jillian Cwycyshyn","Nicholas Galioto","Alex Gorodetsky","Alvaro Velasquez","Indika Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2409.02864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00847v2","updated":"2024-09-04T16:39:22Z","published":"2024-09-01T21:30:14Z","title":"The Design of an LLM-powered Unstructured Analytics System","summary":"  LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild.\n","authors":["Eric Anderson","Jonathan Fritz","Austin Lee","Bohou Li","Mark Lindblad","Henry Lindeman","Alex Meyer","Parth Parmar","Tanvi Ranade","Mehul A. Shah","Benjamin Sowell","Dan Tecuci","Vinayak Thapliyal","Matt Welsh"],"pdf_url":"https://arxiv.org/pdf/2409.00847v2.pdf","comment":"6 pages, 3 figures, fixed typos"},{"id":"http://arxiv.org/abs/2409.02856v1","updated":"2024-09-04T16:29:25Z","published":"2024-09-04T16:29:25Z","title":"Building a Scalable, Effective, and Steerable Search and Ranking\n  Platform","summary":"  Modern e-commerce platforms offer vast product selections, making it\ndifficult for customers to find items that they like and that are relevant to\ntheir current session intent. This is why it is key for e-commerce platforms to\nhave near real-time scalable and adaptable personalized ranking and search\nsystems. While numerous methods exist in the scientific literature for building\nsuch systems, many are unsuitable for large-scale industrial use due to\ncomplexity and performance limitations. Consequently, industrial ranking\nsystems often resort to computationally efficient yet simplistic retrieval or\ncandidate generation approaches, which overlook near real-time and\nheterogeneous customer signals, which results in a less personalized and\nrelevant experience. Moreover, related customer experiences are served by\ncompletely different systems, which increases complexity, maintenance, and\ninconsistent experiences.\n  In this paper, we present a personalized, adaptable near real-time ranking\nplatform that is reusable across various use cases, such as browsing and\nsearch, and that is able to cater to millions of items and customers under\nheavy load (thousands of requests per second). We employ transformer-based\nmodels through different ranking layers which can learn complex behavior\npatterns directly from customer action sequences while being able to\nincorporate temporal (e.g. in-session) and contextual information. We validate\nour system through a series of comprehensive offline and online real-world\nexperiments at a large online e-commerce platform, and we demonstrate its\nsuperiority when compared to existing systems, both in terms of customer\nexperience as well as in net revenue. Finally, we share the lessons learned\nfrom building a comprehensive, modern ranking platform for use in a large-scale\ne-commerce environment.\n","authors":["Marjan Celikik","Jacek Wasilewski","Ana Peleteiro Ramallo","Alexey Kurennoy","Evgeny Labzin","Danilo Ascione","Tural Gurbanov","Géraud Le Falher","Andrii Dzhoha","Ian Harris"],"pdf_url":"https://arxiv.org/pdf/2409.02856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03883v2","updated":"2024-09-04T14:00:42Z","published":"2023-02-08T05:12:54Z","title":"Multimodal Recommender Systems: A Survey","summary":"  The recommender system (RS) has been an integral toolkit of online services.\nThey are equipped with various deep learning techniques to model user\npreference based on identifier and attribute information. With the emergence of\nmultimedia services, such as short videos, news and etc., understanding these\ncontents while recommending becomes critical. Besides, multimodal features are\nalso helpful in alleviating the problem of data sparsity in RS. Thus,\nMultimodal Recommender System (MRS) has attracted much attention from both\nacademia and industry recently. In this paper, we will give a comprehensive\nsurvey of the MRS models, mainly from technical views. First, we conclude the\ngeneral procedures and major challenges for MRS. Then, we introduce the\nexisting MRS models according to four categories, i.e., Modality Encoder,\nFeature Interaction, Feature Enhancement and Model Optimization. Besides, to\nmake it convenient for those who want to research this field, we also summarize\nthe dataset and code resources. Finally, we discuss some promising future\ndirections of MRS and conclude this paper. To access more details of the\nsurveyed papers, such as implementation code, we open source a repository.\n","authors":["Qidong Liu","Jiaxi Hu","Yutian Xiao","Xiangyu Zhao","Jingtong Gao","Wanyu Wang","Qing Li","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2302.03883v2.pdf","comment":"accepted by CSUR"},{"id":"http://arxiv.org/abs/2409.00702v2","updated":"2024-09-04T13:19:42Z","published":"2024-09-01T12:11:48Z","title":"MARS: Matching Attribute-aware Representations for Text-based Sequential\n  Recommendation","summary":"  Sequential recommendation aims to predict the next item a user is likely to\nprefer based on their sequential interaction history. Recently, text-based\nsequential recommendation has emerged as a promising paradigm that uses\npre-trained language models to exploit textual item features to enhance\nperformance and facilitate knowledge transfer to unseen datasets. However,\nexisting text-based recommender models still struggle with two key challenges:\n(i) representing users and items with multiple attributes, and (ii) matching\nitems with complex user interests. To address these challenges, we propose a\nnovel model, Matching Attribute-aware Representations for Text-based Sequential\nRecommendation (MARS). MARS extracts detailed user and item representations\nthrough attribute-aware text encoding, capturing diverse user intents with\nmultiple attribute-aware representations. It then computes user-item scores via\nattribute-wise interaction matching, effectively capturing attribute-level user\npreferences. Our extensive experiments demonstrate that MARS significantly\noutperforms existing sequential models, achieving improvements of up to 24.43%\nand 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is\navailable at https://github.com/junieberry/MARS\n","authors":["Hyunsoo Kim","Junyoung Kim","Minjin Choi","Sunkyung Lee","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2409.00702v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2409.02685v1","updated":"2024-09-04T13:16:55Z","published":"2024-09-04T13:16:55Z","title":"RouterRetriever: Exploring the Benefits of Routing over Multiple Expert\n  Embedding Models","summary":"  Information retrieval methods often rely on a single embedding model trained\non large, general-domain datasets like MSMARCO. While this approach can produce\na retriever with reasonable overall performance, models trained on\ndomain-specific data often yield better results within their respective\ndomains. While prior work in information retrieval has tackled this through\nmulti-task training, the topic of combining multiple domain-specific expert\nretrievers remains unexplored, despite its popularity in language model\ngeneration. In this work, we introduce RouterRetriever, a retrieval model that\nleverages multiple domain-specific experts along with a routing mechanism to\nselect the most appropriate expert for each query. It is lightweight and allows\neasy addition or removal of experts without additional training. Evaluation on\nthe BEIR benchmark demonstrates that RouterRetriever outperforms both\nMSMARCO-trained (+2.1 absolute nDCG@10) and multi-task trained (+3.2) models.\nThis is achieved by employing our routing mechanism, which surpasses other\nrouting techniques (+1.8 on average) commonly used in language modeling.\nFurthermore, the benefit generalizes well to other datasets, even in the\nabsence of a specific expert on the dataset. To our knowledge, RouterRetriever\nis the first work to demonstrate the advantages of using multiple\ndomain-specific expert embedding models with effective routing over a single,\ngeneral-purpose embedding model in retrieval tasks.\n","authors":["Hyunji Lee","Luca Soldaini","Arman Cohan","Minjoon Seo","Kyle Lo"],"pdf_url":"https://arxiv.org/pdf/2409.02685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09979v2","updated":"2024-09-04T12:33:24Z","published":"2024-06-14T12:41:07Z","title":"HIRO: Hierarchical Information Retrieval Optimization","summary":"  Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.\n","authors":["Krish Goel","Mahek Chandak"],"pdf_url":"https://arxiv.org/pdf/2406.09979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07107v4","updated":"2024-09-04T11:39:56Z","published":"2023-08-14T12:47:22Z","title":"Large Language Models for Information Retrieval: A Survey","summary":"  As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.\n","authors":["Yutao Zhu","Huaying Yuan","Shuting Wang","Jiongnan Liu","Wenhan Liu","Chenlong Deng","Haonan Chen","Zheng Liu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.07107v4.pdf","comment":"updated to version 3"},{"id":"http://arxiv.org/abs/2409.01137v2","updated":"2024-09-04T10:58:57Z","published":"2024-09-02T10:19:31Z","title":"Smart E-commerce Recommendations with Semantic AI","summary":"  In e-commerce, web mining for page recommendations is widely used but often\nfails to meet user needs. To address this, we propose a novel solution\ncombining semantic web mining with BP neural networks. We process user search\nlogs to extract five key features: content priority, time spent, user feedback,\nrecommendation semantics, and input deviation. These features are then fed into\na BP neural network to classify and prioritize web pages. The prioritized pages\nare recommended to users. Using book sales pages for testing, our results\ndemonstrate that this solution can quickly and accurately identify the pages\nusers need. Our approach ensures that recommendations are more relevant and\ntailored to individual preferences, enhancing the online shopping experience.\nBy leveraging advanced semantic analysis and neural network techniques, we\nbridge the gap between user expectations and actual recommendations. This\ninnovative method not only improves accuracy but also speeds up the\nrecommendation process, making it a valuable tool for e-commerce platforms\naiming to boost user satisfaction and engagement. Additionally, our system\nability to handle large datasets and provide real-time recommendations makes it\na scalable and efficient solution for modern e-commerce challenges.\n","authors":["M. Badouch","M. Boutaounte"],"pdf_url":"https://arxiv.org/pdf/2409.01137v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2409.02599v1","updated":"2024-09-04T10:30:11Z","published":"2024-09-04T10:30:11Z","title":"A Fashion Item Recommendation Model in Hyperbolic Space","summary":"  In this work, we propose a fashion item recommendation model that\nincorporates hyperbolic geometry into user and item representations. Using\nhyperbolic space, our model aims to capture implicit hierarchies among items\nbased on their visual data and users' purchase history. During training, we\napply a multi-task learning framework that considers both hyperbolic and\nEuclidean distances in the loss function. Our experiments on three data sets\nshow that our model performs better than previous models trained in Euclidean\nspace only, confirming the effectiveness of our model. Our ablation studies\nshow that multi-task learning plays a key role, and removing the Euclidean loss\nsubstantially deteriorates the model performance.\n","authors":["Ryotaro Shimizu","Yu Wang","Masanari Kimura","Yuki Hirakawa","Takashi Wada","Yuki Saito","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.02599v1.pdf","comment":"This work was presented at the CVFAD Workshop at CVPR 2024"},{"id":"http://arxiv.org/abs/2409.02580v1","updated":"2024-09-04T10:03:09Z","published":"2024-09-04T10:03:09Z","title":"AlignGroup: Learning and Aligning Group Consensus with Member\n  Preferences for Group Recommendation","summary":"  Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines.\n","authors":["Jinfeng Xu","Zheyu Chen","Jinze Li","Shuo Yang","Hewei Wang","Edith C. -H. Ngai"],"pdf_url":"https://arxiv.org/pdf/2409.02580v1.pdf","comment":"10 pages, accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2409.02571v1","updated":"2024-09-04T09:41:52Z","published":"2024-09-04T09:41:52Z","title":"iRangeGraph: Improvising Range-dedicated Graphs for Range-filtering\n  Nearest Neighbor Search","summary":"  Range-filtering approximate nearest neighbor (RFANN) search is attracting\nincreasing attention in academia and industry. Given a set of data objects,\neach being a pair of a high-dimensional vector and a numeric value, an RFANN\nquery with a vector and a numeric range as parameters returns the data object\nwhose numeric value is in the query range and whose vector is nearest to the\nquery vector. To process this query, a recent study proposes to build $O(n^2)$\ndedicated graph-based indexes for all possible query ranges to enable efficient\nprocessing on a database of $n$ objects. As storing all these indexes is\nprohibitively expensive, the study constructs compressed indexes instead, which\nreduces the memory consumption considerably. However, this incurs suboptimal\nperformance because the compression is lossy. In this study, instead of\nmaterializing a compressed index for every possible query range in preparation\nfor querying, we materialize graph-based indexes, called elemental graphs, for\na moderate number of ranges. We then provide an effective and efficient\nalgorithm that during querying can construct an index for any query range using\nthe elemental graphs. We prove that the time needed to construct such an index\nis low. We also cover an experimental study on real-world datasets that\nprovides evidence that the materialized elemental graphs only consume moderate\nspace and that the proposed method is capable of superior and stable query\nperformance across different query workloads.\n","authors":["Yuexuan Xu","Jianyang Gao","Yutong Gou","Cheng Long","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2409.02571v1.pdf","comment":"The paper has been accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2404.06900v3","updated":"2024-09-04T06:55:21Z","published":"2024-04-10T10:45:30Z","title":"NFARec: A Negative Feedback-Aware Recommender Model","summary":"  Graph neural network (GNN)-based models have been extensively studied for\nrecommendations, as they can extract high-order collaborative signals\naccurately which is required for high-quality recommender systems. However,\nthey neglect the valuable information gained through negative feedback in two\naspects: (1) different users might hold opposite feedback on the same item,\nwhich hampers optimal information propagation in GNNs, and (2) even when an\nitem vastly deviates from users' preferences, they might still choose it and\nprovide a negative rating. In this paper, we propose a negative feedback-aware\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\ntransfer information to multi-hop neighbors along an optimal path effectively,\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\n(HGCs) to learn users' structural representations. Moreover, NFARec\nincorporates an auxiliary task - predicting the feedback sentiment polarity\n(i.e., positive or negative) of the next interaction - based on the Transformer\nHawkes Process. The task is beneficial for understanding users by learning the\nsentiment expressed in their previous sequential feedback patterns and\npredicting future interactions. Extensive experiments demonstrate that NFARec\noutperforms competitive baselines. Our source code and data are released at\nhttps://github.com/WangXFng/NFARec.\n","authors":["Xinfeng Wang","Fumiyo Fukumoto","Jin Cui","Yoshimi Suzuki","Dongjin Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06900v3.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2404.06895v3","updated":"2024-09-04T06:51:55Z","published":"2024-04-10T10:38:24Z","title":"CaDRec: Contextualized and Debiased Recommender Model","summary":"  Recommender models aimed at mining users' behavioral patterns have raised\ngreat attention as one of the essential applications in daily life. Recent work\non graph neural networks (GNNs) or debiasing methods has attained remarkable\ngains. However, they still suffer from (1) over-smoothing node embeddings\ncaused by recursive convolutions with GNNs, and (2) the skewed distribution of\ninteractions due to popularity and user-individual biases. This paper proposes\na contextualized and debiased recommender model (CaDRec). To overcome the\nover-smoothing issue, we explore a novel hypergraph convolution operator that\ncan select effective neighbors during convolution by introducing both\nstructural context and sequential context. To tackle the skewed distribution,\nwe propose two strategies for disentangling interactions: (1) modeling\nindividual biases to learn unbiased item embeddings, and (2) incorporating item\npopularity with positional encoding. Moreover, we mathematically show that the\nimbalance of the gradients to update item embeddings exacerbates the popularity\nbias, thus adopting regularization and weighting schemes as solutions.\nExtensive experiments on four datasets demonstrate the superiority of the\nCaDRec against state-of-the-art (SOTA) methods. Our source code and data are\nreleased at https://github.com/WangXFng/CaDRec.\n","authors":["Xinfeng Wang","Fumiyo Fukumoto","Jin Cui","Yoshimi Suzuki","Jiyi Li","Dongjin Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06895v3.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2408.15796v2","updated":"2024-09-04T06:36:22Z","published":"2024-08-28T13:42:28Z","title":"Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models","summary":"  This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.\n","authors":["Hédi Zeghidi","Ludovic Moncla"],"pdf_url":"https://arxiv.org/pdf/2408.15796v2.pdf","comment":"Github repo: https://github.com/GEODE-project/ner-llm"},{"id":"http://arxiv.org/abs/2409.02455v1","updated":"2024-09-04T05:36:00Z","published":"2024-09-04T05:36:00Z","title":"An Effective Tag Assignment Approach for Billboard Advertisement","summary":"  Billboard Advertisement has gained popularity due to its significant outrage\nin return on investment. To make this advertisement approach more effective,\nthe relevant information about the product needs to be reached to the relevant\nset of people. This can be achieved if the relevant set of tags can be mapped\nto the correct slots. Formally, we call this problem the Tag Assignment Problem\nin Billboard Advertisement. Given trajectory, billboard database, and a set of\nselected billboard slots and tags, this problem asks to output a mapping of\nselected tags to the selected slots so that the influence is maximized. We\nmodel this as a variant of traditional bipartite matching called One-To-Many\nBipartite Matching (OMBM). Unlike traditional bipartite matching, a tag can be\nassigned to only one slot; in the OMBM, a tag can be assigned to multiple slots\nwhile the vice versa can not happen. We propose an iterative solution approach\nthat incrementally allocates the tags to the slots. The proposed methodology\nhas been explained with an illustrated example. A complexity analysis of the\nproposed solution approach has also been conducted. The experimental results on\nreal-world trajectory and billboard datasets prove our claim on the\neffectiveness and efficiency of the proposed solution.\n","authors":["Dildar Ali","Harishchandra Kumar","Suman Banerjee","Yamuna Prasad"],"pdf_url":"https://arxiv.org/pdf/2409.02455v1.pdf","comment":"This Paper has been accepted at The 25th International Web\n  Information Systems Engineering Conference (WISE-2024)"},{"id":"http://arxiv.org/abs/2408.16672v3","updated":"2024-09-04T05:09:00Z","published":"2024-08-29T16:21:00Z","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","summary":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis paper, we introduce a novel architecture and a training framework to\nsupport long context window and multilingual retrieval. Our new model,\nJina-ColBERT-v2, demonstrates strong performance across a range of English and\nmultilingual retrieval tasks,\n","authors":["Rohan Jha","Bo Wang","Michael Günther","Georgios Mastrapas","Saba Sturua","Isabelle Mohr","Andreas Koukounas","Mohammad Kalim Akram","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.16672v3.pdf","comment":"8 pages, references at pp7,8; EMNLP workshop submission"},{"id":"http://arxiv.org/abs/2409.02425v1","updated":"2024-09-04T04:12:22Z","published":"2024-09-04T04:12:22Z","title":"Deep Adaptive Interest Network: Personalized Recommendation with\n  Context-Aware Learning","summary":"  In personalized recommendation systems, accurately capturing users' evolving\ninterests and combining them with contextual information is a critical research\narea. This paper proposes a novel model called the Deep Adaptive Interest\nNetwork (DAIN), which dynamically models users' interests while incorporating\ncontext-aware learning mechanisms to achieve precise and adaptive personalized\nrecommendations. DAIN leverages deep learning techniques to build an adaptive\ninterest network structure that can capture users' interest changes in\nreal-time while further optimizing recommendation results by integrating\ncontextual information. Experiments conducted on several public datasets\ndemonstrate that DAIN excels in both recommendation performance and\ncomputational efficiency. This research not only provides a new solution for\npersonalized recommendation systems but also offers fresh insights into the\napplication of context-aware learning in recommendation systems.\n","authors":["Shuaishuai Huang","Haowei Yang","You Yao","Xueting Lin","Yuming Tu"],"pdf_url":"https://arxiv.org/pdf/2409.02425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02965v1","updated":"2024-09-04T02:17:32Z","published":"2024-09-04T02:17:32Z","title":"Do We Trust What They Say or What They Do? A Multimodal User Embedding\n  Provides Personalized Explanations","summary":"  With the rapid development of social media, the importance of analyzing\nsocial network user data has also been put on the agenda. User representation\nlearning in social media is a critical area of research, based on which we can\nconduct personalized content delivery, or detect malicious actors. Being more\ncomplicated than many other types of data, social network user data has\ninherent multimodal nature. Various multimodal approaches have been proposed to\nharness both text (i.e. post content) and relation (i.e. inter-user\ninteraction) information to learn user embeddings of higher quality. The advent\nof Graph Neural Network models enables more end-to-end integration of user text\nembeddings and user interaction graphs in social networks. However, most of\nthose approaches do not adequately elucidate which aspects of the data - text\nor graph structure information - are more helpful for predicting each specific\nuser under a particular task, putting some burden on personalized downstream\nanalysis and untrustworthy information filtering. We propose a simple yet\neffective framework called Contribution-Aware Multimodal User Embedding (CAMUE)\nfor social networks. We have demonstrated with empirical evidence, that our\napproach can provide personalized explainable predictions, automatically\nmitigating the impact of unreliable information. We also conducted case studies\nto show how reasonable our results are. We observe that for most users, graph\nstructure information is more trustworthy than text information, but there are\nsome reasonable cases where text helps more. Our work paves the way for more\nexplainable, reliable, and effective social media user embedding which allows\nfor better personalized content delivery.\n","authors":["Zhicheng Ren","Zhiping Xiao","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02343v1","updated":"2024-09-04T00:10:36Z","published":"2024-09-04T00:10:36Z","title":"NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for\n  Retrieval","summary":"  $k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)\nfrom pre-trained embedding models is the predominant retrieval method for text\nand images, as well as Retrieval-Augmented Generation (RAG) pipelines. In\npractice, application developers often fine-tune the embeddings to improve\ntheir accuracy on the dataset and query workload in hand. Existing approaches\neither fine-tune the pre-trained model itself or, more efficiently, but at the\ncost of accuracy, train adaptor models to transform the output of the\npre-trained model. We present NUDGE, a family of novel non-parametric embedding\nfine-tuning approaches that are significantly more accurate and efficient than\nboth sets of existing approaches. NUDGE directly modifies the embeddings of\ndata records to maximize the accuracy of $k$-NN retrieval. We present a\nthorough theoretical and experimental study of NUDGE's non-parametric approach.\nWe show that even though the underlying problem is NP-Hard, constrained\nvariations can be solved efficiently. These constraints additionally ensure\nthat the changes to the embeddings are modest, avoiding large distortions to\nthe semantics learned during pre-training. In experiments across five\npre-trained models and nine standard text and image retrieval datasets, NUDGE\nruns in minutes and often improves NDCG@10 by more than 10% over existing\nfine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase\nin accuracy and runs 200x and 3x faster, respectively, over fine-tuning the\npre-trained model and training adaptors.\n","authors":["Sepanta Zeighami","Zac Wellmer","Aditya Parameswaran"],"pdf_url":"https://arxiv.org/pdf/2409.02343v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.02889v1","updated":"2024-09-04T17:25:21Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v1.pdf","comment":"19 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2409.02845v1","updated":"2024-09-04T16:17:41Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02828v1","updated":"2024-09-04T15:50:16Z","published":"2024-09-04T15:50:16Z","title":"ExpLLM: Towards Chain of Thought for Facial Expression Recognition","summary":"  Facial expression recognition (FER) is a critical task in multimedia with\nsignificant implications across various domains. However, analyzing the causes\nof facial expressions is essential for accurately recognizing them. Current\napproaches, such as those based on facial action units (AUs), typically provide\nAU names and intensities but lack insight into the interactions and\nrelationships between AUs and the overall expression. In this paper, we propose\na novel method called ExpLLM, which leverages large language models to generate\nan accurate chain of thought (CoT) for facial expression recognition.\nSpecifically, we have designed the CoT mechanism from three key perspectives:\nkey observations, overall emotional interpretation, and conclusion. The key\nobservations describe the AU's name, intensity, and associated emotions. The\noverall emotional interpretation provides an analysis based on multiple AUs and\ntheir interactions, identifying the dominant emotions and their relationships.\nFinally, the conclusion presents the final expression label derived from the\npreceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designed\nto construct this expression CoT and generate instruction-description data for\ntraining our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasets\ndemonstrate that ExpLLM outperforms current state-of-the-art FER methods.\nExpLLM also surpasses the latest GPT-4o in expression CoT generation,\nparticularly in recognizing micro-expressions where GPT-4o frequently fails.\n","authors":["Xing Lan","Jian Xue","Ji Qi","Dongmei Jiang","Ke Lu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.02828v1.pdf","comment":"project page: https://starhiking.github.io/ExpLLM_Page/"},{"id":"http://arxiv.org/abs/2409.02657v1","updated":"2024-09-04T12:30:25Z","published":"2024-09-04T12:30:25Z","title":"PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for\n  One-Shot Talking Head Generation","summary":"  While previous audio-driven talking head generation (THG) methods generate\nhead poses from driving audio, the generated poses or lips cannot match the\naudio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a\nTHG system that can freely generate lip-synchronized talking head videos with\nfree head poses conditioned on text prompts and audio. The core insight of our\nmethod is using head pose to connect visual, linguistic, and audio signals.\nFirst, we propose to generate poses from both audio and text prompts, where the\naudio offers short-term variations and rhythm correspondence of the head\nmovements and the text prompts describe the long-term semantics of head\nmotions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to\ngenerate motion latent from text prompts and audio cues in a pose latent space.\nSecond, we observe a loss-imbalance problem: the loss for the lip region\ncontributes less than 4\\% of the total reconstruction loss caused by both pose\nand lip, making optimization lean towards head movements rather than lip\nshapes. To address this issue, we propose a refinement-based learning strategy\nto synthesize natural talking videos using two cascaded networks, i.e.,\nCoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce\nanimated images in novel poses and the RefineNet focuses on learning finer lip\nmotions by progressively estimating lip motions from low-to-high resolutions,\nyielding improved lip-synchronization performance. Experiments demonstrate our\npose prediction strategy achieves better pose diversity and realness compared\nto text-only or audio-only, and our video generator model outperforms\nstate-of-the-art methods in synthesizing talking videos with natural head\nmotions. Project: https://junleen.github.io/projects/posetalk.\n","authors":["Jun Ling","Yiwen Wang","Han Xue","Rong Xie","Li Song"],"pdf_url":"https://arxiv.org/pdf/2409.02657v1.pdf","comment":"7+5 pages, 15 figures"},{"id":"http://arxiv.org/abs/2409.02555v1","updated":"2024-09-04T09:21:13Z","published":"2024-09-04T09:21:13Z","title":"Low-Resolution Object Recognition with Cross-Resolution Relational\n  Contrastive Distillation","summary":"  Recognizing objects in low-resolution images is a challenging task due to the\nlack of informative details. Recent studies have shown that knowledge\ndistillation approaches can effectively transfer knowledge from a\nhigh-resolution teacher model to a low-resolution student model by aligning\ncross-resolution representations. However, these approaches still face\nlimitations in adapting to the situation where the recognized objects exhibit\nsignificant representation discrepancies between training and testing images.\nIn this study, we propose a cross-resolution relational contrastive\ndistillation approach to facilitate low-resolution object recognition. Our\napproach enables the student model to mimic the behavior of a well-trained\nteacher model which delivers high accuracy in identifying high-resolution\nobjects. To extract sufficient knowledge, the student learning is supervised\nwith contrastive relational distillation loss, which preserves the similarities\nin various relational structures in contrastive representation space. In this\nmanner, the capability of recovering missing details of familiar low-resolution\nobjects can be effectively enhanced, leading to a better knowledge transfer.\nExtensive experiments on low-resolution object classification and\nlow-resolution face recognition clearly demonstrate the effectiveness and\nadaptability of our approach.\n","authors":["Kangkai Zhang","Shiming Ge","Ruixin Shi","Dan Zeng"],"pdf_url":"https://arxiv.org/pdf/2409.02555v1.pdf","comment":"This paper is accepted by IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2408.15461v2","updated":"2024-09-04T02:45:56Z","published":"2024-08-28T00:54:51Z","title":"Hand1000: Generating Realistic Hands from Text with Only 1,000 Images","summary":"  Text-to-image generation models have achieved remarkable advancements in\nrecent years, aiming to produce realistic images from textual descriptions.\nHowever, these models often struggle with generating anatomically accurate\nrepresentations of human hands. The resulting images frequently exhibit issues\nsuch as incorrect numbers of fingers, unnatural twisting or interlacing of\nfingers, or blurred and indistinct hands. These issues stem from the inherent\ncomplexity of hand structures and the difficulty in aligning textual\ndescriptions with precise visual depictions of hands. To address these\nchallenges, we propose a novel approach named Hand1000 that enables the\ngeneration of realistic hand images with target gesture using only 1,000\ntraining samples. The training of Hand1000 is divided into three stages with\nthe first stage aiming to enhance the model's understanding of hand anatomy by\nusing a pre-trained hand gesture recognition model to extract gesture\nrepresentation. The second stage further optimizes text embedding by\nincorporating the extracted hand gesture representation, to improve alignment\nbetween the textual descriptions and the generated hand images. The third stage\nutilizes the optimized embedding to fine-tune the Stable Diffusion model to\ngenerate realistic hand images. In addition, we construct the first publicly\navailable dataset specifically designed for text-to-hand image generation.\nBased on the existing hand gesture recognition dataset, we adopt advanced image\ncaptioning models and LLaMA3 to generate high-quality textual descriptions\nenriched with detailed gesture information. Extensive experiments demonstrate\nthat Hand1000 significantly outperforms existing models in producing\nanatomically correct hand images while faithfully representing other details in\nthe text, such as faces, clothing, and colors.\n","authors":["Haozhuo Zhang","Bin Zhu","Yu Cao","Yanbin Hao"],"pdf_url":"https://arxiv.org/pdf/2408.15461v2.pdf","comment":"Project page https://haozhuo-zhang.github.io/Hand1000-project-page/"},{"id":"http://arxiv.org/abs/2409.02376v1","updated":"2024-09-04T01:54:20Z","published":"2024-09-04T01:54:20Z","title":"Coral Model Generation from Single Images for Virtual Reality\n  Applications","summary":"  With the rapid development of VR technology, the demand for high-quality 3D\nmodels is increasing. Traditional methods struggle with efficiency and quality\nin large-scale customization. This paper introduces a deep-learning framework\nthat generates high-precision 3D coral models from a single image. Using the\nCoral dataset, the framework extracts geometric and texture features, performs\n3D reconstruction, and optimizes design and material blending. Advanced\noptimization and polygon count control ensure shape accuracy, detail retention,\nand flexible output for various complexities, catering to high-quality\nrendering and real-time interaction needs.The project incorporates Explainable\nAI (XAI) to transform AI-generated models into interactive \"artworks,\" best\nviewed in VR and XR. This enhances model interpretability and human-machine\ncollaboration. Real-time feedback in VR interactions displays information like\ncoral species and habitat, enriching user experience. The generated models\nsurpass traditional methods in detail, visual quality, and efficiency. This\nresearch offers an intelligent approach to 3D content creation for VR, lowering\nproduction barriers, and promoting widespread VR applications. Additionally,\nintegrating XAI provides new insights into AI-generated visual content and\nadvances research in 3D vision interpretability.\n","authors":["Jie Fu","Shun Fu","Mick Grierson"],"pdf_url":"https://arxiv.org/pdf/2409.02376v1.pdf","comment":"In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts\n  2024) arXiv:2406.14485"},{"id":"http://arxiv.org/abs/2408.11593v3","updated":"2024-09-04T01:25:55Z","published":"2024-08-21T12:59:42Z","title":"MCDubber: Multimodal Context-Aware Expressive Video Dubbing","summary":"  Automatic Video Dubbing (AVD) aims to take the given script and generate\nspeech that aligns with lip motion and prosody expressiveness. Current AVD\nmodels mainly utilize visual information of the current sentence to enhance the\nprosody of synthesized speech. However, it is crucial to consider whether the\nprosody of the generated dubbing aligns with the multimodal context, as the\ndubbing will be combined with the original context in the final video. This\naspect has been overlooked in previous studies. To address this issue, we\npropose a Multimodal Context-aware video Dubbing model, termed\n\\textbf{MCDubber}, to convert the modeling object from a single sentence to a\nlonger sequence with context information to ensure the consistency of the\nglobal context prosody. MCDubber comprises three main components: (1) A context\nduration aligner aims to learn the context-aware alignment between the text and\nlip frames; (2) A context prosody predictor seeks to read the global context\nvisual sequence and predict the context-aware global energy and pitch; (3) A\ncontext acoustic decoder ultimately predicts the global context mel-spectrogram\nwith the assistance of adjacent ground-truth mel-spectrograms of the target\nsentence. Through this process, MCDubber fully considers the influence of\nmultimodal context on the prosody expressiveness of the current sentence when\ndubbing. The extracted mel-spectrogram belonging to the target sentence from\nthe output context mel-spectrograms is the final required dubbing audio.\nExtensive experiments on the Chem benchmark dataset demonstrate that our\nMCDubber significantly improves dubbing expressiveness compared to all advanced\nbaselines. The code and demos are available at\nhttps://github.com/XiaoYuanJun-zy/MCDubber.\n","authors":["Yuan Zhao","Zhenqi Jia","Rui Liu","De Hu","Feilong Bao","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2408.11593v3.pdf","comment":"Accepted by NCMMSC2024"}]},"2024-09-03T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.17344v2","updated":"2024-09-03T10:50:17Z","published":"2024-08-30T15:16:52Z","title":"rerankers: A Lightweight Python Library to Unify Ranking Methods","summary":"  This paper presents rerankers, a Python library which provides an easy-to-use\ninterface to the most commonly used re-ranking approaches. Re-ranking is an\nintegral component of many retrieval pipelines; however, there exist numerous\napproaches to it, relying on different implementation methods. rerankers\nunifies these methods into a single user-friendly interface, allowing\npractitioners and researchers alike to explore different methods while only\nchanging a single line of Python code. Moreover ,rerankers ensures that its\nimplementations are done with the fewest dependencies possible, and re-uses the\noriginal implementation whenever possible, guaranteeing that our simplified\ninterface results in no performance degradation compared to more complex ones.\nThe full source code and list of supported models are updated regularly and\navailable at https://github.com/answerdotai/rerankers.\n","authors":["Benjamin Clavié"],"pdf_url":"https://arxiv.org/pdf/2408.17344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01736v1","updated":"2024-09-03T09:25:26Z","published":"2024-09-03T09:25:26Z","title":"SpannerLib: Embedding Declarative Information Extraction in an\n  Imperative Workflow","summary":"  Document spanners have been proposed as a formal framework for declarative\nInformation Extraction (IE) from text, following IE products from the industry\nand academia. Over the past decade, the framework has been studied thoroughly\nin terms of expressive power, complexity, and the ability to naturally combine\ntext analysis with relational querying. This demonstration presents SpannerLib\na library for embedding document spanners in Python code. SpannerLib\nfacilitates the development of IE programs by providing an implementation of\nSpannerlog (Datalog-based documentspanners) that interacts with the Python code\nin two directions: rules can be embedded inside Python, and they can invoke\ncustom Python code (e.g., calls to ML-based NLP models) via user-defined\nfunctions. The demonstration scenarios showcase IE programs, with increasing\nlevels of complexity, within Jupyter Notebook.\n","authors":["Dean Light","Ahmad Aiashy","Mahmoud Diab","Daniel Nachmias","Stijn Vansummeren","Benny Kimelfeld"],"pdf_url":"https://arxiv.org/pdf/2409.01736v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.01605v1","updated":"2024-09-03T04:55:03Z","published":"2024-09-03T04:55:03Z","title":"Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation\n  with Collaborative Information","summary":"  Sequential recommender systems are essential for discerning user preferences\nfrom historical interactions and facilitating targeted recommendations. Recent\ninnovations employing Large Language Models (LLMs) have advanced the field by\nencoding item semantics, yet they often necessitate substantial parameter\ntuning and are resource-demanding. Moreover, these works fails to consider the\ndiverse characteristics of different types of users and thus diminishes the\nrecommendation accuracy. In this paper, we propose a parameter-efficient Large\nLanguage Model Bi-Tuning framework for sequential recommendation with\ncollaborative information (Laser). Specifically, Bi-Tuning works by inserting\ntrainable virtual tokens at both the prefix and suffix of the input sequence\nand freezing the LLM parameters, thus optimizing the LLM for the sequential\nrecommendation. In our Laser, the prefix is utilized to incorporate user-item\ncollaborative information and adapt the LLM to the recommendation task, while\nthe suffix converts the output embeddings of the LLM from the language space to\nthe recommendation space for the follow-up item recommendation. Furthermore, to\ncapture the characteristics of different types of users when integrating the\ncollaborative information via the prefix, we introduce M-Former, a lightweight\nMoE-based querying transformer that uses a set of query experts to integrate\ndiverse user-specific collaborative information encoded by frozen ID-based\nsequential recommender systems, significantly improving the accuracy of\nrecommendations. Extensive experiments on real-world datasets demonstrate that\nLaser can parameter-efficiently adapt LLMs to effective recommender systems,\nsignificantly outperforming state-of-the-art methods.\n","authors":["Xinyu Zhang","Linmei Hu","Luhao Zhang","Dandan Song","Heyan Huang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2409.01605v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.01563v1","updated":"2024-09-03T03:00:59Z","published":"2024-09-03T03:00:59Z","title":"Blockchain-based Federated Recommendation with Incentive Mechanism","summary":"  Nowadays, federated recommendation technology is rapidly evolving to help\nmultiple organisations share data and train models while meeting user privacy,\ndata security and government regulatory requirements. However, federated\nrecommendation increases customer system costs such as power, computational and\ncommunication resources. Besides, federated recommendation systems are also\nsusceptible to model attacks and data poisoning by participating malicious\nclients. Therefore, most customers are unwilling to participate in federated\nrecommendation without any incentive. To address these problems, we propose a\nblockchain-based federated recommendation system with incentive mechanism to\npromote more trustworthy, secure, and efficient federated recommendation\nservice. First, we construct a federated recommendation system based on NeuMF\nand FedAvg. Then we introduce a reverse auction mechanism to select optimal\nclients that can maximize the social surplus. Finally, we employ blockchain for\non-chain evidence storage of models to ensure the safety of the federated\nrecommendation system. The experimental results show that our proposed\nincentive mechanism can attract clients with superior training data to engage\nin the federal recommendation at a lower cost, which can increase the economic\nbenefit of federal recommendation by 54.9\\% while improve the recommendation\nperformance. Thus our work provides theoretical and technological support for\nthe construction of a harmonious and healthy ecological environment for the\napplication of federal recommendation.\n","authors":["Jianhai Chen","Yanlin Wu","Dazhong Rong","Guoyao Yu","Lingqi Jiang","Zhenguang Liu","Peng Zhou","Rui Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01563v1.pdf","comment":"This paper has been accepted on 2024 Blockchain and Web3 Technology\n  Innovation and Application Exchange Conference (BWTAC 2024)"},{"id":"http://arxiv.org/abs/2405.06242v2","updated":"2024-09-03T00:21:23Z","published":"2024-05-10T04:44:34Z","title":"Impedance vs. Power Side-channel Vulnerabilities: A Comparative Study","summary":"  In recent times, impedance side-channel analysis has emerged as a potent\nstrategy for adversaries seeking to extract sensitive information from\ncomputing systems. It leverages variations in the intrinsic impedance of a\nchip's internal structure across different logic states. In this study, we\nconduct a comparative analysis between the newly explored impedance side\nchannel and the well-established power side channel. Through experimental\nevaluation, we investigate the efficacy of these two side channels in\nextracting the cryptographic key from the Advanced Encryption Standard (AES)\nand analyze their performance. Our results indicate that impedance analysis\ndemonstrates a higher potential for cryptographic key extraction compared to\npower side-channel analysis. Moreover, we identify scenarios where power\nside-channel analysis does not yield satisfactory results, whereas impedance\nanalysis proves to be more robust and effective. This work not only underscores\nthe significance of impedance side-channel analysis in enhancing cryptographic\nsecurity but also emphasizes the necessity for a deeper understanding of its\nmechanisms and implications.\n","authors":["Md Sadik Awal","Buddhipriya Gayanath","Md Tauhidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2405.06242v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.01690v2","updated":"2024-09-03T22:30:34Z","published":"2024-08-03T07:05:40Z","title":"IDNet: A Novel Dataset for Identity Document Analysis and Fraud\n  Detection","summary":"  Effective fraud detection and analysis of government-issued identity\ndocuments, such as passports, driver's licenses, and identity cards, are\nessential in thwarting identity theft and bolstering security on online\nplatforms. The training of accurate fraud detection and analysis tools depends\non the availability of extensive identity document datasets. However, current\npublicly available benchmark datasets for identity document analysis, including\nMIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a\nlimited number of samples, cover insufficient varieties of fraud patterns, and\nseldom include alterations in critical personal identifying fields like\nportrait images, limiting their utility in training models capable of detecting\nrealistic frauds while preserving privacy.\n  In response to these shortcomings, our research introduces a new benchmark\ndataset, IDNet, designed to advance privacy-preserving fraud detection efforts.\nThe IDNet dataset comprises 837,060 images of synthetically generated identity\ndocuments, totaling approximately 490 gigabytes, categorized into 20 types from\n$10$ U.S. states and 10 European countries. We evaluate the utility and present\nuse cases of the dataset, illustrating how it can aid in training\nprivacy-preserving fraud detection methods, facilitating the generation of\ncamera and video capturing of identity documents, and testing schema\nunification and other identity document management functionalities.\n","authors":["Hong Guan","Yancheng Wang","Lulu Xie","Soham Nag","Rajeev Goel","Niranjan Erappa Narayana Swamy","Yingzhen Yang","Chaowei Xiao","Jonathan Prisby","Ross Maciejewski","Jia Zou"],"pdf_url":"https://arxiv.org/pdf/2408.01690v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2409.02266v1","updated":"2024-09-03T19:52:49Z","published":"2024-09-03T19:52:49Z","title":"LSTMSE-Net: Long Short Term Speech Enhancement Network for Audio-visual\n  Speech Enhancement","summary":"  In this paper, we propose long short term memory speech enhancement network\n(LSTMSE-Net), an audio-visual speech enhancement (AVSE) method. This innovative\nmethod leverages the complementary nature of visual and audio information to\nboost the quality of speech signals. Visual features are extracted with\nVisualFeatNet (VFN), and audio features are processed through an encoder and\ndecoder. The system scales and concatenates visual and audio features, then\nprocesses them through a separator network for optimized speech enhancement.\nThe architecture highlights advancements in leveraging multi-modal data and\ninterpolation techniques for robust AVSE challenge systems. The performance of\nLSTMSE-Net surpasses that of the baseline model from the COG-MHEAR AVSE\nChallenge 2024 by a margin of 0.06 in scale-invariant signal-to-distortion\nratio (SISDR), $0.03$ in short-time objective intelligibility (STOI), and\n$1.32$ in perceptual evaluation of speech quality (PESQ). The source code of\nthe proposed LSTMSE-Net is available at\n\\url{https://github.com/mtanveer1/AVSEC-3-Challenge}.\n","authors":["Arnav Jain","Jasmer Singh Sanjotra","Harshvardhan Choudhary","Krish Agrawal","Rupal Shah","Rohan Jha","M. Sajid","Amir Hussain","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2409.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02108v1","updated":"2024-09-03T17:59:05Z","published":"2024-09-03T17:59:05Z","title":"Unveiling Deep Shadows: A Survey on Image and Video Shadow Detection,\n  Removal, and Generation in the Era of Deep Learning","summary":"  Shadows are formed when light encounters obstacles, leading to areas of\ndiminished illumination. In computer vision, shadow detection, removal, and\ngeneration are crucial for enhancing scene understanding, refining image\nquality, ensuring visual consistency in video editing, and improving virtual\nenvironments. This paper presents a comprehensive survey of shadow detection,\nremoval, and generation in images and videos within the deep learning landscape\nover the past decade, covering tasks, deep models, datasets, and evaluation\nmetrics. Our key contributions include a comprehensive survey of shadow\nanalysis, standardization of experimental comparisons, exploration of the\nrelationships among model size, speed, and performance, a cross-dataset\ngeneralization study, identification of open issues and future directions, and\nprovision of publicly available resources to support further research.\n","authors":["Xiaowei Hu","Zhenghao Xing","Tianyu Wang","Chi-Wing Fu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2409.02108v1.pdf","comment":"Publicly available results, trained models, and evaluation metrics at\n  https://github.com/xw-hu/Unveiling-Deep-Shadows"},{"id":"http://arxiv.org/abs/2409.02101v1","updated":"2024-09-03T17:56:51Z","published":"2024-09-03T17:56:51Z","title":"Towards Real-World Adverse Weather Image Restoration: Enhancing\n  Clearness and Semantics with Vision-Language Models","summary":"  This paper addresses the limitations of adverse weather image restoration\napproaches trained on synthetic data when applied to real-world scenarios. We\nformulate a semi-supervised learning framework employing vision-language models\nto enhance restoration performance across diverse adverse weather conditions in\nreal-world settings. Our approach involves assessing image clearness and\nproviding semantics using vision-language models on real data, serving as\nsupervision signals for training restoration models. For clearness enhancement,\nwe use real-world data, utilizing a dual-step strategy with pseudo-labels\nassessed by vision-language models and weather prompt learning. For semantic\nenhancement, we integrate real-world data by adjusting weather conditions in\nvision-language model descriptions while preserving semantic meaning.\nAdditionally, we introduce an effective training strategy to bootstrap\nrestoration performance. Our approach achieves superior results in real-world\nadverse weather image restoration, demonstrated through qualitative and\nquantitative comparisons with state-of-the-art works.\n","authors":["Jiaqi Xu","Mengyang Wu","Xiaowei Hu","Chi-Wing Fu","Qi Dou","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2409.02101v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2409.02049v1","updated":"2024-09-03T16:53:34Z","published":"2024-09-03T16:53:34Z","title":"Low-Resolution Face Recognition via Adaptable Instance-Relation\n  Distillation","summary":"  Low-resolution face recognition is a challenging task due to the missing of\ninformative details. Recent approaches based on knowledge distillation have\nproven that high-resolution clues can well guide low-resolution face\nrecognition via proper knowledge transfer. However, due to the distribution\ndifference between training and testing faces, the learned models often suffer\nfrom poor adaptability. To address that, we split the knowledge transfer\nprocess into distillation and adaptation steps, and propose an adaptable\ninstance-relation distillation approach to facilitate low-resolution face\nrecognition. In the approach, the student distills knowledge from\nhigh-resolution teacher in both instance level and relation level, providing\nsufficient cross-resolution knowledge transfer. Then, the learned student can\nbe adaptable to recognize low-resolution faces with adaptive batch\nnormalization in inference. In this manner, the capability of recovering\nmissing details of familiar low-resolution faces can be effectively enhanced,\nleading to a better knowledge transfer. Extensive experiments on low-resolution\nface recognition clearly demonstrate the effectiveness and adaptability of our\napproach.\n","authors":["Ruixin Shi","Weijia Guo","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2409.02049v1.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2409.01761v1","updated":"2024-09-03T10:15:30Z","published":"2024-09-03T10:15:30Z","title":"PRoGS: Progressive Rendering of Gaussian Splats","summary":"  Over the past year, 3D Gaussian Splatting (3DGS) has received significant\nattention for its ability to represent 3D scenes in a perceptually accurate\nmanner. However, it can require a substantial amount of storage since each\nsplat's individual data must be stored. While compression techniques offer a\npotential solution by reducing the memory footprint, they still necessitate\nretrieving the entire scene before any part of it can be rendered. In this\nwork, we introduce a novel approach for progressively rendering such scenes,\naiming to display visible content that closely approximates the final scene as\nearly as possible without loading the entire scene into memory. This approach\nbenefits both on-device rendering applications limited by memory constraints\nand streaming applications where minimal bandwidth usage is preferred. To\nachieve this, we approximate the contribution of each Gaussian to the final\nscene and construct an order of prioritization on their inclusion in the\nrendering process. Additionally, we demonstrate that our approach can be\ncombined with existing compression methods to progressively render (and stream)\n3DGS scenes, optimizing bandwidth usage by focusing on the most important\nsplats within a scene. Overall, our work establishes a foundation for making\nremotely hosted 3DGS content more quickly accessible to end-users in\nover-the-top consumption scenarios, with our results showing significant\nimprovements in quality across all metrics compared to existing methods.\n","authors":["Brent Zoomers","Maarten Wijnants","Ivan Molenaers","Joni Vanherck","Jeroen Put","Lode Jorissen","Nick Michiels"],"pdf_url":"https://arxiv.org/pdf/2409.01761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01710v1","updated":"2024-09-03T08:47:17Z","published":"2024-09-03T08:47:17Z","title":"Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation","summary":"  Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.\n","authors":["Zhongze Tang","Mengmei Ye","Yao Liu","Sheng Wei"],"pdf_url":"https://arxiv.org/pdf/2409.01710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05449v2","updated":"2024-09-03T08:01:47Z","published":"2023-12-09T03:33:14Z","title":"TALDS-Net: Task-Aware Adaptive Local Descriptors Selection for Few-shot\n  Image Classification","summary":"  Few-shot image classification aims to classify images from unseen novel\nclasses with few samples. Recent works demonstrate that deep local descriptors\nexhibit enhanced representational capabilities compared to image-level\nfeatures. However, most existing methods solely rely on either employing all\nlocal descriptors or directly utilizing partial descriptors, potentially\nresulting in the loss of crucial information. Moreover, these methods primarily\nemphasize the selection of query descriptors while overlooking support\ndescriptors. In this paper, we propose a novel Task-Aware Adaptive Local\nDescriptors Selection Network (TALDS-Net), which exhibits the capacity for\nadaptive selection of task-aware support descriptors and query descriptors.\nSpecifically, we compare the similarity of each local support descriptor with\nother local support descriptors to obtain the optimal support descriptor subset\nand then compare the query descriptors with the optimal support subset to\nobtain discriminative query descriptors. Extensive experiments demonstrate that\nour TALDS-Net outperforms state-of-the-art methods on both general and\nfine-grained datasets.\n","authors":["Qian Qiao","Yu Xie","Ziyin Zeng","Fanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2312.05449v2.pdf","comment":"4 pages, 1 figures, is accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2409.01534v1","updated":"2024-09-03T02:08:47Z","published":"2024-09-03T02:08:47Z","title":"Think Twice Before Recognizing: Large Multimodal Models for General\n  Fine-grained Traffic Sign Recognition","summary":"  We propose a new strategy called think twice before recognizing to improve\nfine-grained traffic sign recognition (TSR). Fine-grained TSR in the wild is\ndifficult due to the complex road conditions, and existing approaches\nparticularly struggle with cross-country TSR when data is lacking. Our strategy\nachieves effective fine-grained TSR by stimulating the multiple-thinking\ncapability of large multimodal models (LMM). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for the LMM. The context descriptions with center coordinate prompt\noptimization help the LMM to locate the target traffic sign in the original\nroad images containing multiple traffic signs and filter irrelevant answers\nthrough the proposed prior traffic sign hypothesis. The characteristic\ndescription is based on few-shot in-context learning of template traffic signs,\nwhich decreases the cross-domain difference and enhances the fine-grained\nrecognition capability of the LMM. The differential descriptions of similar\ntraffic signs optimize the multimodal thinking capability of the LMM. The\nproposed method is independent of training data and requires only simple and\nuniform instructions. We conducted extensive experiments on three benchmark\ndatasets and two real-world datasets from different countries, and the proposed\nmethod achieves state-of-the-art TSR results on all five datasets.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2409.01534v1.pdf","comment":null}]}}